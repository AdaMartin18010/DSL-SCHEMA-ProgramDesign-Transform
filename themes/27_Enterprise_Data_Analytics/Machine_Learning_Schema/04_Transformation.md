# æœºå™¨å­¦ä¹ Schemaè½¬æ¢ä½“ç³»

## ğŸ“‘ ç›®å½•

- [æœºå™¨å­¦ä¹ Schemaè½¬æ¢ä½“ç³»](#æœºå™¨å­¦ä¹ schemaè½¬æ¢ä½“ç³»)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. è½¬æ¢ä½“ç³»æ¦‚è¿°](#1-è½¬æ¢ä½“ç³»æ¦‚è¿°)
    - [1.1 è½¬æ¢ç›®æ ‡](#11-è½¬æ¢ç›®æ ‡)
  - [2. æœºå™¨å­¦ä¹ åˆ°MLflowè½¬æ¢](#2-æœºå™¨å­¦ä¹ åˆ°mlflowè½¬æ¢)
  - [3. æœºå™¨å­¦ä¹ åˆ°Kubeflowè½¬æ¢](#3-æœºå™¨å­¦ä¹ åˆ°kubeflowè½¬æ¢)
  - [4. æœºå™¨å­¦ä¹ åˆ°ONNXè½¬æ¢](#4-æœºå™¨å­¦ä¹ åˆ°onnxè½¬æ¢)
  - [5. æœºå™¨å­¦ä¹ æ•°æ®å­˜å‚¨ä¸åˆ†æ](#5-æœºå™¨å­¦ä¹ æ•°æ®å­˜å‚¨ä¸åˆ†æ)
    - [5.1 PostgreSQLæœºå™¨å­¦ä¹ æ•°æ®å­˜å‚¨](#51-postgresqlæœºå™¨å­¦ä¹ æ•°æ®å­˜å‚¨)
    - [5.2 æœºå™¨å­¦ä¹ æ•°æ®åˆ†ææŸ¥è¯¢](#52-æœºå™¨å­¦ä¹ æ•°æ®åˆ†ææŸ¥è¯¢)

---

## 1. è½¬æ¢ä½“ç³»æ¦‚è¿°

æœºå™¨å­¦ä¹ Schemaè½¬æ¢ä½“ç³»æ”¯æŒæœºå™¨å­¦ä¹ åˆ°MLflowã€Kubeflowã€ONNXæ ¼å¼è½¬æ¢ï¼Œä»¥åŠæœºå™¨å­¦ä¹ æ•°æ®å­˜å‚¨ã€‚

### 1.1 è½¬æ¢ç›®æ ‡

1. **æœºå™¨å­¦ä¹ åˆ°MLflowè½¬æ¢**ï¼šæœºå™¨å­¦ä¹ å®éªŒåˆ°MLflowæ ¼å¼
2. **æœºå™¨å­¦ä¹ åˆ°Kubeflowè½¬æ¢**ï¼šæœºå™¨å­¦ä¹ ç®¡é“åˆ°Kubeflowæ ¼å¼
3. **æœºå™¨å­¦ä¹ åˆ°ONNXè½¬æ¢**ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹åˆ°ONNXæ ¼å¼
4. **æœºå™¨å­¦ä¹ åˆ°æ•°æ®åº“è½¬æ¢**ï¼šæœºå™¨å­¦ä¹ æ•°æ®åˆ°PostgreSQLå­˜å‚¨

---

## 2. æœºå™¨å­¦ä¹ åˆ°MLflowè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- å®éªŒ â†’ MLflow Experiment
- è¿è¡Œ â†’ MLflow Run
- æ¨¡å‹ â†’ MLflow Model

**è½¬æ¢ç¤ºä¾‹**ï¼š

```python
def convert_ml_to_mlflow(ml_data: MachineLearningSchema) -> MLflowExperiment:
    """å°†æœºå™¨å­¦ä¹ Schemaè½¬æ¢ä¸ºMLflowæ ¼å¼"""
    import mlflow

    experiment = ml_data.experiment_management.experiments[0]

    # åˆ›å»ºMLflowå®éªŒ
    mlflow_experiment = mlflow.create_experiment(experiment.experiment_name)

    # è½¬æ¢è¿è¡Œ
    for run in ml_data.experiment_management.runs:
        with mlflow.start_run(experiment_id=mlflow_experiment, run_name=run.run_name):
            # è®°å½•å‚æ•°
            mlflow.log_params(run.parameters)

            # è®°å½•æŒ‡æ ‡
            mlflow.log_metrics(run.metrics)

            # è®°å½•å·¥ä»¶
            for artifact in run.artifacts:
                if artifact.artifact_type == "Model":
                    mlflow.sklearn.log_model(
                        artifact.artifact_path,
                        "model"
                    )
                else:
                    mlflow.log_artifact(artifact.artifact_path)

            # è®°å½•æ ‡ç­¾
            if run.tags:
                mlflow.set_tags(run.tags)

    return mlflow_experiment
```

---

## 3. æœºå™¨å­¦ä¹ åˆ°Kubeflowè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- è®­ç»ƒå®šä¹‰ â†’ Kubeflow Pipeline Component
- è®­ç»ƒæµç¨‹ â†’ Kubeflow Pipeline
- æ¨¡å‹éƒ¨ç½² â†’ Kubeflow Serving

**è½¬æ¢ç¤ºä¾‹**ï¼š

```python
def convert_ml_to_kubeflow(ml_data: MachineLearningSchema) -> KubeflowPipeline:
    """å°†æœºå™¨å­¦ä¹ Schemaè½¬æ¢ä¸ºKubeflow Pipelineæ ¼å¼"""
    from kfp import dsl

    @dsl.pipeline(
        name=ml_data.experiment_management.experiments[0].experiment_name,
        description="Machine Learning Pipeline"
    )
    def ml_pipeline():
        # æ•°æ®å‡†å¤‡ç»„ä»¶
        data_prep = dsl.ContainerOp(
            name="data-preparation",
            image="data-prep:latest",
            command=["python", "prepare_data.py"],
            arguments=[
                "--input-path", ml_data.model_training.training_definitions[0].data_config.training_data_path,
                "--output-path", "/data/processed"
            ]
        )

        # æ¨¡å‹è®­ç»ƒç»„ä»¶
        train = dsl.ContainerOp(
            name="model-training",
            image="train:latest",
            command=["python", "train_model.py"],
            arguments=[
                "--data-path", data_prep.outputs["output-path"],
                "--model-type", ml_data.model_training.training_definitions[0].model_architecture.model_type,
                "--epochs", str(ml_data.model_training.training_definitions[0].training_config.epochs)
            ]
        )
        train.after(data_prep)

        # æ¨¡å‹è¯„ä¼°ç»„ä»¶
        evaluate = dsl.ContainerOp(
            name="model-evaluation",
            image="evaluate:latest",
            command=["python", "evaluate_model.py"],
            arguments=[
                "--model-path", train.outputs["model-path"],
                "--test-data-path", ml_data.model_training.training_definitions[0].data_config.test_data_path
            ]
        )
        evaluate.after(train)

    return ml_pipeline
```

---

## 4. æœºå™¨å­¦ä¹ åˆ°ONNXè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- æ¨¡å‹æ¶æ„ â†’ ONNX Graph
- æ¨¡å‹å‚æ•° â†’ ONNX Initializers
- æ¨¡å‹è¾“å…¥è¾“å‡º â†’ ONNX Inputs/Outputs

**è½¬æ¢ç¤ºä¾‹**ï¼š

```python
def convert_ml_to_onnx(ml_data: MachineLearningSchema) -> ONNXModel:
    """å°†æœºå™¨å­¦ä¹ æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼"""
    import onnx
    from onnx import helper, TensorProto

    model_arch = ml_data.model_training.training_definitions[0].model_architecture

    # åˆ›å»ºONNXå›¾è¾“å…¥
    graph_inputs = [
        helper.make_tensor_value_info(
            "input",
            TensorProto.FLOAT,
            model_arch.input_shape
        )
    ]

    # åˆ›å»ºONNXå›¾è¾“å‡º
    graph_outputs = [
        helper.make_tensor_value_info(
            "output",
            TensorProto.FLOAT,
            model_arch.output_shape
        )
    ]

    # åˆ›å»ºONNXèŠ‚ç‚¹
    graph_nodes = []
    if model_arch.model_type == "Neural_Network":
        # ç¥ç»ç½‘ç»œå±‚
        for i, layer_size in enumerate(model_arch.hidden_layers):
            node = helper.make_node(
                "MatMul",
                inputs=[f"input_{i}", f"weight_{i}"],
                outputs=[f"output_{i}"]
            )
            graph_nodes.append(node)

            # æ¿€æ´»å‡½æ•°
            if model_arch.activation_functions and i < len(model_arch.activation_functions):
                activation_node = helper.make_node(
                    model_arch.activation_functions[i],
                    inputs=[f"output_{i}"],
                    outputs=[f"activated_{i}"]
                )
                graph_nodes.append(activation_node)

    # åˆ›å»ºONNXæ¨¡å‹
    onnx_model = helper.make_model(
        helper.make_graph(
            graph_nodes,
            ml_data.model_training.training_definitions[0].definition_name,
            graph_inputs,
            graph_outputs
        )
    )

    return onnx_model
```

---

## 5. æœºå™¨å­¦ä¹ æ•°æ®å­˜å‚¨ä¸åˆ†æ

### 5.1 PostgreSQLæœºå™¨å­¦ä¹ æ•°æ®å­˜å‚¨

**è¡¨ç»“æ„è®¾è®¡**ï¼š

```sql
-- å®éªŒå…ƒæ•°æ®è¡¨
CREATE TABLE experiment_metadata (
    experiment_id VARCHAR(50) PRIMARY KEY,
    experiment_name VARCHAR(200) NOT NULL,
    experiment_description TEXT,
    created_by VARCHAR(100) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- è¿è¡Œå…ƒæ•°æ®è¡¨
CREATE TABLE run_metadata (
    run_id VARCHAR(50) PRIMARY KEY,
    experiment_id VARCHAR(50) NOT NULL,
    run_name VARCHAR(200) NOT NULL,
    run_status VARCHAR(20) DEFAULT 'Running',
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP,
    duration INTEGER,
    parent_run_id VARCHAR(50),
    FOREIGN KEY (experiment_id) REFERENCES experiment_metadata(experiment_id)
);

-- è¿è¡Œå‚æ•°è¡¨
CREATE TABLE run_parameters (
    parameter_id VARCHAR(50) PRIMARY KEY,
    run_id VARCHAR(50) NOT NULL,
    parameter_name VARCHAR(100) NOT NULL,
    parameter_value VARCHAR(500) NOT NULL,
    FOREIGN KEY (run_id) REFERENCES run_metadata(run_id)
);

-- è¿è¡ŒæŒ‡æ ‡è¡¨
CREATE TABLE run_metrics (
    metric_id VARCHAR(50) PRIMARY KEY,
    run_id VARCHAR(50) NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    metric_value DECIMAL(18, 4) NOT NULL,
    metric_timestamp TIMESTAMP NOT NULL,
    FOREIGN KEY (run_id) REFERENCES run_metadata(run_id)
);

-- æ¨¡å‹æ³¨å†Œè¡¨
CREATE TABLE model_registry (
    model_id VARCHAR(50) PRIMARY KEY,
    model_name VARCHAR(200) NOT NULL,
    model_description TEXT,
    created_by VARCHAR(100) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æ¨¡å‹ç‰ˆæœ¬è¡¨
CREATE TABLE model_versions (
    version_id VARCHAR(50) PRIMARY KEY,
    model_id VARCHAR(50) NOT NULL,
    version_number INT NOT NULL,
    version_stage VARCHAR(20) DEFAULT 'None',
    run_id VARCHAR(50) NOT NULL,
    model_uri VARCHAR(500) NOT NULL,
    model_format VARCHAR(20) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by VARCHAR(100) NOT NULL,
    FOREIGN KEY (model_id) REFERENCES model_registry(model_id),
    FOREIGN KEY (run_id) REFERENCES run_metadata(run_id)
);

-- æ¨¡å‹éƒ¨ç½²è¡¨
CREATE TABLE model_deployments (
    deployment_id VARCHAR(50) PRIMARY KEY,
    model_id VARCHAR(50) NOT NULL,
    version_id VARCHAR(50) NOT NULL,
    deployment_name VARCHAR(200) NOT NULL,
    deployment_environment VARCHAR(20) NOT NULL,
    deployment_status VARCHAR(20) DEFAULT 'Deploying',
    deployment_date TIMESTAMP NOT NULL,
    deployment_endpoint VARCHAR(500) NOT NULL,
    FOREIGN KEY (model_id) REFERENCES model_registry(model_id),
    FOREIGN KEY (version_id) REFERENCES model_versions(version_id)
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_run_metadata_experiment ON run_metadata(experiment_id);
CREATE INDEX idx_run_metadata_status ON run_metadata(run_status);
CREATE INDEX idx_run_parameters_run ON run_parameters(run_id);
CREATE INDEX idx_run_metrics_run ON run_metrics(run_id);
CREATE INDEX idx_model_versions_model ON model_versions(model_id);
CREATE INDEX idx_model_versions_stage ON model_versions(version_stage);
CREATE INDEX idx_model_deployments_model ON model_deployments(model_id);
CREATE INDEX idx_model_deployments_status ON model_deployments(deployment_status);
```

### 5.2 æœºå™¨å­¦ä¹ æ•°æ®åˆ†ææŸ¥è¯¢

**æŸ¥è¯¢ç¤ºä¾‹**ï¼š

```python
def analyze_ml_data(conn):
    """åˆ†ææœºå™¨å­¦ä¹ æ•°æ®"""
    cursor = conn.cursor()

    # æŸ¥è¯¢å®éªŒæ±‡æ€»
    cursor.execute("""
        SELECT
            em.experiment_name,
            COUNT(rm.run_id) as run_count,
            SUM(CASE WHEN rm.run_status = 'Finished' THEN 1 ELSE 0 END) as completed_runs,
            SUM(CASE WHEN rm.run_status = 'Failed' THEN 1 ELSE 0 END) as failed_runs
        FROM experiment_metadata em
        LEFT JOIN run_metadata rm ON em.experiment_id = rm.experiment_id
        GROUP BY em.experiment_id, em.experiment_name
        ORDER BY em.experiment_name
    """)

    experiment_summary = cursor.fetchall()

    # æŸ¥è¯¢æ¨¡å‹æ€§èƒ½æ±‡æ€»
    cursor.execute("""
        SELECT
            mr.model_name,
            mv.version_number,
            mv.version_stage,
            AVG(rm_metrics.metric_value) as avg_metric_value
        FROM model_registry mr
        JOIN model_versions mv ON mr.model_id = mv.model_id
        JOIN run_metadata rm ON mv.run_id = rm.run_id
        JOIN run_metrics rm_metrics ON rm.run_id = rm_metrics.run_id
        WHERE rm_metrics.metric_name = 'accuracy'
        GROUP BY mr.model_id, mr.model_name, mv.version_id, mv.version_number, mv.version_stage
        ORDER BY mr.model_name, mv.version_number
    """)

    model_performance = cursor.fetchall()

    # æŸ¥è¯¢æ¨¡å‹éƒ¨ç½²æ±‡æ€»
    cursor.execute("""
        SELECT
            mr.model_name,
            mv.version_number,
            md.deployment_environment,
            md.deployment_status,
            md.deployment_date
        FROM model_registry mr
        JOIN model_versions mv ON mr.model_id = mv.model_id
        JOIN model_deployments md ON mv.version_id = md.version_id
        WHERE md.deployment_status = 'Active'
        ORDER BY md.deployment_date DESC
    """)

    deployment_summary = cursor.fetchall()

    return {
        "experiment_summary": experiment_summary,
        "model_performance": model_performance,
        "deployment_summary": deployment_summary
    }
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `05_Case_Studies.md` - å®è·µæ¡ˆä¾‹

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
