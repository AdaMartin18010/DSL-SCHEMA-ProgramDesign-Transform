# ETL Schemaè½¬æ¢ä½“ç³»

## ğŸ“‘ ç›®å½•

- [ETL Schemaè½¬æ¢ä½“ç³»](#etl-schemaè½¬æ¢ä½“ç³»)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. è½¬æ¢ä½“ç³»æ¦‚è¿°](#1-è½¬æ¢ä½“ç³»æ¦‚è¿°)
    - [1.1 è½¬æ¢ç›®æ ‡](#11-è½¬æ¢ç›®æ ‡)
  - [2. ETLåˆ°Informaticaè½¬æ¢](#2-etlåˆ°informaticaè½¬æ¢)
  - [3. ETLåˆ°Talendè½¬æ¢](#3-etlåˆ°talendè½¬æ¢)
  - [4. ETLåˆ°JSON Schemaè½¬æ¢](#4-etlåˆ°json-schemaè½¬æ¢)
  - [5. ETLæ•°æ®å­˜å‚¨ä¸åˆ†æ](#5-etlæ•°æ®å­˜å‚¨ä¸åˆ†æ)
    - [5.1 PostgreSQL ETLå…ƒæ•°æ®å­˜å‚¨](#51-postgresql-etlå…ƒæ•°æ®å­˜å‚¨)
    - [5.2 ETLæ•°æ®åˆ†ææŸ¥è¯¢](#52-etlæ•°æ®åˆ†ææŸ¥è¯¢)

---

## 1. è½¬æ¢ä½“ç³»æ¦‚è¿°

ETL Schemaè½¬æ¢ä½“ç³»æ”¯æŒETLåˆ°Informaticaã€Talendã€JSON Schemaæ ¼å¼è½¬æ¢ï¼Œä»¥åŠETLå…ƒæ•°æ®å­˜å‚¨ã€‚

### 1.1 è½¬æ¢ç›®æ ‡

1. **ETLåˆ°Informaticaè½¬æ¢**ï¼šETL Schemaåˆ°Informaticaæ ¼å¼
2. **ETLåˆ°Talendè½¬æ¢**ï¼šETL Schemaåˆ°Talendæ ¼å¼
3. **ETLåˆ°JSON Schemaè½¬æ¢**ï¼šETL Schemaåˆ°JSON Schemaæ ¼å¼
4. **ETLåˆ°æ•°æ®åº“è½¬æ¢**ï¼šETLå…ƒæ•°æ®åˆ°PostgreSQLå­˜å‚¨

---

## 2. ETLåˆ°Informaticaè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- æ•°æ®æºè¿æ¥ â†’ Informatica Connection
- æå–è§„åˆ™ â†’ Informatica Source
- è½¬æ¢è§„åˆ™ â†’ Informatica Transformation
- åŠ è½½ç­–ç•¥ â†’ Informatica Target

**è½¬æ¢ç¤ºä¾‹**ï¼š

```python
def convert_etl_to_informatica(etl_data: ETLSchema) -> InformaticaWorkflow:
    """å°†ETL Schemaè½¬æ¢ä¸ºInformaticaæ ¼å¼"""
    workflow = InformaticaWorkflow()

    # è½¬æ¢æ•°æ®æºè¿æ¥
    for connection in etl_data.extract.data_source_connections:
        infa_connection = InformaticaConnection()
        infa_connection.name = connection.connection_name
        infa_connection.type = map_connection_type_to_informatica(connection.connection_type)
        infa_connection.connection_string = connection.connection_string
        infa_connection.parameters = connection.connection_parameters
        workflow.connections.append(infa_connection)

    # è½¬æ¢ETLæµç¨‹
    for process in etl_data.etl_process.process_definitions:
        # åˆ›å»ºæ˜ å°„
        mapping = InformaticaMapping()
        mapping.name = process.process_name

        # è½¬æ¢æå–è§„åˆ™
        extract_rule = find_extract_rule(etl_data, process.extract_rule_id)
        source = InformaticaSource()
        source.name = extract_rule.source_table or "Source"
        source.connection = find_connection_by_id(workflow, extract_rule.connection_id)
        source.query = extract_rule.source_query
        mapping.sources.append(source)

        # è½¬æ¢è½¬æ¢è§„åˆ™
        for transform_rule_id in process.transform_rule_ids:
            transform_rule = find_transform_rule(etl_data, transform_rule_id)
            transformation = InformaticaTransformation()
            transformation.name = transform_rule.rule_name
            transformation.type = map_transform_type_to_informatica(transform_rule.rule_type)
            transformation.expression = transform_rule.transform_logic
            transformation.parameters = transform_rule.transform_parameters
            mapping.transformations.append(transformation)

        # è½¬æ¢åŠ è½½ç­–ç•¥
        load_strategy = find_load_strategy(etl_data, process.load_strategy_id)
        target = InformaticaTarget()
        target.name = find_target_table(etl_data, load_strategy.table_id).table_name
        target.connection = create_target_connection(workflow, load_strategy)
        target.load_mode = map_load_mode_to_informatica(load_strategy)
        mapping.targets.append(target)

        workflow.mappings.append(mapping)

        # åˆ›å»ºä¼šè¯
        session = InformaticaSession()
        session.name = f"{process.process_name}_Session"
        session.mapping = mapping.name
        session.source_connection = source.connection.name
        session.target_connection = target.connection.name
        workflow.sessions.append(session)

        # åˆ›å»ºå·¥ä½œæµä»»åŠ¡
        task = InformaticaTask()
        task.name = f"{process.process_name}_Task"
        task.type = "Session"
        task.session = session.name
        workflow.tasks.append(task)

    return workflow
```

---

## 3. ETLåˆ°Talendè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- æ•°æ®æºè¿æ¥ â†’ Talend Connection
- æå–è§„åˆ™ â†’ Talend Input Component
- è½¬æ¢è§„åˆ™ â†’ Talend Transformation Component
- åŠ è½½ç­–ç•¥ â†’ Talend Output Component

**è½¬æ¢ç¤ºä¾‹**ï¼š

```python
def convert_etl_to_talend(etl_data: ETLSchema) -> TalendJob:
    """å°†ETL Schemaè½¬æ¢ä¸ºTalendæ ¼å¼"""
    job = TalendJob()
    job.name = "ETL_Job"

    # è½¬æ¢æ•°æ®æºè¿æ¥
    for connection in etl_data.extract.data_source_connections:
        talend_connection = TalendConnection()
        talend_connection.name = connection.connection_name
        talend_connection.type = map_connection_type_to_talend(connection.connection_type)
        talend_connection.properties = {
            "connection_string": connection.connection_string,
            **connection.connection_parameters
        }
        job.connections.append(talend_connection)

    # è½¬æ¢ETLæµç¨‹
    for process in etl_data.etl_process.process_definitions:
        # è½¬æ¢æå–è§„åˆ™
        extract_rule = find_extract_rule(etl_data, process.extract_rule_id)
        input_component = TalendInputComponent()
        input_component.name = f"tInput_{extract_rule.rule_id}"
        input_component.type = map_connection_type_to_talend_input(extract_rule.connection_id)
        input_component.connection = find_connection_by_id(job, extract_rule.connection_id)
        input_component.query = extract_rule.source_query
        input_component.table = extract_rule.source_table
        job.components.append(input_component)

        prev_component = input_component

        # è½¬æ¢è½¬æ¢è§„åˆ™
        for transform_rule_id in process.transform_rule_ids:
            transform_rule = find_transform_rule(etl_data, transform_rule_id)
            transform_component = TalendTransformComponent()
            transform_component.name = f"tTransform_{transform_rule.rule_id}"
            transform_component.type = map_transform_type_to_talend(transform_rule.rule_type)
            transform_component.expression = transform_rule.transform_logic
            transform_component.parameters = transform_rule.transform_parameters
            job.components.append(transform_component)

            # åˆ›å»ºè¿æ¥
            link = TalendLink()
            link.from_component = prev_component.name
            link.to_component = transform_component.name
            job.links.append(link)

            prev_component = transform_component

        # è½¬æ¢åŠ è½½ç­–ç•¥
        load_strategy = find_load_strategy(etl_data, process.load_strategy_id)
        target_table = find_target_table(etl_data, load_strategy.table_id)
        output_component = TalendOutputComponent()
        output_component.name = f"tOutput_{load_strategy.strategy_id}"
        output_component.type = map_load_strategy_to_talend_output(load_strategy)
        output_component.table = target_table.table_name
        output_component.schema = target_table.table_schema
        output_component.mode = map_load_mode_to_talend(load_strategy)
        job.components.append(output_component)

        # åˆ›å»ºè¿æ¥
        link = TalendLink()
        link.from_component = prev_component.name
        link.to_component = output_component.name
        job.links.append(link)

    return job
```

---

## 4. ETLåˆ°JSON Schemaè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- ETLæµç¨‹ â†’ JSON Schema Object
- æå–è§„åˆ™ â†’ JSON Schema Property
- è½¬æ¢è§„åˆ™ â†’ JSON Schema Property
- åŠ è½½ç­–ç•¥ â†’ JSON Schema Property

**è½¬æ¢ç¤ºä¾‹**ï¼š

```python
def convert_etl_to_json_schema(etl_data: ETLSchema) -> JSONSchema:
    """å°†ETL Schemaè½¬æ¢ä¸ºJSON Schemaæ ¼å¼"""
    json_schema = {
        "$schema": "http://json-schema.org/draft-07/schema#",
        "type": "object",
        "properties": {}
    }

    # è½¬æ¢ETLæµç¨‹
    for process in etl_data.etl_process.process_definitions:
        process_schema = {
            "type": "object",
            "properties": {
                "process_id": {"type": "string"},
                "process_name": {"type": "string"},
                "process_type": {"type": "string"},
                "extract": {
                    "type": "object",
                    "properties": {
                        "rule_id": {"type": "string"},
                        "source_table": {"type": "string"},
                        "source_query": {"type": "string"}
                    }
                },
                "transform": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "rule_id": {"type": "string"},
                            "rule_name": {"type": "string"},
                            "rule_type": {"type": "string"},
                            "transform_logic": {"type": "string"}
                        }
                    }
                },
                "load": {
                    "type": "object",
                    "properties": {
                        "strategy_id": {"type": "string"},
                        "table_id": {"type": "string"},
                        "strategy_type": {"type": "string"}
                    }
                }
            }
        }

        json_schema["properties"][process.process_name] = process_schema

    return json_schema
```

---

## 5. ETLæ•°æ®å­˜å‚¨ä¸åˆ†æ

### 5.1 PostgreSQL ETLå…ƒæ•°æ®å­˜å‚¨

**è¡¨ç»“æ„è®¾è®¡**ï¼š

```sql
-- æ•°æ®æºè¿æ¥è¡¨
CREATE TABLE data_source_connections (
    connection_id VARCHAR(50) PRIMARY KEY,
    connection_name VARCHAR(200) NOT NULL,
    connection_type VARCHAR(20) NOT NULL,
    connection_string VARCHAR(500) NOT NULL,
    connection_parameters JSONB,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æå–è§„åˆ™è¡¨
CREATE TABLE extract_rules (
    rule_id VARCHAR(50) PRIMARY KEY,
    connection_id VARCHAR(50) NOT NULL,
    source_table VARCHAR(200),
    source_query TEXT,
    extract_condition TEXT,
    extract_fields TEXT[],
    extract_frequency VARCHAR(20) DEFAULT 'Daily',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (connection_id) REFERENCES data_source_connections(connection_id)
);

-- è½¬æ¢è§„åˆ™è¡¨
CREATE TABLE transform_rules (
    rule_id VARCHAR(50) PRIMARY KEY,
    rule_name VARCHAR(200) NOT NULL,
    rule_type VARCHAR(20) NOT NULL,
    source_fields TEXT[] NOT NULL,
    target_fields TEXT[] NOT NULL,
    transform_logic TEXT NOT NULL,
    transform_parameters JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ç›®æ ‡è¡¨è¡¨
CREATE TABLE target_tables (
    table_id VARCHAR(50) PRIMARY KEY,
    table_name VARCHAR(200) NOT NULL,
    table_schema VARCHAR(100) NOT NULL,
    table_type VARCHAR(20) NOT NULL,
    table_structure JSONB,
    primary_key VARCHAR(200),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- åŠ è½½ç­–ç•¥è¡¨
CREATE TABLE load_strategies (
    strategy_id VARCHAR(50) PRIMARY KEY,
    table_id VARCHAR(50) NOT NULL,
    strategy_type VARCHAR(20) NOT NULL,
    strategy_parameters JSONB,
    load_frequency VARCHAR(20) DEFAULT 'Daily',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (table_id) REFERENCES target_tables(table_id)
);

-- ETLæµç¨‹å®šä¹‰è¡¨
CREATE TABLE etl_process_definitions (
    process_id VARCHAR(50) PRIMARY KEY,
    process_name VARCHAR(200) NOT NULL,
    process_type VARCHAR(20) NOT NULL,
    extract_rule_id VARCHAR(50) NOT NULL,
    transform_rule_ids TEXT[] NOT NULL,
    load_strategy_id VARCHAR(50) NOT NULL,
    process_dependencies TEXT[],
    process_parameters JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (extract_rule_id) REFERENCES extract_rules(rule_id),
    FOREIGN KEY (load_strategy_id) REFERENCES load_strategies(strategy_id)
);

-- ETLæ‰§è¡Œå†å²è¡¨
CREATE TABLE etl_execution_history (
    execution_id VARCHAR(50) PRIMARY KEY,
    process_id VARCHAR(50) NOT NULL,
    execution_start_time TIMESTAMP NOT NULL,
    execution_end_time TIMESTAMP,
    execution_status VARCHAR(20) DEFAULT 'Running',
    rows_extracted BIGINT,
    rows_transformed BIGINT,
    rows_loaded BIGINT,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (process_id) REFERENCES etl_process_definitions(process_id)
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_extract_rules_connection ON extract_rules(connection_id);
CREATE INDEX idx_load_strategies_table ON load_strategies(table_id);
CREATE INDEX idx_etl_process_extract ON etl_process_definitions(extract_rule_id);
CREATE INDEX idx_etl_process_load ON etl_process_definitions(load_strategy_id);
CREATE INDEX idx_etl_execution_process ON etl_execution_history(process_id);
CREATE INDEX idx_etl_execution_status ON etl_execution_history(execution_status);
```

### 5.2 ETLæ•°æ®åˆ†ææŸ¥è¯¢

**æŸ¥è¯¢ç¤ºä¾‹**ï¼š

```python
def analyze_etl_data(conn):
    """åˆ†æETLæ•°æ®"""
    cursor = conn.cursor()

    # æŸ¥è¯¢ETLæµç¨‹æ‰§è¡Œç»Ÿè®¡
    cursor.execute("""
        SELECT
            epd.process_name,
            epd.process_type,
            COUNT(eh.execution_id) as execution_count,
            SUM(CASE WHEN eh.execution_status = 'Completed' THEN 1 ELSE 0 END) as success_count,
            SUM(CASE WHEN eh.execution_status = 'Failed' THEN 1 ELSE 0 END) as failed_count,
            AVG(eh.rows_loaded) as avg_rows_loaded,
            AVG(EXTRACT(EPOCH FROM (eh.execution_end_time - eh.execution_start_time))) as avg_duration_seconds
        FROM etl_process_definitions epd
        LEFT JOIN etl_execution_history eh ON epd.process_id = eh.process_id
        WHERE eh.execution_start_time >= CURRENT_DATE - INTERVAL '30 days'
        GROUP BY epd.process_id, epd.process_name, epd.process_type
        ORDER BY execution_count DESC
    """)

    process_statistics = cursor.fetchall()

    # æŸ¥è¯¢æ•°æ®æºè¿æ¥ä½¿ç”¨æƒ…å†µ
    cursor.execute("""
        SELECT
            dsc.connection_type,
            COUNT(DISTINCT dsc.connection_id) as connection_count,
            COUNT(DISTINCT er.rule_id) as extract_rule_count
        FROM data_source_connections dsc
        LEFT JOIN extract_rules er ON dsc.connection_id = er.connection_id
        WHERE dsc.is_active = TRUE
        GROUP BY dsc.connection_type
        ORDER BY connection_count DESC
    """)

    connection_usage = cursor.fetchall()

    # æŸ¥è¯¢è½¬æ¢è§„åˆ™ä½¿ç”¨æƒ…å†µ
    cursor.execute("""
        SELECT
            tr.rule_type,
            COUNT(*) as rule_count,
            COUNT(DISTINCT epd.process_id) as process_count
        FROM transform_rules tr
        LEFT JOIN etl_process_definitions epd ON tr.rule_id = ANY(epd.transform_rule_ids)
        GROUP BY tr.rule_type
        ORDER BY rule_count DESC
    """)

    transform_usage = cursor.fetchall()

    return {
        "process_statistics": process_statistics,
        "connection_usage": connection_usage,
        "transform_usage": transform_usage
    }
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `05_Case_Studies.md` - å®è·µæ¡ˆä¾‹

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
