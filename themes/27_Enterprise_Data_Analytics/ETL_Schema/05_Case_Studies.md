# ETL Schemaå®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [ETL Schemaå®è·µæ¡ˆä¾‹](#etl-schemaå®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¡ˆä¾‹æ¦‚è¿°](#1-æ¡ˆä¾‹æ¦‚è¿°)
  - [2. æ¡ˆä¾‹1ï¼šé”€å”®æ•°æ®ETLæµç¨‹](#2-æ¡ˆä¾‹1é”€å”®æ•°æ®etlæµç¨‹)
    - [2.1 åœºæ™¯æè¿°](#21-åœºæ™¯æè¿°)
    - [2.2 Schemaå®šä¹‰](#22-schemaå®šä¹‰)
  - [3. æ¡ˆä¾‹2ï¼šETLåˆ°Informaticaè½¬æ¢](#3-æ¡ˆä¾‹2etlåˆ°informaticaè½¬æ¢)
    - [3.1 åœºæ™¯æè¿°](#31-åœºæ™¯æè¿°)
    - [3.2 å®ç°ä»£ç ](#32-å®ç°ä»£ç )
  - [4. æ¡ˆä¾‹3ï¼šå¢é‡ETLæµç¨‹](#4-æ¡ˆä¾‹3å¢é‡etlæµç¨‹)
    - [4.1 åœºæ™¯æè¿°](#41-åœºæ™¯æè¿°)
    - [4.2 å®ç°ä»£ç ](#42-å®ç°ä»£ç )
  - [5. æ¡ˆä¾‹4ï¼šæ•°æ®è´¨é‡æ£€æŸ¥ETL](#5-æ¡ˆä¾‹4æ•°æ®è´¨é‡æ£€æŸ¥etl)
    - [5.1 åœºæ™¯æè¿°](#51-åœºæ™¯æè¿°)
    - [5.2 å®ç°ä»£ç ](#52-å®ç°ä»£ç )
  - [6. æ¡ˆä¾‹5ï¼šETLæ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ](#6-æ¡ˆä¾‹5etlæ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ)
    - [6.1 åœºæ™¯æè¿°](#61-åœºæ™¯æè¿°)
    - [6.2 å®ç°ä»£ç ](#62-å®ç°ä»£ç )

---

## 1. æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›ETL Schemaåœ¨å®é™…åº”ç”¨ä¸­çš„å®è·µæ¡ˆä¾‹ã€‚

---

## 2. æ¡ˆä¾‹1ï¼šé”€å”®æ•°æ®ETLæµç¨‹

### 2.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
æ„å»ºé”€å”®æ•°æ®ETLæµç¨‹ï¼Œä»æºç³»ç»Ÿæå–é”€å”®æ•°æ®ï¼Œè¿›è¡Œæ•°æ®è½¬æ¢å’Œæ¸…æ´—ï¼ŒåŠ è½½åˆ°æ•°æ®ä»“åº“ã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒå¢é‡æ•°æ®æå–
- æ”¯æŒæ•°æ®è½¬æ¢å’Œæ¸…æ´—
- æ”¯æŒæ•°æ®åŠ è½½åˆ°æ•°æ®ä»“åº“

### 2.2 Schemaå®šä¹‰

**é”€å”®æ•°æ®ETLæµç¨‹Schema**ï¼š

```dsl
schema SalesDataETL {
  extract_rule: ExtractRule {
    rule_id: String @value("RULE-SALES-EXTRACT")
    connection_id: String @value("CONN-SALES-DB")
    source_table: String @value("sales_transactions")
    extract_condition: String @value("sale_date >= :last_extract_date")
    extract_fields: List<String> {
      "sale_id"
      "sale_date"
      "customer_id"
      "product_id"
      "sale_amount"
      "sale_quantity"
    }
    extract_frequency: Enum @value("Daily")
  }

  transform_rule: TransformRule {
    rule_id: String @value("RULE-SALES-TRANSFORM")
    rule_name: String @value("é”€å”®æ•°æ®è½¬æ¢")
    rule_type: Enum @value("Mapping")
    source_fields: List<String> {
      "sale_id"
      "sale_date"
      "customer_id"
      "product_id"
      "sale_amount"
      "sale_quantity"
    }
    target_fields: List<String> {
      "sale_id"
      "sale_date"
      "customer_key"
      "product_key"
      "sale_amount"
      "sale_quantity"
    }
    transform_logic: String @value("customer_key = lookup_customer(customer_id); product_key = lookup_product(product_id);")
  }

  load_strategy: LoadStrategy {
    strategy_id: String @value("STRATEGY-SALES-LOAD")
    table_id: String @value("TBL-FACT-SALES")
    strategy_type: Enum @value("Incremental_Load")
    load_frequency: Enum @value("Daily")
  }

  etl_process: ProcessDefinition {
    process_id: String @value("PROC-SALES-ETL")
    process_name: String @value("é”€å”®æ•°æ®ETLæµç¨‹")
    process_type: Enum @value("Batch")
    extract_rule_id: String @value("RULE-SALES-EXTRACT")
    transform_rule_ids: List<String> {
      "RULE-SALES-TRANSFORM"
    }
    load_strategy_id: String @value("STRATEGY-SALES-LOAD")
  }
}
```

---

## 3. æ¡ˆä¾‹2ï¼šETLåˆ°Informaticaè½¬æ¢

### 3.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
å°†ETL Schemaè½¬æ¢ä¸ºInformatica Workflowæ ¼å¼ï¼Œç”¨äºInformaticaæ‰§è¡Œã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒè‡ªåŠ¨è½¬æ¢åˆ°Informatica
- æ”¯æŒæ•°æ®æºè¿æ¥é…ç½®
- æ”¯æŒè½¬æ¢é€»è¾‘é…ç½®

### 3.2 å®ç°ä»£ç 

```python
def convert_etl_to_informatica_complete(etl_data: ETLSchema) -> InformaticaWorkflow:
    """å®Œæ•´è½¬æ¢ETL Schemaåˆ°Informatica"""
    workflow = InformaticaWorkflow()
    workflow.name = "ETL_Workflow"

    # è½¬æ¢æ•°æ®æºè¿æ¥
    connections_map = {}
    for connection in etl_data.extract.data_source_connections:
        infa_connection = InformaticaConnection()
        infa_connection.name = connection.connection_name
        infa_connection.type = map_connection_type_to_informatica(connection.connection_type)
        infa_connection.connection_string = connection.connection_string

        # è½¬æ¢è¿æ¥å‚æ•°
        if connection.connection_type == "Database":
            infa_connection.properties = {
                "database_type": connection.connection_parameters.get("database_type", "Oracle"),
                "host": connection.connection_parameters.get("host", ""),
                "port": connection.connection_parameters.get("port", "1521"),
                "database_name": connection.connection_parameters.get("database_name", ""),
                "username": connection.authentication.credentials.get("username", ""),
                "password": connection.authentication.credentials.get("password", "")
            }
        elif connection.connection_type == "File":
            infa_connection.properties = {
                "file_type": connection.connection_parameters.get("file_type", "Delimited"),
                "file_path": connection.connection_string
            }

        workflow.connections.append(infa_connection)
        connections_map[connection.connection_id] = infa_connection

    # è½¬æ¢ETLæµç¨‹
    for process in etl_data.etl_process.process_definitions:
        # åˆ›å»ºæ˜ å°„
        mapping = InformaticaMapping()
        mapping.name = f"{process.process_name}_Mapping"

        # è½¬æ¢æå–è§„åˆ™
        extract_rule = find_extract_rule(etl_data, process.extract_rule_id)
        connection = connections_map[extract_rule.connection_id]

        source = InformaticaSource()
        source.name = extract_rule.source_table or "Source"
        source.connection = connection.name
        source.type = "Relational"

        # è½¬æ¢å­—æ®µ
        for field in extract_rule.extract_fields:
            source_field = InformaticaSourceField()
            source_field.name = field
            source_field.data_type = infer_data_type(field)
            source.fields.append(source_field)

        if extract_rule.source_query:
            source.query = extract_rule.source_query
        elif extract_rule.extract_condition:
            source.query = f"SELECT * FROM {extract_rule.source_table} WHERE {extract_rule.extract_condition}"

        mapping.sources.append(source)

        # è½¬æ¢è½¬æ¢è§„åˆ™
        prev_output = source
        for transform_rule_id in process.transform_rule_ids:
            transform_rule = find_transform_rule(etl_data, transform_rule_id)

            if transform_rule.rule_type == "Mapping":
                # å­—æ®µæ˜ å°„è½¬æ¢
                transformation = InformaticaExpression()
                transformation.name = f"{transform_rule.rule_name}_Expression"

                for i, source_field in enumerate(transform_rule.source_fields):
                    target_field = InformaticaField()
                    target_field.name = transform_rule.target_fields[i]
                    target_field.data_type = infer_data_type(transform_rule.target_fields[i])
                    target_field.expression = f"{prev_output.name}.{source_field}"
                    transformation.fields.append(target_field)

                mapping.transformations.append(transformation)
                prev_output = transformation

            elif transform_rule.rule_type == "Calculation":
                # è®¡ç®—è½¬æ¢
                transformation = InformaticaExpression()
                transformation.name = f"{transform_rule.rule_name}_Expression"

                for target_field in transform_rule.target_fields:
                    field = InformaticaField()
                    field.name = target_field
                    field.expression = extract_calculation_expression(transform_rule.transform_logic, target_field)
                    transformation.fields.append(field)

                mapping.transformations.append(transformation)
                prev_output = transformation

        # è½¬æ¢åŠ è½½ç­–ç•¥
        load_strategy = find_load_strategy(etl_data, process.load_strategy_id)
        target_table = find_target_table(etl_data, load_strategy.table_id)

        target = InformaticaTarget()
        target.name = target_table.table_name
        target.type = "Relational"
        target.connection = create_target_connection(workflow, target_table).name
        target.load_mode = map_load_mode_to_informatica(load_strategy)

        # è½¬æ¢å­—æ®µ
        for field_name, field_type in target_table.table_structure.items():
            target_field = InformaticaTargetField()
            target_field.name = field_name
            target_field.data_type = map_data_type_to_informatica(field_type)
            target.fields.append(target_field)

        mapping.targets.append(target)

        # åˆ›å»ºè¿æ¥
        create_mapping_links(mapping, prev_output, target)

        workflow.mappings.append(mapping)

        # åˆ›å»ºä¼šè¯
        session = InformaticaSession()
        session.name = f"{process.process_name}_Session"
        session.mapping = mapping.name
        session.source_connection = connection.name
        session.target_connection = target.connection
        session.commit_interval = 10000
        workflow.sessions.append(session)

        # åˆ›å»ºå·¥ä½œæµä»»åŠ¡
        task = InformaticaTask()
        task.name = f"{process.process_name}_Task"
        task.type = "Session"
        task.session = session.name
        workflow.tasks.append(task)

    return workflow
```

---

## 4. æ¡ˆä¾‹3ï¼šå¢é‡ETLæµç¨‹

### 4.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
æ„å»ºå¢é‡ETLæµç¨‹ï¼Œæ”¯æŒåŸºäºæ—¶é—´æˆ³çš„å¢é‡æ•°æ®æå–å’ŒåŠ è½½ã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒå¢é‡æ•°æ®æå–
- æ”¯æŒå¢é‡æ•°æ®åŠ è½½
- æ”¯æŒå¢é‡çŠ¶æ€ç®¡ç†

### 4.2 å®ç°ä»£ç 

```python
def execute_incremental_etl(etl_data: ETLSchema, process_id: str) -> ExecutionResult:
    """æ‰§è¡Œå¢é‡ETLæµç¨‹"""
    process = find_process(etl_data, process_id)
    extract_rule = find_extract_rule(etl_data, process.extract_rule_id)
    incremental_extract = find_incremental_extract(etl_data, extract_rule.rule_id)

    # è·å–ä¸Šæ¬¡æå–å€¼
    last_extract_value = incremental_extract.last_extract_value
    last_extract_time = incremental_extract.last_extract_time

    # æ‰§è¡Œæå–
    if incremental_extract.incremental_strategy == "Timestamp":
        extract_condition = f"{incremental_extract.incremental_field} > '{last_extract_time}'"
    elif incremental_extract.incremental_strategy == "Sequence":
        extract_condition = f"{incremental_extract.incremental_field} > {last_extract_value}"
    elif incremental_extract.incremental_strategy == "Change_Data_Capture":
        extract_condition = f"change_type IN ('INSERT', 'UPDATE')"

    # æ›´æ–°æå–æ¡ä»¶
    extract_rule.extract_condition = extract_condition

    # æ‰§è¡ŒETL
    result = execute_etl_process(etl_data, process_id)

    # æ›´æ–°å¢é‡çŠ¶æ€
    if result.status == "Completed":
        # è·å–æœ¬æ¬¡æå–çš„æœ€å¤§å€¼
        new_max_value = get_max_incremental_value(
            extract_rule.connection_id,
            incremental_extract.incremental_field,
            extract_condition
        )

        # æ›´æ–°å¢é‡æå–è®°å½•
        incremental_extract.last_extract_value = str(new_max_value)
        incremental_extract.last_extract_time = datetime.now()

    return result
```

---

## 5. æ¡ˆä¾‹4ï¼šæ•°æ®è´¨é‡æ£€æŸ¥ETL

### 5.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
åœ¨ETLæµç¨‹ä¸­æ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥æ­¥éª¤ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒæ•°æ®è´¨é‡æ£€æŸ¥
- æ”¯æŒæ•°æ®è´¨é‡æŠ¥å‘Š
- æ”¯æŒæ•°æ®è´¨é‡ä¿®å¤

### 5.2 å®ç°ä»£ç 

```python
def add_data_quality_check(etl_data: ETLSchema, process_id: str, quality_rules: List[QualityRule]):
    """æ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥"""
    process = find_process(etl_data, process_id)

    # åˆ›å»ºæ•°æ®è´¨é‡æ£€æŸ¥è½¬æ¢è§„åˆ™
    for quality_rule in quality_rules:
        transform_rule = TransformRule()
        transform_rule.rule_id = f"RULE-QC-{quality_rule.rule_id}"
        transform_rule.rule_name = f"æ•°æ®è´¨é‡æ£€æŸ¥-{quality_rule.rule_name}"
        transform_rule.rule_type = "Validate"
        transform_rule.source_fields = quality_rule.fields
        transform_rule.target_fields = [f"{field}_valid" for field in quality_rule.fields]
        transform_rule.transform_logic = quality_rule.validation_rule

        # æ·»åŠ æ•°æ®æ¸…æ´—è§„åˆ™
        if quality_rule.cleaning_enabled:
            cleaning = DataCleaning()
            cleaning.cleaning_id = f"CLEAN-{quality_rule.rule_id}"
            cleaning.rule_id = transform_rule.rule_id
            cleaning.cleaning_type = quality_rule.cleaning_type
            cleaning.cleaning_rule = quality_rule.cleaning_rule
            transform_rule.data_cleaning.append(cleaning)

        etl_data.transform.transform_rules.append(transform_rule)
        process.transform_rule_ids.append(transform_rule.rule_id)

    return etl_data

def execute_etl_with_quality_check(etl_data: ETLSchema, process_id: str) -> ExecutionResult:
    """æ‰§è¡Œå¸¦æ•°æ®è´¨é‡æ£€æŸ¥çš„ETL"""
    process = find_process(etl_data, process_id)

    # æ‰§è¡Œæå–
    extract_result = execute_extract(etl_data, process.extract_rule_id)

    if extract_result.status != "Success":
        return ExecutionResult(status="Failed", error=extract_result.error)

    # æ‰§è¡Œè½¬æ¢å’Œè´¨é‡æ£€æŸ¥
    for transform_rule_id in process.transform_rule_ids:
        transform_rule = find_transform_rule(etl_data, transform_rule_id)

        # æ‰§è¡Œè½¬æ¢
        transform_result = execute_transform(transform_rule, extract_result.data)

        if transform_result.status != "Success":
            return ExecutionResult(status="Failed", error=transform_result.error)

        # æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
        if transform_rule.rule_type == "Validate":
            quality_result = execute_quality_check(transform_rule, transform_result.data)

            if quality_result.passed:
                # æ‰§è¡Œæ•°æ®æ¸…æ´—
                if transform_rule.data_cleaning:
                    for cleaning in transform_rule.data_cleaning:
                        transform_result.data = execute_cleaning(cleaning, transform_result.data)
            else:
                # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
                quality_report = generate_quality_report(quality_result)
                log_quality_issues(quality_report)

                # æ ¹æ®é”™è¯¯å¤„ç†ç­–ç•¥å†³å®šæ˜¯å¦ç»§ç»­
                if quality_result.error_action == "Stop":
                    return ExecutionResult(status="Failed", error="Data quality check failed")

        extract_result.data = transform_result.data

    # æ‰§è¡ŒåŠ è½½
    load_result = execute_load(etl_data, process.load_strategy_id, extract_result.data)

    return ExecutionResult(
        status=load_result.status,
        rows_extracted=extract_result.rows_count,
        rows_transformed=len(extract_result.data),
        rows_loaded=load_result.rows_count
    )
```

---

## 6. æ¡ˆä¾‹5ï¼šETLæ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ

### 6.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
ETLæ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿï¼Œæ”¯æŒETLå…ƒæ•°æ®å­˜å‚¨ã€æŸ¥è¯¢ã€åˆ†æã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒETLå…ƒæ•°æ®å­˜å‚¨
- æ”¯æŒETLæ‰§è¡Œå†å²æŸ¥è¯¢
- æ”¯æŒETLæ€§èƒ½åˆ†æ

### 6.2 å®ç°ä»£ç 

```python
def store_etl_data(etl_data: ETLSchema, conn):
    """å­˜å‚¨ETLæ•°æ®åˆ°PostgreSQL"""
    cursor = conn.cursor()

    # å­˜å‚¨æ•°æ®æºè¿æ¥
    for connection in etl_data.extract.data_source_connections:
        cursor.execute("""
            INSERT INTO data_source_connections
            (connection_id, connection_name, connection_type, connection_string, connection_parameters, is_active)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (connection_id) DO UPDATE SET
            connection_name = EXCLUDED.connection_name,
            connection_string = EXCLUDED.connection_string,
            connection_parameters = EXCLUDED.connection_parameters,
            is_active = EXCLUDED.is_active,
            updated_at = CURRENT_TIMESTAMP
        """, (connection.connection_id, connection.connection_name,
              connection.connection_type, connection.connection_string,
              json.dumps(connection.connection_parameters), connection.is_active))

    # å­˜å‚¨æå–è§„åˆ™
    for rule in etl_data.extract.extract_rules:
        cursor.execute("""
            INSERT INTO extract_rules
            (rule_id, connection_id, source_table, source_query, extract_condition, extract_fields, extract_frequency)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (rule_id) DO UPDATE SET
            source_table = EXCLUDED.source_table,
            source_query = EXCLUDED.source_query,
            extract_condition = EXCLUDED.extract_condition,
            extract_fields = EXCLUDED.extract_fields,
            extract_frequency = EXCLUDED.extract_frequency
        """, (rule.rule_id, rule.connection_id, rule.source_table,
              rule.source_query, rule.extract_condition,
              rule.extract_fields, rule.extract_frequency))

    # å­˜å‚¨è½¬æ¢è§„åˆ™
    for rule in etl_data.transform.transform_rules:
        cursor.execute("""
            INSERT INTO transform_rules
            (rule_id, rule_name, rule_type, source_fields, target_fields, transform_logic, transform_parameters)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (rule_id) DO UPDATE SET
            rule_name = EXCLUDED.rule_name,
            rule_type = EXCLUDED.rule_type,
            source_fields = EXCLUDED.source_fields,
            target_fields = EXCLUDED.target_fields,
            transform_logic = EXCLUDED.transform_logic,
            transform_parameters = EXCLUDED.transform_parameters
        """, (rule.rule_id, rule.rule_name, rule.rule_type,
              rule.source_fields, rule.target_fields,
              rule.transform_logic, json.dumps(rule.transform_parameters)))

    # å­˜å‚¨ç›®æ ‡è¡¨
    for table in etl_data.load.target_tables:
        cursor.execute("""
            INSERT INTO target_tables
            (table_id, table_name, table_schema, table_type, table_structure, primary_key)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (table_id) DO UPDATE SET
            table_name = EXCLUDED.table_name,
            table_schema = EXCLUDED.table_schema,
            table_type = EXCLUDED.table_type,
            table_structure = EXCLUDED.table_structure,
            primary_key = EXCLUDED.primary_key
        """, (table.table_id, table.table_name, table.table_schema,
              table.table_type, json.dumps(table.table_structure), table.primary_key))

    # å­˜å‚¨åŠ è½½ç­–ç•¥
    for strategy in etl_data.load.load_strategies:
        cursor.execute("""
            INSERT INTO load_strategies
            (strategy_id, table_id, strategy_type, strategy_parameters, load_frequency)
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT (strategy_id) DO UPDATE SET
            strategy_type = EXCLUDED.strategy_type,
            strategy_parameters = EXCLUDED.strategy_parameters,
            load_frequency = EXCLUDED.load_frequency
        """, (strategy.strategy_id, strategy.table_id,
              strategy.strategy_type, json.dumps(strategy.strategy_parameters),
              strategy.load_frequency))

    # å­˜å‚¨ETLæµç¨‹å®šä¹‰
    for process in etl_data.etl_process.process_definitions:
        cursor.execute("""
            INSERT INTO etl_process_definitions
            (process_id, process_name, process_type, extract_rule_id, transform_rule_ids, load_strategy_id, process_dependencies, process_parameters)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (process_id) DO UPDATE SET
            process_name = EXCLUDED.process_name,
            process_type = EXCLUDED.process_type,
            extract_rule_id = EXCLUDED.extract_rule_id,
            transform_rule_ids = EXCLUDED.transform_rule_ids,
            load_strategy_id = EXCLUDED.load_strategy_id,
            process_dependencies = EXCLUDED.process_dependencies,
            process_parameters = EXCLUDED.process_parameters
        """, (process.process_id, process.process_name, process.process_type,
              process.extract_rule_id, process.transform_rule_ids,
              process.load_strategy_id, process.process_dependencies,
              json.dumps(process.process_parameters)))

    conn.commit()

def generate_etl_report(conn):
    """ç”ŸæˆETLæŠ¥è¡¨"""
    cursor = conn.cursor()

    # æŸ¥è¯¢ETLæµç¨‹æ‰§è¡Œç»Ÿè®¡
    cursor.execute("""
        SELECT
            epd.process_name,
            epd.process_type,
            COUNT(eh.execution_id) as total_executions,
            SUM(CASE WHEN eh.execution_status = 'Completed' THEN 1 ELSE 0 END) as success_count,
            SUM(CASE WHEN eh.execution_status = 'Failed' THEN 1 ELSE 0 END) as failed_count,
            AVG(eh.rows_loaded) as avg_rows_loaded,
            AVG(EXTRACT(EPOCH FROM (eh.execution_end_time - eh.execution_start_time))) as avg_duration_seconds
        FROM etl_process_definitions epd
        LEFT JOIN etl_execution_history eh ON epd.process_id = eh.process_id
        WHERE eh.execution_start_time >= CURRENT_DATE - INTERVAL '30 days'
        GROUP BY epd.process_id, epd.process_name, epd.process_type
        ORDER BY total_executions DESC
    """)

    process_statistics = cursor.fetchall()

    # æŸ¥è¯¢æ•°æ®æºè¿æ¥ä½¿ç”¨æƒ…å†µ
    cursor.execute("""
        SELECT
            dsc.connection_type,
            COUNT(DISTINCT dsc.connection_id) as connection_count,
            COUNT(DISTINCT er.rule_id) as extract_rule_count,
            COUNT(DISTINCT epd.process_id) as process_count
        FROM data_source_connections dsc
        LEFT JOIN extract_rules er ON dsc.connection_id = er.connection_id
        LEFT JOIN etl_process_definitions epd ON er.rule_id = epd.extract_rule_id
        WHERE dsc.is_active = TRUE
        GROUP BY dsc.connection_type
        ORDER BY connection_count DESC
    """)

    connection_usage = cursor.fetchall()

    return {
        "process_statistics": process_statistics,
        "connection_usage": connection_usage
    }
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `04_Transformation.md` - è½¬æ¢ä½“ç³»

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
