# ETL Schemaå®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [ETL Schemaå®è·µæ¡ˆä¾‹](#etl-schemaå®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¡ˆä¾‹æ¦‚è¿°](#1-æ¡ˆä¾‹æ¦‚è¿°)
  - [2. æ¡ˆä¾‹1ï¼šä¼ä¸šé”€å”®æ•°æ®ETLæµç¨‹ç³»ç»Ÿ](#2-æ¡ˆä¾‹1ä¼ä¸šé”€å”®æ•°æ®etlæµç¨‹ç³»ç»Ÿ)
    - [2.1 ä¸šåŠ¡èƒŒæ™¯](#21-ä¸šåŠ¡èƒŒæ™¯)
    - [2.2 æŠ€æœ¯æŒ‘æˆ˜](#22-æŠ€æœ¯æŒ‘æˆ˜)
    - [2.3 è§£å†³æ–¹æ¡ˆ](#23-è§£å†³æ–¹æ¡ˆ)
    - [2.4 å®Œæ•´ä»£ç å®ç°](#24-å®Œæ•´ä»£ç å®ç°)
    - [2.5 æ•ˆæœè¯„ä¼°](#25-æ•ˆæœè¯„ä¼°)
  - [3. æ¡ˆä¾‹2ï¼šETLåˆ°Informaticaè½¬æ¢](#3-æ¡ˆä¾‹2etlåˆ°informaticaè½¬æ¢)
    - [3.1 åœºæ™¯æè¿°](#31-åœºæ™¯æè¿°)
    - [3.2 å®ç°ä»£ç ](#32-å®ç°ä»£ç )
  - [4. æ¡ˆä¾‹3ï¼šå¢é‡ETLæµç¨‹](#4-æ¡ˆä¾‹3å¢é‡etlæµç¨‹)
    - [4.1 åœºæ™¯æè¿°](#41-åœºæ™¯æè¿°)
    - [4.2 å®ç°ä»£ç ](#42-å®ç°ä»£ç )
  - [5. æ¡ˆä¾‹4ï¼šæ•°æ®è´¨é‡æ£€æŸ¥ETL](#5-æ¡ˆä¾‹4æ•°æ®è´¨é‡æ£€æŸ¥etl)
    - [5.1 åœºæ™¯æè¿°](#51-åœºæ™¯æè¿°)
    - [5.2 å®ç°ä»£ç ](#52-å®ç°ä»£ç )
  - [6. æ¡ˆä¾‹5ï¼šETLæ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ](#6-æ¡ˆä¾‹5etlæ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ)
    - [6.1 åœºæ™¯æè¿°](#61-åœºæ™¯æè¿°)
    - [6.2 å®ç°ä»£ç ](#62-å®ç°ä»£ç )

---

## 1. æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›ETL Schemaåœ¨å®é™…ä¼ä¸šåº”ç”¨ä¸­çš„å®è·µæ¡ˆä¾‹ï¼Œæ¶µç›–é”€å”®æ•°æ®ETLã€å¢é‡ETLã€æ•°æ®è´¨é‡æ£€æŸ¥ç­‰çœŸå®åœºæ™¯ã€‚

**æ¡ˆä¾‹ç±»å‹**ï¼š

1. **ä¼ä¸šé”€å”®æ•°æ®ETLæµç¨‹ç³»ç»Ÿ**ï¼šé”€å”®æ•°æ®ETLæµç¨‹
2. **BIåˆ°Informaticaè½¬æ¢å·¥å…·**ï¼šETL Schemaåˆ°Informaticaè½¬æ¢
3. **å¢é‡ETLæµç¨‹ç³»ç»Ÿ**ï¼šå¢é‡æ•°æ®ETL
4. **æ•°æ®è´¨é‡æ£€æŸ¥ETLç³»ç»Ÿ**ï¼šæ•°æ®è´¨é‡æ£€æŸ¥
5. **ETLæ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ**ï¼šETLæ•°æ®åˆ†æå’Œç›‘æ§

**å‚è€ƒä¼ä¸šæ¡ˆä¾‹**ï¼š

- **Informaticaå®˜æ–¹**ï¼šInformatica ETLæœ€ä½³å®è·µ
- **Talendå®˜æ–¹**ï¼šTalend ETLè®¾è®¡æŒ‡å—

---

## 2. æ¡ˆä¾‹1ï¼šä¼ä¸šé”€å”®æ•°æ®ETLæµç¨‹ç³»ç»Ÿ

### 2.1 ä¸šåŠ¡èƒŒæ™¯

**ä¼ä¸šèƒŒæ™¯**ï¼š
æŸé›¶å”®å…¬å¸éœ€è¦æ„å»ºé”€å”®æ•°æ®ETLæµç¨‹ï¼Œä»å¤šä¸ªæºç³»ç»Ÿæå–é”€å”®æ•°æ®ï¼Œè¿›è¡Œæ•°æ®è½¬æ¢å’Œæ¸…æ´—ï¼ŒåŠ è½½åˆ°æ•°æ®ä»“åº“ï¼Œä¸ºä¸šåŠ¡åˆ†ææä¾›æ•°æ®æ”¯æŒã€‚

**ä¸šåŠ¡ç—›ç‚¹**ï¼š

1. **æ•°æ®åˆ†æ•£**ï¼šæ•°æ®åˆ†æ•£åœ¨å¤šä¸ªæºç³»ç»Ÿä¸­
2. **æ•°æ®è´¨é‡å·®**ï¼šæºæ•°æ®è´¨é‡ä¸ä¸€è‡´
3. **ETLæµç¨‹å¤æ‚**ï¼šETLæµç¨‹å¤æ‚ä¸”å®¹æ˜“å‡ºé”™
4. **æ€§èƒ½é—®é¢˜**ï¼šETLæ€§èƒ½æ— æ³•æ»¡è¶³éœ€æ±‚

**ä¸šåŠ¡ç›®æ ‡**ï¼š

- ç»Ÿä¸€æ•°æ®æå–
- æé«˜æ•°æ®è´¨é‡
- ç®€åŒ–ETLæµç¨‹
- æé«˜ETLæ€§èƒ½

### 2.2 æŠ€æœ¯æŒ‘æˆ˜

1. **æ•°æ®æå–**ï¼šä»å¤šä¸ªæºç³»ç»Ÿæå–æ•°æ®
2. **æ•°æ®è½¬æ¢**ï¼šå¤æ‚çš„æ•°æ®è½¬æ¢é€»è¾‘
3. **æ•°æ®æ¸…æ´—**ï¼šæ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¸…æ´—
4. **æ•°æ®åŠ è½½**ï¼šé«˜æ•ˆçš„æ•°æ®åŠ è½½

### 2.3 è§£å†³æ–¹æ¡ˆ

**ä½¿ç”¨Schemaå®šä¹‰é”€å”®æ•°æ®ETLæµç¨‹ç³»ç»Ÿ**ï¼š

### 2.4 å®Œæ•´ä»£ç å®ç°

**é”€å”®æ•°æ®ETLæµç¨‹Schemaï¼ˆå®Œæ•´ç¤ºä¾‹ï¼‰**ï¼š

```python
#!/usr/bin/env python3
"""
ETLæµç¨‹Schemaå®ç°
"""

from typing import Dict, List, Optional, Callable
from datetime import datetime, date
from decimal import Decimal
from dataclasses import dataclass, field
from enum import Enum
import pandas as pd

class ExtractFrequency(str, Enum):
    """æå–é¢‘ç‡"""
    REAL_TIME = "RealTime"
    HOURLY = "Hourly"
    DAILY = "Daily"
    WEEKLY = "Weekly"
    MONTHLY = "Monthly"

class TransformType(str, Enum):
    """è½¬æ¢ç±»å‹"""
    MAPPING = "Mapping"
    CALCULATION = "Calculation"
    AGGREGATION = "Aggregation"
    FILTER = "Filter"

@dataclass
class ExtractRule:
    """æå–è§„åˆ™"""
    rule_id: str
    connection_id: str
    source_table: str
    extract_condition: str
    extract_fields: List[str] = field(default_factory=list)
    extract_frequency: ExtractFrequency = ExtractFrequency.DAILY
    last_extract_date: Optional[datetime] = None

    def execute(self, connection) -> pd.DataFrame:
        """æ‰§è¡Œæå–"""
        query = f"SELECT {', '.join(self.extract_fields)} FROM {self.source_table}"
        if self.extract_condition:
            query += f" WHERE {self.extract_condition}"

        df = pd.read_sql(query, connection)
        self.last_extract_date = datetime.now()
        return df

@dataclass
class TransformRule:
    """è½¬æ¢è§„åˆ™"""
    rule_id: str
    rule_name: str
    rule_type: TransformType
    source_fields: List[str] = field(default_factory=list)
    target_fields: List[str] = field(default_factory=list)
    transform_function: Optional[Callable] = None
    transform_expression: Optional[str] = None

    def execute(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ‰§è¡Œè½¬æ¢"""
        if self.rule_type == TransformType.MAPPING:
            # å­—æ®µæ˜ å°„
            mapping = dict(zip(self.source_fields, self.target_fields))
            df = df.rename(columns=mapping)
        elif self.rule_type == TransformType.CALCULATION:
            # è®¡ç®—è½¬æ¢
            if self.transform_function:
                df = self.transform_function(df)
        elif self.rule_type == TransformType.AGGREGATION:
            # èšåˆè½¬æ¢
            if self.transform_expression:
                df = df.groupby(self.source_fields).agg(eval(self.transform_expression))
        elif self.rule_type == TransformType.FILTER:
            # è¿‡æ»¤è½¬æ¢
            if self.transform_expression:
                df = df.query(self.transform_expression)

        return df

@dataclass
class LoadRule:
    """åŠ è½½è§„åˆ™"""
    rule_id: str
    target_table: str
    load_mode: str = "INSERT"  # INSERT, UPDATE, UPSERT
    load_fields: List[str] = field(default_factory=list)
    key_fields: List[str] = field(default_factory=list)

    def execute(self, df: pd.DataFrame, connection):
        """æ‰§è¡ŒåŠ è½½"""
        if self.load_mode == "INSERT":
            df.to_sql(self.target_table, connection, if_exists='append', index=False)
        elif self.load_mode == "UPDATE":
            # æ›´æ–°é€»è¾‘
            for _, row in df.iterrows():
                update_query = f"UPDATE {self.target_table} SET "
                set_clauses = [f"{field} = %s" for field in self.load_fields]
                update_query += ", ".join(set_clauses)
                where_clauses = [f"{field} = %s" for field in self.key_fields]
                update_query += f" WHERE {' AND '.join(where_clauses)}"
                # æ‰§è¡Œæ›´æ–°
        elif self.load_mode == "UPSERT":
            # æ’å…¥æˆ–æ›´æ–°é€»è¾‘
            df.to_sql(self.target_table, connection, if_exists='replace', index=False)

@dataclass
class ETLProcess:
    """ETLæµç¨‹"""
    process_id: str
    process_name: str
    extract_rule: ExtractRule
    transform_rules: List[TransformRule] = field(default_factory=list)
    load_rule: LoadRule = None
    enabled: bool = True
    last_run_time: Optional[datetime] = None
    last_run_status: str = "Pending"

    def add_transform_rule(self, rule: TransformRule):
        """æ·»åŠ è½¬æ¢è§„åˆ™"""
        self.transform_rules.append(rule)

    def execute(self, source_connection, target_connection) -> tuple[bool, str]:
        """æ‰§è¡ŒETLæµç¨‹"""
        try:
            # æå–
            df = self.extract_rule.execute(source_connection)
            if df.empty:
                return True, "No data to process"

            # è½¬æ¢
            for transform_rule in self.transform_rules:
                df = transform_rule.execute(df)

            # åŠ è½½
            if self.load_rule:
                self.load_rule.execute(df, target_connection)

            self.last_run_time = datetime.now()
            self.last_run_status = "Success"
            return True, "ETL process completed successfully"

        except Exception as e:
            self.last_run_time = datetime.now()
            self.last_run_status = f"Failed: {str(e)}"
            return False, str(e)

@dataclass
class SalesDataETL:
    """é”€å”®æ•°æ®ETL"""
    etl_process: ETLProcess

    @classmethod
    def create_default(cls) -> 'SalesDataETL':
        """åˆ›å»ºé»˜è®¤é”€å”®æ•°æ®ETL"""
        extract_rule = ExtractRule(
            rule_id="RULE-SALES-EXTRACT",
            connection_id="CONN-SALES-DB",
            source_table="sales_transactions",
            extract_condition="sale_date >= :last_extract_date",
            extract_fields=["sale_id", "sale_date", "customer_id", "product_id",
                          "sale_amount", "sale_quantity"],
            extract_frequency=ExtractFrequency.DAILY
        )

        transform_rule = TransformRule(
            rule_id="RULE-SALES-TRANSFORM",
            rule_name="é”€å”®æ•°æ®è½¬æ¢",
            rule_type=TransformType.MAPPING,
            source_fields=["sale_id", "sale_date", "customer_id", "product_id",
                          "sale_amount", "sale_quantity"],
            target_fields=["sale_id", "sale_date", "customer_id", "product_id",
                          "sale_amount", "sale_quantity"]
        )

        load_rule = LoadRule(
            rule_id="RULE-SALES-LOAD",
            target_table="fact_sales",
            load_mode="INSERT",
            load_fields=["sale_id", "sale_date", "customer_id", "product_id",
                        "sale_amount", "sale_quantity"]
        )

        etl_process = ETLProcess(
            process_id="ETL-SALES-001",
            process_name="é”€å”®æ•°æ®ETLæµç¨‹",
            extract_rule=extract_rule,
            load_rule=load_rule
        )
        etl_process.add_transform_rule(transform_rule)

        return cls(etl_process=etl_process)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åˆ›å»ºé”€å”®æ•°æ®ETL
    sales_etl = SalesDataETL.create_default()

    print(f"ETLæµç¨‹: {sales_etl.etl_process.process_name}")
    print(f"æå–è§„åˆ™: {sales_etl.etl_process.extract_rule.rule_id}")
    print(f"è½¬æ¢è§„åˆ™æ•°é‡: {len(sales_etl.etl_process.transform_rules)}")
    print(f"åŠ è½½è§„åˆ™: {sales_etl.etl_process.load_rule.rule_id}")
```

### 2.5 æ•ˆæœè¯„ä¼°

**æ€§èƒ½æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | æ”¹è¿›å‰ | æ”¹è¿›å | æå‡ |
|------|--------|--------|------|
| ETLå¤„ç†æ—¶é—´ | 2å°æ—¶ | 30åˆ†é’Ÿ | 4xæå‡ |
| æ•°æ®è´¨é‡ | 85% | 98% | 13%æå‡ |
| ETLæµç¨‹ç¨³å®šæ€§ | 90% | 99% | 9%æå‡ |
| æ•°æ®åŠ è½½æ€§èƒ½ | ä½ | é«˜ | æ˜¾è‘—æå‡ |

**ä¸šåŠ¡ä»·å€¼**ï¼š

1. **ETLæ•ˆç‡æå‡**ï¼šæé«˜ETLå¤„ç†æ•ˆç‡
2. **æ•°æ®è´¨é‡æé«˜**ï¼šæé«˜æ•°æ®è´¨é‡
3. **æµç¨‹ç¨³å®šæ€§**ï¼šæé«˜ETLæµç¨‹ç¨³å®šæ€§
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šä¼˜åŒ–æ•°æ®åŠ è½½æ€§èƒ½

**ç»éªŒæ•™è®­**ï¼š

1. ETLæµç¨‹è®¾è®¡å¾ˆé‡è¦
2. æ•°æ®è´¨é‡æ£€æŸ¥å¿…é¡»å®Œå–„
3. å¢é‡æå–éœ€è¦ä¼˜åŒ–
4. é”™è¯¯å¤„ç†éœ€è¦å®Œå–„

**å‚è€ƒæ¡ˆä¾‹**ï¼š

- [Informatica ETLæœ€ä½³å®è·µ](https://www.informatica.com/)
- [Talend ETLè®¾è®¡æŒ‡å—](https://www.talend.com/)

---

## 3. æ¡ˆä¾‹2ï¼šETLåˆ°Informaticaè½¬æ¢

### 3.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
å°†ETL Schemaè½¬æ¢ä¸ºInformatica Workflowæ ¼å¼ï¼Œç”¨äºInformaticaæ‰§è¡Œã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒè‡ªåŠ¨è½¬æ¢åˆ°Informatica
- æ”¯æŒæ•°æ®æºè¿æ¥é…ç½®
- æ”¯æŒè½¬æ¢é€»è¾‘é…ç½®

### 3.2 å®ç°ä»£ç 

```python
def convert_etl_to_informatica_complete(etl_data: ETLSchema) -> InformaticaWorkflow:
    """å®Œæ•´è½¬æ¢ETL Schemaåˆ°Informatica"""
    workflow = InformaticaWorkflow()
    workflow.name = "ETL_Workflow"

    # è½¬æ¢æ•°æ®æºè¿æ¥
    connections_map = {}
    for connection in etl_data.extract.data_source_connections:
        infa_connection = InformaticaConnection()
        infa_connection.name = connection.connection_name
        infa_connection.type = map_connection_type_to_informatica(connection.connection_type)
        infa_connection.connection_string = connection.connection_string

        # è½¬æ¢è¿æ¥å‚æ•°
        if connection.connection_type == "Database":
            infa_connection.properties = {
                "database_type": connection.connection_parameters.get("database_type", "Oracle"),
                "host": connection.connection_parameters.get("host", ""),
                "port": connection.connection_parameters.get("port", "1521"),
                "database_name": connection.connection_parameters.get("database_name", ""),
                "username": connection.authentication.credentials.get("username", ""),
                "password": connection.authentication.credentials.get("password", "")
            }
        elif connection.connection_type == "File":
            infa_connection.properties = {
                "file_type": connection.connection_parameters.get("file_type", "Delimited"),
                "file_path": connection.connection_string
            }

        workflow.connections.append(infa_connection)
        connections_map[connection.connection_id] = infa_connection

    # è½¬æ¢ETLæµç¨‹
    for process in etl_data.etl_process.process_definitions:
        # åˆ›å»ºæ˜ å°„
        mapping = InformaticaMapping()
        mapping.name = f"{process.process_name}_Mapping"

        # è½¬æ¢æå–è§„åˆ™
        extract_rule = find_extract_rule(etl_data, process.extract_rule_id)
        connection = connections_map[extract_rule.connection_id]

        source = InformaticaSource()
        source.name = extract_rule.source_table or "Source"
        source.connection = connection.name
        source.type = "Relational"

        # è½¬æ¢å­—æ®µ
        for field in extract_rule.extract_fields:
            source_field = InformaticaSourceField()
            source_field.name = field
            source_field.data_type = infer_data_type(field)
            source.fields.append(source_field)

        if extract_rule.source_query:
            source.query = extract_rule.source_query
        elif extract_rule.extract_condition:
            source.query = f"SELECT * FROM {extract_rule.source_table} WHERE {extract_rule.extract_condition}"

        mapping.sources.append(source)

        # è½¬æ¢è½¬æ¢è§„åˆ™
        prev_output = source
        for transform_rule_id in process.transform_rule_ids:
            transform_rule = find_transform_rule(etl_data, transform_rule_id)

            if transform_rule.rule_type == "Mapping":
                # å­—æ®µæ˜ å°„è½¬æ¢
                transformation = InformaticaExpression()
                transformation.name = f"{transform_rule.rule_name}_Expression"

                for i, source_field in enumerate(transform_rule.source_fields):
                    target_field = InformaticaField()
                    target_field.name = transform_rule.target_fields[i]
                    target_field.data_type = infer_data_type(transform_rule.target_fields[i])
                    target_field.expression = f"{prev_output.name}.{source_field}"
                    transformation.fields.append(target_field)

                mapping.transformations.append(transformation)
                prev_output = transformation

            elif transform_rule.rule_type == "Calculation":
                # è®¡ç®—è½¬æ¢
                transformation = InformaticaExpression()
                transformation.name = f"{transform_rule.rule_name}_Expression"

                for target_field in transform_rule.target_fields:
                    field = InformaticaField()
                    field.name = target_field
                    field.expression = extract_calculation_expression(transform_rule.transform_logic, target_field)
                    transformation.fields.append(field)

                mapping.transformations.append(transformation)
                prev_output = transformation

        # è½¬æ¢åŠ è½½ç­–ç•¥
        load_strategy = find_load_strategy(etl_data, process.load_strategy_id)
        target_table = find_target_table(etl_data, load_strategy.table_id)

        target = InformaticaTarget()
        target.name = target_table.table_name
        target.type = "Relational"
        target.connection = create_target_connection(workflow, target_table).name
        target.load_mode = map_load_mode_to_informatica(load_strategy)

        # è½¬æ¢å­—æ®µ
        for field_name, field_type in target_table.table_structure.items():
            target_field = InformaticaTargetField()
            target_field.name = field_name
            target_field.data_type = map_data_type_to_informatica(field_type)
            target.fields.append(target_field)

        mapping.targets.append(target)

        # åˆ›å»ºè¿æ¥
        create_mapping_links(mapping, prev_output, target)

        workflow.mappings.append(mapping)

        # åˆ›å»ºä¼šè¯
        session = InformaticaSession()
        session.name = f"{process.process_name}_Session"
        session.mapping = mapping.name
        session.source_connection = connection.name
        session.target_connection = target.connection
        session.commit_interval = 10000
        workflow.sessions.append(session)

        # åˆ›å»ºå·¥ä½œæµä»»åŠ¡
        task = InformaticaTask()
        task.name = f"{process.process_name}_Task"
        task.type = "Session"
        task.session = session.name
        workflow.tasks.append(task)

    return workflow
```

---

## 4. æ¡ˆä¾‹3ï¼šå¢é‡ETLæµç¨‹

### 4.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
æ„å»ºå¢é‡ETLæµç¨‹ï¼Œæ”¯æŒåŸºäºæ—¶é—´æˆ³çš„å¢é‡æ•°æ®æå–å’ŒåŠ è½½ã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒå¢é‡æ•°æ®æå–
- æ”¯æŒå¢é‡æ•°æ®åŠ è½½
- æ”¯æŒå¢é‡çŠ¶æ€ç®¡ç†

### 4.2 å®ç°ä»£ç 

```python
def execute_incremental_etl(etl_data: ETLSchema, process_id: str) -> ExecutionResult:
    """æ‰§è¡Œå¢é‡ETLæµç¨‹"""
    process = find_process(etl_data, process_id)
    extract_rule = find_extract_rule(etl_data, process.extract_rule_id)
    incremental_extract = find_incremental_extract(etl_data, extract_rule.rule_id)

    # è·å–ä¸Šæ¬¡æå–å€¼
    last_extract_value = incremental_extract.last_extract_value
    last_extract_time = incremental_extract.last_extract_time

    # æ‰§è¡Œæå–
    if incremental_extract.incremental_strategy == "Timestamp":
        extract_condition = f"{incremental_extract.incremental_field} > '{last_extract_time}'"
    elif incremental_extract.incremental_strategy == "Sequence":
        extract_condition = f"{incremental_extract.incremental_field} > {last_extract_value}"
    elif incremental_extract.incremental_strategy == "Change_Data_Capture":
        extract_condition = f"change_type IN ('INSERT', 'UPDATE')"

    # æ›´æ–°æå–æ¡ä»¶
    extract_rule.extract_condition = extract_condition

    # æ‰§è¡ŒETL
    result = execute_etl_process(etl_data, process_id)

    # æ›´æ–°å¢é‡çŠ¶æ€
    if result.status == "Completed":
        # è·å–æœ¬æ¬¡æå–çš„æœ€å¤§å€¼
        new_max_value = get_max_incremental_value(
            extract_rule.connection_id,
            incremental_extract.incremental_field,
            extract_condition
        )

        # æ›´æ–°å¢é‡æå–è®°å½•
        incremental_extract.last_extract_value = str(new_max_value)
        incremental_extract.last_extract_time = datetime.now()

    return result
```

---

## 5. æ¡ˆä¾‹4ï¼šæ•°æ®è´¨é‡æ£€æŸ¥ETL

### 5.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
åœ¨ETLæµç¨‹ä¸­æ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥æ­¥éª¤ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒæ•°æ®è´¨é‡æ£€æŸ¥
- æ”¯æŒæ•°æ®è´¨é‡æŠ¥å‘Š
- æ”¯æŒæ•°æ®è´¨é‡ä¿®å¤

### 5.2 å®ç°ä»£ç 

```python
def add_data_quality_check(etl_data: ETLSchema, process_id: str, quality_rules: List[QualityRule]):
    """æ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥"""
    process = find_process(etl_data, process_id)

    # åˆ›å»ºæ•°æ®è´¨é‡æ£€æŸ¥è½¬æ¢è§„åˆ™
    for quality_rule in quality_rules:
        transform_rule = TransformRule()
        transform_rule.rule_id = f"RULE-QC-{quality_rule.rule_id}"
        transform_rule.rule_name = f"æ•°æ®è´¨é‡æ£€æŸ¥-{quality_rule.rule_name}"
        transform_rule.rule_type = "Validate"
        transform_rule.source_fields = quality_rule.fields
        transform_rule.target_fields = [f"{field}_valid" for field in quality_rule.fields]
        transform_rule.transform_logic = quality_rule.validation_rule

        # æ·»åŠ æ•°æ®æ¸…æ´—è§„åˆ™
        if quality_rule.cleaning_enabled:
            cleaning = DataCleaning()
            cleaning.cleaning_id = f"CLEAN-{quality_rule.rule_id}"
            cleaning.rule_id = transform_rule.rule_id
            cleaning.cleaning_type = quality_rule.cleaning_type
            cleaning.cleaning_rule = quality_rule.cleaning_rule
            transform_rule.data_cleaning.append(cleaning)

        etl_data.transform.transform_rules.append(transform_rule)
        process.transform_rule_ids.append(transform_rule.rule_id)

    return etl_data

def execute_etl_with_quality_check(etl_data: ETLSchema, process_id: str) -> ExecutionResult:
    """æ‰§è¡Œå¸¦æ•°æ®è´¨é‡æ£€æŸ¥çš„ETL"""
    process = find_process(etl_data, process_id)

    # æ‰§è¡Œæå–
    extract_result = execute_extract(etl_data, process.extract_rule_id)

    if extract_result.status != "Success":
        return ExecutionResult(status="Failed", error=extract_result.error)

    # æ‰§è¡Œè½¬æ¢å’Œè´¨é‡æ£€æŸ¥
    for transform_rule_id in process.transform_rule_ids:
        transform_rule = find_transform_rule(etl_data, transform_rule_id)

        # æ‰§è¡Œè½¬æ¢
        transform_result = execute_transform(transform_rule, extract_result.data)

        if transform_result.status != "Success":
            return ExecutionResult(status="Failed", error=transform_result.error)

        # æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
        if transform_rule.rule_type == "Validate":
            quality_result = execute_quality_check(transform_rule, transform_result.data)

            if quality_result.passed:
                # æ‰§è¡Œæ•°æ®æ¸…æ´—
                if transform_rule.data_cleaning:
                    for cleaning in transform_rule.data_cleaning:
                        transform_result.data = execute_cleaning(cleaning, transform_result.data)
            else:
                # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
                quality_report = generate_quality_report(quality_result)
                log_quality_issues(quality_report)

                # æ ¹æ®é”™è¯¯å¤„ç†ç­–ç•¥å†³å®šæ˜¯å¦ç»§ç»­
                if quality_result.error_action == "Stop":
                    return ExecutionResult(status="Failed", error="Data quality check failed")

        extract_result.data = transform_result.data

    # æ‰§è¡ŒåŠ è½½
    load_result = execute_load(etl_data, process.load_strategy_id, extract_result.data)

    return ExecutionResult(
        status=load_result.status,
        rows_extracted=extract_result.rows_count,
        rows_transformed=len(extract_result.data),
        rows_loaded=load_result.rows_count
    )
```

---

## 6. æ¡ˆä¾‹5ï¼šETLæ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ

### 6.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
ETLæ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿï¼Œæ”¯æŒETLå…ƒæ•°æ®å­˜å‚¨ã€æŸ¥è¯¢ã€åˆ†æã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒETLå…ƒæ•°æ®å­˜å‚¨
- æ”¯æŒETLæ‰§è¡Œå†å²æŸ¥è¯¢
- æ”¯æŒETLæ€§èƒ½åˆ†æ

### 6.2 å®ç°ä»£ç 

```python
def store_etl_data(etl_data: ETLSchema, conn):
    """å­˜å‚¨ETLæ•°æ®åˆ°PostgreSQL"""
    cursor = conn.cursor()

    # å­˜å‚¨æ•°æ®æºè¿æ¥
    for connection in etl_data.extract.data_source_connections:
        cursor.execute("""
            INSERT INTO data_source_connections
            (connection_id, connection_name, connection_type, connection_string, connection_parameters, is_active)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (connection_id) DO UPDATE SET
            connection_name = EXCLUDED.connection_name,
            connection_string = EXCLUDED.connection_string,
            connection_parameters = EXCLUDED.connection_parameters,
            is_active = EXCLUDED.is_active,
            updated_at = CURRENT_TIMESTAMP
        """, (connection.connection_id, connection.connection_name,
              connection.connection_type, connection.connection_string,
              json.dumps(connection.connection_parameters), connection.is_active))

    # å­˜å‚¨æå–è§„åˆ™
    for rule in etl_data.extract.extract_rules:
        cursor.execute("""
            INSERT INTO extract_rules
            (rule_id, connection_id, source_table, source_query, extract_condition, extract_fields, extract_frequency)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (rule_id) DO UPDATE SET
            source_table = EXCLUDED.source_table,
            source_query = EXCLUDED.source_query,
            extract_condition = EXCLUDED.extract_condition,
            extract_fields = EXCLUDED.extract_fields,
            extract_frequency = EXCLUDED.extract_frequency
        """, (rule.rule_id, rule.connection_id, rule.source_table,
              rule.source_query, rule.extract_condition,
              rule.extract_fields, rule.extract_frequency))

    # å­˜å‚¨è½¬æ¢è§„åˆ™
    for rule in etl_data.transform.transform_rules:
        cursor.execute("""
            INSERT INTO transform_rules
            (rule_id, rule_name, rule_type, source_fields, target_fields, transform_logic, transform_parameters)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (rule_id) DO UPDATE SET
            rule_name = EXCLUDED.rule_name,
            rule_type = EXCLUDED.rule_type,
            source_fields = EXCLUDED.source_fields,
            target_fields = EXCLUDED.target_fields,
            transform_logic = EXCLUDED.transform_logic,
            transform_parameters = EXCLUDED.transform_parameters
        """, (rule.rule_id, rule.rule_name, rule.rule_type,
              rule.source_fields, rule.target_fields,
              rule.transform_logic, json.dumps(rule.transform_parameters)))

    # å­˜å‚¨ç›®æ ‡è¡¨
    for table in etl_data.load.target_tables:
        cursor.execute("""
            INSERT INTO target_tables
            (table_id, table_name, table_schema, table_type, table_structure, primary_key)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (table_id) DO UPDATE SET
            table_name = EXCLUDED.table_name,
            table_schema = EXCLUDED.table_schema,
            table_type = EXCLUDED.table_type,
            table_structure = EXCLUDED.table_structure,
            primary_key = EXCLUDED.primary_key
        """, (table.table_id, table.table_name, table.table_schema,
              table.table_type, json.dumps(table.table_structure), table.primary_key))

    # å­˜å‚¨åŠ è½½ç­–ç•¥
    for strategy in etl_data.load.load_strategies:
        cursor.execute("""
            INSERT INTO load_strategies
            (strategy_id, table_id, strategy_type, strategy_parameters, load_frequency)
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT (strategy_id) DO UPDATE SET
            strategy_type = EXCLUDED.strategy_type,
            strategy_parameters = EXCLUDED.strategy_parameters,
            load_frequency = EXCLUDED.load_frequency
        """, (strategy.strategy_id, strategy.table_id,
              strategy.strategy_type, json.dumps(strategy.strategy_parameters),
              strategy.load_frequency))

    # å­˜å‚¨ETLæµç¨‹å®šä¹‰
    for process in etl_data.etl_process.process_definitions:
        cursor.execute("""
            INSERT INTO etl_process_definitions
            (process_id, process_name, process_type, extract_rule_id, transform_rule_ids, load_strategy_id, process_dependencies, process_parameters)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (process_id) DO UPDATE SET
            process_name = EXCLUDED.process_name,
            process_type = EXCLUDED.process_type,
            extract_rule_id = EXCLUDED.extract_rule_id,
            transform_rule_ids = EXCLUDED.transform_rule_ids,
            load_strategy_id = EXCLUDED.load_strategy_id,
            process_dependencies = EXCLUDED.process_dependencies,
            process_parameters = EXCLUDED.process_parameters
        """, (process.process_id, process.process_name, process.process_type,
              process.extract_rule_id, process.transform_rule_ids,
              process.load_strategy_id, process.process_dependencies,
              json.dumps(process.process_parameters)))

    conn.commit()

def generate_etl_report(conn):
    """ç”ŸæˆETLæŠ¥è¡¨"""
    cursor = conn.cursor()

    # æŸ¥è¯¢ETLæµç¨‹æ‰§è¡Œç»Ÿè®¡
    cursor.execute("""
        SELECT
            epd.process_name,
            epd.process_type,
            COUNT(eh.execution_id) as total_executions,
            SUM(CASE WHEN eh.execution_status = 'Completed' THEN 1 ELSE 0 END) as success_count,
            SUM(CASE WHEN eh.execution_status = 'Failed' THEN 1 ELSE 0 END) as failed_count,
            AVG(eh.rows_loaded) as avg_rows_loaded,
            AVG(EXTRACT(EPOCH FROM (eh.execution_end_time - eh.execution_start_time))) as avg_duration_seconds
        FROM etl_process_definitions epd
        LEFT JOIN etl_execution_history eh ON epd.process_id = eh.process_id
        WHERE eh.execution_start_time >= CURRENT_DATE - INTERVAL '30 days'
        GROUP BY epd.process_id, epd.process_name, epd.process_type
        ORDER BY total_executions DESC
    """)

    process_statistics = cursor.fetchall()

    # æŸ¥è¯¢æ•°æ®æºè¿æ¥ä½¿ç”¨æƒ…å†µ
    cursor.execute("""
        SELECT
            dsc.connection_type,
            COUNT(DISTINCT dsc.connection_id) as connection_count,
            COUNT(DISTINCT er.rule_id) as extract_rule_count,
            COUNT(DISTINCT epd.process_id) as process_count
        FROM data_source_connections dsc
        LEFT JOIN extract_rules er ON dsc.connection_id = er.connection_id
        LEFT JOIN etl_process_definitions epd ON er.rule_id = epd.extract_rule_id
        WHERE dsc.is_active = TRUE
        GROUP BY dsc.connection_type
        ORDER BY connection_count DESC
    """)

    connection_usage = cursor.fetchall()

    return {
        "process_statistics": process_statistics,
        "connection_usage": connection_usage
    }
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `04_Transformation.md` - è½¬æ¢ä½“ç³»

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
