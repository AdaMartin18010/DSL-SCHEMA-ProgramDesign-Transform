# æ•°æ®åˆ†æSchemaå®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [æ•°æ®åˆ†æSchemaå®è·µæ¡ˆä¾‹](#æ•°æ®åˆ†æschemaå®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¡ˆä¾‹æ¦‚è¿°](#1-æ¡ˆä¾‹æ¦‚è¿°)
  - [2. æ¡ˆä¾‹1ï¼šé”€å”®æ•°æ®åˆ†æ](#2-æ¡ˆä¾‹1é”€å”®æ•°æ®åˆ†æ)
    - [2.1 åœºæ™¯æè¿°](#21-åœºæ™¯æè¿°)
    - [2.2 Schemaå®šä¹‰](#22-schemaå®šä¹‰)
  - [3. æ¡ˆä¾‹2ï¼šå®¢æˆ·è¡Œä¸ºåˆ†æ](#3-æ¡ˆä¾‹2å®¢æˆ·è¡Œä¸ºåˆ†æ)
    - [3.1 åœºæ™¯æè¿°](#31-åœºæ™¯æè¿°)
    - [3.2 Schemaå®šä¹‰](#32-schemaå®šä¹‰)
  - [4. æ¡ˆä¾‹3ï¼šé¢„æµ‹åˆ†æ](#4-æ¡ˆä¾‹3é¢„æµ‹åˆ†æ)
    - [4.1 åœºæ™¯æè¿°](#41-åœºæ™¯æè¿°)
    - [4.2 Schemaå®šä¹‰](#42-schemaå®šä¹‰)
  - [5. æ¡ˆä¾‹4ï¼šæ•°æ®åˆ†æåˆ°æ•°æ®ä»“åº“è½¬æ¢](#5-æ¡ˆä¾‹4æ•°æ®åˆ†æåˆ°æ•°æ®ä»“åº“è½¬æ¢)
    - [5.1 åœºæ™¯æè¿°](#51-åœºæ™¯æè¿°)
    - [5.2 å®ç°ä»£ç ](#52-å®ç°ä»£ç )
  - [6. æ¡ˆä¾‹5ï¼šæ•°æ®åˆ†ææ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ](#6-æ¡ˆä¾‹5æ•°æ®åˆ†ææ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ)
    - [6.1 åœºæ™¯æè¿°](#61-åœºæ™¯æè¿°)
    - [6.2 å®ç°ä»£ç ](#62-å®ç°ä»£ç )

---

## 1. æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›æ•°æ®åˆ†æSchemaåœ¨å®é™…åº”ç”¨ä¸­çš„å®è·µæ¡ˆä¾‹ã€‚

---

## 2. æ¡ˆä¾‹1ï¼šé”€å”®æ•°æ®åˆ†æ

### 2.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
ä¼ä¸šé”€å”®æ•°æ®åˆ†æï¼ŒåŒ…æ‹¬é”€å”®è¶‹åŠ¿åˆ†æã€é”€å”®é¢„æµ‹ã€å®¢æˆ·åˆ†æã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- åˆ†æé”€å”®è¶‹åŠ¿
- é¢„æµ‹æœªæ¥é”€å”®
- åˆ†æå®¢æˆ·è¡Œä¸º
- ç”Ÿæˆé”€å”®æŠ¥è¡¨

### 2.2 Schemaå®šä¹‰

**é”€å”®æ•°æ®åˆ†æSchema**ï¼š

```dsl
schema SalesDataAnalysis {
  data_collection: DataCollection {
    data_sources: List<DataSource> {
      source1: DataSource {
        source_id: String @value("DS-SALES")
        source_type: Enum @value("Database")
        source_connection: String @value("postgresql://sales_db")
      }
    }
  }

  data_analysis: DataAnalysis {
    statistical_analysis: StatisticalAnalysis {
      analyses: List<Analysis> {
        analysis1: Analysis {
          analysis_id: String @value("ANALYSIS-SALES-TREND")
          analysis_type: Enum @value("Descriptive")
          analysis_method: String @value("Time Series Analysis")
          output_results: Map<String, Decimal> {
            "average_sales": Decimal @value(100000.00)
            "growth_rate": Decimal @value(10.50)
          }
        }
      }
    }
    predictive_analysis: PredictiveAnalysis {
      forecasts: List<Forecast> {
        forecast1: Forecast {
          forecast_id: String @value("FORECAST-SALES-2025")
          forecast_type: Enum @value("TimeSeries")
          forecast_period: Date @value("2025-12-31")
          forecast_value: Decimal @value(1200000.00)
          confidence_level: Decimal @value(85.00)
        }
      }
    }
  }

  data_visualization: DataVisualization {
    dashboards: List<Dashboard> {
      dashboard1: Dashboard {
        dashboard_id: String @value("DASHBOARD-SALES")
        dashboard_name: String @value("é”€å”®åˆ†æä»ªè¡¨æ¿")
        dashboard_components: List<DashboardComponent> {
          component1: DashboardComponent {
            component_type: Enum @value("Chart")
            component_config: Map<String, String> {
              "chart_type": String @value("Line")
              "data_source": String @value("sales_trend")
            }
          }
        }
      }
    }
  }
} @standard("Kimball", "OLAP")
```

---

## 3. æ¡ˆä¾‹2ï¼šå®¢æˆ·è¡Œä¸ºåˆ†æ

### 3.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
ä¼ä¸šå®¢æˆ·è¡Œä¸ºåˆ†æï¼ŒåŒ…æ‹¬å®¢æˆ·ç»†åˆ†ã€å®¢æˆ·é¢„æµ‹ã€å®¢æˆ·ä»·å€¼åˆ†æã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- å®¢æˆ·ç»†åˆ†
- å®¢æˆ·è¡Œä¸ºé¢„æµ‹
- å®¢æˆ·ä»·å€¼åˆ†æ
- å®¢æˆ·æ¨è

### 3.2 Schemaå®šä¹‰

**å®¢æˆ·è¡Œä¸ºåˆ†æSchema**ï¼š

```dsl
schema CustomerBehaviorAnalysis {
  data_analysis: DataAnalysis {
    machine_learning: MachineLearning {
      models: List<MLModel> {
        model1: MLModel {
          model_id: String @value("MODEL-CUSTOMER-SEGMENT")
          model_type: Enum @value("Unsupervised")
          algorithm: String @value("K-Means Clustering")
          model_accuracy: Decimal @value(90.00)
        }
        model2: MLModel {
          model_id: String @value("MODEL-CUSTOMER-PREDICT")
          model_type: Enum @value("Supervised")
          algorithm: String @value("Random Forest")
          model_accuracy: Decimal @value(85.00)
        }
      }
      predictions: List<Prediction> {
        prediction1: Prediction {
          prediction_id: String @value("PRED-CUSTOMER-001")
          model_id: String @value("MODEL-CUSTOMER-PREDICT")
          predicted_value: Decimal @value(5000.00)
          confidence_score: Decimal @value(88.00)
        }
      }
    }
  }
} @standard("Machine Learning")
```

---

## 4. æ¡ˆä¾‹3ï¼šé¢„æµ‹åˆ†æ

### 4.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
ä¼ä¸šé¢„æµ‹åˆ†æï¼ŒåŒ…æ‹¬éœ€æ±‚é¢„æµ‹ã€é”€å”®é¢„æµ‹ã€é£é™©é¢„æµ‹ã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- éœ€æ±‚é¢„æµ‹
- é”€å”®é¢„æµ‹
- é£é™©é¢„æµ‹
- é¢„æµ‹å‡†ç¡®æ€§è¯„ä¼°

### 4.2 Schemaå®šä¹‰

**é¢„æµ‹åˆ†æSchema**ï¼š

```dsl
schema PredictiveAnalysis {
  predictive_analysis: PredictiveAnalysis {
    forecasts: List<Forecast> {
      forecast1: Forecast {
        forecast_id: String @value("FORECAST-DEMAND-2025")
        forecast_type: Enum @value("TimeSeries")
        forecast_method: String @value("ARIMA")
        forecast_period: Date @value("2025-12-31")
        forecast_value: Decimal @value(1000000.00)
        confidence_level: Decimal @value(90.00)
      }
      forecast2: Forecast {
        forecast_id: String @value("FORECAST-SALES-2025")
        forecast_type: Enum @value("Regression")
        forecast_method: String @value("Linear Regression")
        forecast_period: Date @value("2025-12-31")
        forecast_value: Decimal @value(2000000.00)
        confidence_level: Decimal @value(85.00)
      }
    }
  }
} @standard("Predictive Analytics")
```

---

## 5. æ¡ˆä¾‹4ï¼šæ•°æ®åˆ†æåˆ°æ•°æ®ä»“åº“è½¬æ¢

### 5.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
å°†ä¼ä¸šæ•°æ®åˆ†æç»“æœè½¬æ¢ä¸ºæ•°æ®ä»“åº“æ ¼å¼ï¼Œç”¨äºæ•°æ®ä»“åº“å»ºè®¾ã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- åˆ†æç»“æœè½¬æ¢ä¸ºäº‹å®è¡¨
- åˆ†æç»´åº¦è½¬æ¢ä¸ºç»´åº¦è¡¨
- åˆ†ææŒ‡æ ‡è½¬æ¢ä¸ºåº¦é‡å€¼

### 5.2 å®ç°ä»£ç 

```python
from data_analytics_schema import DataAnalyticsSchema
from data_warehouse_schema import DataWarehouseSchema, StarSchema, FactTable, DimensionTable

def convert_analytics_to_data_warehouse(analytics_data: DataAnalyticsSchema) -> DataWarehouseSchema:
    """å°†æ•°æ®åˆ†ææ•°æ®è½¬æ¢ä¸ºæ•°æ®ä»“åº“æ ¼å¼"""
    dw_schema = DataWarehouseSchema()

    # è½¬æ¢æ˜Ÿå‹æ¨¡å¼
    star_schema = StarSchema()

    # åˆ›å»ºé”€å”®äº‹å®è¡¨
    fact_table = FactTable()
    fact_table.fact_table_name = "sales_fact"
    fact_table.measures = [
        {"name": "sales_amount", "type": "Decimal"},
        {"name": "quantity", "type": "Integer"},
        {"name": "profit", "type": "Decimal"}
    ]
    fact_table.grain = "Daily Sales by Product and Customer"
    star_schema.fact_tables.append(fact_table)

    # åˆ›å»ºäº§å“ç»´åº¦è¡¨
    product_dimension = DimensionTable()
    product_dimension.dimension_name = "product_dimension"
    product_dimension.dimension_attributes = [
        "product_id", "product_name", "product_category", "product_price"
    ]
    star_schema.dimension_tables.append(product_dimension)

    # åˆ›å»ºå®¢æˆ·ç»´åº¦è¡¨
    customer_dimension = DimensionTable()
    customer_dimension.dimension_name = "customer_dimension"
    customer_dimension.dimension_attributes = [
        "customer_id", "customer_name", "customer_segment", "customer_region"
    ]
    star_schema.dimension_tables.append(customer_dimension)

    dw_schema.star_schema = star_schema

    return dw_schema

# ä½¿ç”¨ç¤ºä¾‹
analytics_data = DataAnalyticsSchema.load_from_database("2025-01")
dw_schema = convert_analytics_to_data_warehouse(analytics_data)
dw_schema.save_to_database()
```

---

## 6. æ¡ˆä¾‹5ï¼šæ•°æ®åˆ†ææ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ

### 6.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
ä¼ä¸šæ•°æ®åˆ†ææ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿï¼Œæ”¯æŒæ•°æ®åˆ†ææ•°æ®å­˜å‚¨ã€æŸ¥è¯¢ã€åˆ†æå’ŒæŠ¥è¡¨ç”Ÿæˆã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- PostgreSQLæ•°æ®åº“å­˜å‚¨
- æ”¯æŒå¤æ‚æŸ¥è¯¢å’Œåˆ†æ
- æ”¯æŒæ•°æ®è´¨é‡ç›‘æ§
- æ”¯æŒæ¨¡å‹æ€§èƒ½åˆ†æ

### 6.2 å®ç°ä»£ç 

```python
import psycopg2
from data_analytics_schema import DataAnalyticsSchema, DataSource, MLModel

class AnalyticsDataStore:
    def __init__(self, db_config):
        self.conn = psycopg2.connect(**db_config)

    def store_analytics_data(self, analytics_data: DataAnalyticsSchema):
        """å­˜å‚¨æ•°æ®åˆ†ææ•°æ®"""
        cursor = self.conn.cursor()

        # æ’å…¥æ•°æ®æº
        for source in analytics_data.data_collection.data_sources:
            cursor.execute("""
                INSERT INTO data_sources
                (source_id, source_type, source_connection, is_active)
                VALUES (%s, %s, %s, %s)
            """, (source.source_id, source.source_type,
                  source.source_connection, source.is_active))

        # æ’å…¥æœºå™¨å­¦ä¹ æ¨¡å‹
        for model in analytics_data.data_analysis.machine_learning.models:
            cursor.execute("""
                INSERT INTO ml_models
                (model_id, model_type, algorithm, model_accuracy, training_date, model_version)
                VALUES (%s, %s, %s, %s, %s, %s)
            """, (model.model_id, model.model_type, model.algorithm,
                  model.model_accuracy, "2025-01-21", "v1.0"))

        self.conn.commit()

    def generate_analytics_report(self, period_start, period_end):
        """ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Š"""
        cursor = self.conn.cursor()

        # æŸ¥è¯¢æ•°æ®è´¨é‡è¶‹åŠ¿
        cursor.execute("""
            SELECT
                check_date,
                AVG(metric_value) as avg_quality_score,
                COUNT(CASE WHEN is_passed THEN 1 END) as passed_metrics,
                COUNT(*) as total_metrics
            FROM data_quality_metrics
            WHERE check_date BETWEEN %s AND %s
            GROUP BY check_date
            ORDER BY check_date
        """, (period_start, period_end))

        quality_trends = cursor.fetchall()

        # æŸ¥è¯¢æ¨¡å‹æ€§èƒ½
        cursor.execute("""
            SELECT
                model_type,
                AVG(model_accuracy) as avg_accuracy,
                COUNT(*) as model_count
            FROM ml_models
            WHERE training_date BETWEEN %s AND %s
            GROUP BY model_type
            ORDER BY avg_accuracy DESC
        """, (period_start, period_end))

        model_performance = cursor.fetchall()

        return {
            "quality_trends": quality_trends,
            "model_performance": model_performance
        }

# ä½¿ç”¨ç¤ºä¾‹
db_config = {
    "host": "localhost",
    "database": "data_analytics",
    "user": "analytics_user",
    "password": "password"
}

store = AnalyticsDataStore(db_config)

# ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Š
analytics_report = store.generate_analytics_report("2025-01-01", "2025-12-31")
print("æ•°æ®è´¨é‡è¶‹åŠ¿:")
for row in analytics_report["quality_trends"]:
    print(f"æ—¥æœŸ: {row[0]}, å¹³å‡è´¨é‡: {row[1]:.2f}, é€šè¿‡: {row[2]}/{row[3]}")

print("\næ¨¡å‹æ€§èƒ½:")
for row in analytics_report["model_performance"]:
    print(f"{row[0]}: å¹³å‡å‡†ç¡®ç‡={row[1]:.2f}%, æ¨¡å‹æ•°={row[2]}")
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `04_Transformation.md` - è½¬æ¢ä½“ç³»

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
