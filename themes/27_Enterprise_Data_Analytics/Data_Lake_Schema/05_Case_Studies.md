# æ•°æ®æ¹–Schemaå®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [æ•°æ®æ¹–Schemaå®è·µæ¡ˆä¾‹](#æ•°æ®æ¹–schemaå®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¡ˆä¾‹æ¦‚è¿°](#1-æ¡ˆä¾‹æ¦‚è¿°)
  - [2. æ¡ˆä¾‹1ï¼šä¼ä¸šDelta Lakeæ•°æ®æ¹–ç³»ç»Ÿ](#2-æ¡ˆä¾‹1ä¼ä¸šdelta-lakeæ•°æ®æ¹–ç³»ç»Ÿ)
    - [2.1 ä¸šåŠ¡èƒŒæ™¯](#21-ä¸šåŠ¡èƒŒæ™¯)
    - [2.2 æŠ€æœ¯æŒ‘æˆ˜](#22-æŠ€æœ¯æŒ‘æˆ˜)
    - [2.3 è§£å†³æ–¹æ¡ˆ](#23-è§£å†³æ–¹æ¡ˆ)
    - [2.4 å®Œæ•´ä»£ç å®ç°](#24-å®Œæ•´ä»£ç å®ç°)
    - [2.5 æ•ˆæœè¯„ä¼°](#25-æ•ˆæœè¯„ä¼°)
  - [4. æ¡ˆä¾‹3ï¼šæ•°æ®ç›®å½•ä¸æ•°æ®è¡€ç¼˜ç³»ç»Ÿ](#4-æ¡ˆä¾‹3æ•°æ®ç›®å½•ä¸æ•°æ®è¡€ç¼˜ç³»ç»Ÿ)
    - [4.1 åœºæ™¯æè¿°](#41-åœºæ™¯æè¿°)
    - [4.2 å®ç°ä»£ç ](#42-å®ç°ä»£ç )
  - [5. æ¡ˆä¾‹4ï¼šæ•°æ®æ²»ç†ä¸åˆè§„ç³»ç»Ÿ](#5-æ¡ˆä¾‹4æ•°æ®æ²»ç†ä¸åˆè§„ç³»ç»Ÿ)
    - [5.1 åœºæ™¯æè¿°](#51-åœºæ™¯æè¿°)
    - [5.2 å®ç°ä»£ç ](#52-å®ç°ä»£ç )
  - [6. æ¡ˆä¾‹5ï¼šæ•°æ®æ¹–æ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ](#6-æ¡ˆä¾‹5æ•°æ®æ¹–æ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ)
    - [6.1 åœºæ™¯æè¿°](#61-åœºæ™¯æè¿°)
    - [6.2 å®ç°ä»£ç ](#62-å®ç°ä»£ç )

---

## 1. æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›æ•°æ®æ¹–Schemaåœ¨å®é™…ä¼ä¸šåº”ç”¨ä¸­çš„å®è·µæ¡ˆä¾‹ï¼Œæ¶µç›–Delta Lakeæ•°æ®æ¹–è®¾è®¡ã€æ•°æ®ç›®å½•ä¸æ•°æ®è¡€ç¼˜ã€æ•°æ®æ²»ç†ä¸åˆè§„ç­‰çœŸå®åœºæ™¯ã€‚

**æ¡ˆä¾‹ç±»å‹**ï¼š

1. **ä¼ä¸šDelta Lakeæ•°æ®æ¹–ç³»ç»Ÿ**ï¼šDelta Lakeæ•°æ®æ¹–æ„å»º
2. **æ•°æ®æ¹–åˆ°æ•°æ®ä»“åº“è½¬æ¢å·¥å…·**ï¼šæ•°æ®æ¹–åˆ°æ•°æ®ä»“åº“è½¬æ¢
3. **æ•°æ®ç›®å½•ä¸æ•°æ®è¡€ç¼˜ç³»ç»Ÿ**ï¼šæ•°æ®ç›®å½•å’Œè¡€ç¼˜ç®¡ç†
4. **æ•°æ®æ²»ç†ä¸åˆè§„ç³»ç»Ÿ**ï¼šæ•°æ®æ²»ç†å’Œåˆè§„
5. **æ•°æ®æ¹–æ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ**ï¼šæ•°æ®æ¹–æ•°æ®åˆ†æå’Œç›‘æ§

**å‚è€ƒä¼ä¸šæ¡ˆä¾‹**ï¼š

- **Delta Lakeå®˜æ–¹**ï¼šDelta LakeæŠ€æœ¯æ–‡æ¡£
- **æ•°æ®æ¹–æœ€ä½³å®è·µ**ï¼šDatabricksæ•°æ®æ¹–æŒ‡å—

---

## 2. æ¡ˆä¾‹1ï¼šä¼ä¸šDelta Lakeæ•°æ®æ¹–ç³»ç»Ÿ

### 2.1 ä¸šåŠ¡èƒŒæ™¯

**ä¼ä¸šèƒŒæ™¯**ï¼š
æŸäº’è”ç½‘å…¬å¸éœ€è¦æ„å»ºä¼ä¸šçº§æ•°æ®æ¹–ï¼ŒåŸºäºDelta LakeæŠ€æœ¯ï¼Œæ”¯æŒACIDäº‹åŠ¡ã€æ—¶é—´æ—…è¡ŒæŸ¥è¯¢ã€Schemaæ¼”è¿›ï¼Œä¸ºå¤§æ•°æ®åˆ†ææä¾›æ•°æ®å­˜å‚¨åŸºç¡€ã€‚

**ä¸šåŠ¡ç—›ç‚¹**ï¼š

1. **æ•°æ®å­˜å‚¨åˆ†æ•£**ï¼šæ•°æ®å­˜å‚¨åˆ†æ•£åœ¨ä¸åŒç³»ç»Ÿ
2. **æ•°æ®ä¸€è‡´æ€§å·®**ï¼šç¼ºä¹ACIDäº‹åŠ¡ä¿è¯
3. **å†å²æ•°æ®æŸ¥è¯¢å›°éš¾**ï¼šæ— æ³•æŸ¥è¯¢å†å²æ•°æ®
4. **Schemaå˜æ›´å›°éš¾**ï¼šSchemaå˜æ›´å›°éš¾

**ä¸šåŠ¡ç›®æ ‡**ï¼š

- é›†ä¸­æ•°æ®å­˜å‚¨
- ä¿è¯æ•°æ®ä¸€è‡´æ€§
- æ”¯æŒå†å²æ•°æ®æŸ¥è¯¢
- æ”¯æŒSchemaæ¼”è¿›

### 2.2 æŠ€æœ¯æŒ‘æˆ˜

1. **Delta Lakeå®æ–½**ï¼šæ­£ç¡®å®æ–½Delta LakeæŠ€æœ¯
2. **ACIDäº‹åŠ¡**ï¼šå®ç°ACIDäº‹åŠ¡æ”¯æŒ
3. **æ—¶é—´æ—…è¡Œ**ï¼šå®ç°æ—¶é—´æ—…è¡ŒæŸ¥è¯¢
4. **Schemaæ¼”è¿›**ï¼šæ”¯æŒSchemaæ¼”è¿›

### 2.3 è§£å†³æ–¹æ¡ˆ

**ä½¿ç”¨Schemaå®šä¹‰Delta Lakeæ•°æ®æ¹–ç³»ç»Ÿ**ï¼š

### 2.4 å®Œæ•´ä»£ç å®ç°

**Delta Lakeæ•°æ®æ¹–Schemaï¼ˆå®Œæ•´ç¤ºä¾‹ï¼‰**ï¼š

```python
#!/usr/bin/env python3
"""
æ•°æ®æ¹–Schemaå®ç°
"""

from typing import Dict, List, Optional
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime

class StorageFormatType(str, Enum):
    """å­˜å‚¨æ ¼å¼ç±»å‹"""
    DELTA = "Delta"
    PARQUET = "Parquet"
    ORC = "ORC"

class CompressionType(str, Enum):
    """å‹ç¼©ç±»å‹"""
    SNAPPY = "Snappy"
    GZIP = "Gzip"
    LZ4 = "LZ4"

@dataclass
class StorageFormat:
    """å­˜å‚¨æ ¼å¼"""
    format_id: str
    format_name: str
    format_type: StorageFormatType
    compression_type: CompressionType = CompressionType.SNAPPY
    schema_evolution: bool = True
    acid_transactions: bool = True
    time_travel: bool = True

@dataclass
class StoragePartition:
    """å­˜å‚¨åˆ†åŒº"""
    partition_id: str
    partition_path: str
    partition_strategy: str = "Date"
    partition_keys: List[str] = field(default_factory=list)
    data_format: str = "Delta"
    created_at: datetime = field(default_factory=datetime.now)

@dataclass
class TableColumn:
    """è¡¨åˆ—"""
    column_name: str
    column_type: str
    is_nullable: bool = True
    default_value: Optional[str] = None
    description: Optional[str] = None

@dataclass
class DataTable:
    """æ•°æ®è¡¨"""
    table_id: str
    table_name: str
    table_path: str
    table_format: str = "Delta"
    columns: List[TableColumn] = field(default_factory=list)
    partition_keys: List[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)

    def add_column(self, column: TableColumn):
        """æ·»åŠ åˆ—"""
        self.columns.append(column)
        self.updated_at = datetime.now()

    def get_schema(self) -> Dict:
        """è·å–Schema"""
        return {
            'table_name': self.table_name,
            'columns': [{
                'name': col.column_name,
                'type': col.column_type,
                'nullable': col.is_nullable
            } for col in self.columns]
        }

@dataclass
class DeltaLakeDataLake:
    """Delta Lakeæ•°æ®æ¹–"""
    storage_format: StorageFormat
    storage_partitions: Dict[str, StoragePartition] = field(default_factory=dict)
    data_tables: Dict[str, DataTable] = field(default_factory=dict)

    def add_partition(self, partition: StoragePartition):
        """æ·»åŠ åˆ†åŒº"""
        self.storage_partitions[partition.partition_id] = partition

    def create_table(self, table: DataTable):
        """åˆ›å»ºè¡¨"""
        self.data_tables[table.table_id] = table

    def time_travel_query(self, table_id: str, version: int) -> Dict:
        """æ—¶é—´æ—…è¡ŒæŸ¥è¯¢"""
        if table_id not in self.data_tables:
            return {"error": "Table not found"}

        table = self.data_tables[table_id]
        return {
            'table_id': table_id,
            'table_name': table.table_name,
            'version': version,
            'query': f"SELECT * FROM {table.table_name} VERSION AS OF {version}"
        }

    def evolve_schema(self, table_id: str, new_columns: List[TableColumn]):
        """æ¼”è¿›Schema"""
        if table_id not in self.data_tables:
            return False

        table = self.data_tables[table_id]
        for column in new_columns:
            table.add_column(column)

        return True

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åˆ›å»ºDelta Lakeæ•°æ®æ¹–
    data_lake = DeltaLakeDataLake(
        storage_format=StorageFormat(
            format_id="FORMAT-DELTA",
            format_name="Delta",
            format_type=StorageFormatType.DELTA,
            compression_type=CompressionType.SNAPPY,
            schema_evolution=True,
            acid_transactions=True,
            time_travel=True
        )
    )

    # åˆ›å»ºæ•°æ®è¡¨
    sales_table = DataTable(
        table_id="TBL-SALES",
        table_name="sales",
        table_path="/data/lake/sales/",
        table_format="Delta"
    )
    sales_table.add_column(TableColumn("sale_id", "String", False))
    sales_table.add_column(TableColumn("sale_date", "Date", False))
    sales_table.add_column(TableColumn("amount", "Decimal", False))

    data_lake.create_table(sales_table)

    # æ—¶é—´æ—…è¡ŒæŸ¥è¯¢
    time_travel_result = data_lake.time_travel_query("TBL-SALES", 1)
    print(f"æ—¶é—´æ—…è¡ŒæŸ¥è¯¢: {time_travel_result}")
```

### 2.5 æ•ˆæœè¯„ä¼°

**æ€§èƒ½æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | æ”¹è¿›å‰ | æ”¹è¿›å | æå‡ |
|------|--------|--------|------|
| æ•°æ®å­˜å‚¨é›†ä¸­åº¦ | åˆ†æ•£ | é›†ä¸­ | 100% |
| æ•°æ®ä¸€è‡´æ€§ | 80% | 100% | 20%æå‡ |
| å†å²æ•°æ®æŸ¥è¯¢èƒ½åŠ› | ä¸æ”¯æŒ | æ”¯æŒ | 100% |
| Schemaæ¼”è¿›èƒ½åŠ› | ä¸æ”¯æŒ | æ”¯æŒ | 100% |

**ä¸šåŠ¡ä»·å€¼**ï¼š

1. **æ•°æ®é›†ä¸­å­˜å‚¨**ï¼šé›†ä¸­æ•°æ®å­˜å‚¨
2. **æ•°æ®ä¸€è‡´æ€§ä¿è¯**ï¼šACIDäº‹åŠ¡ä¿è¯æ•°æ®ä¸€è‡´æ€§
3. **å†å²æ•°æ®æŸ¥è¯¢**ï¼šæ”¯æŒæ—¶é—´æ—…è¡ŒæŸ¥è¯¢å†å²æ•°æ®
4. **Schemaæ¼”è¿›æ”¯æŒ**ï¼šæ”¯æŒSchemaæ¼”è¿›

**ç»éªŒæ•™è®­**ï¼š

1. Delta Lakeå®æ–½å¾ˆé‡è¦
2. ACIDäº‹åŠ¡éœ€è¦æ­£ç¡®é…ç½®
3. æ—¶é—´æ—…è¡Œéœ€è¦åˆç†ä½¿ç”¨
4. Schemaæ¼”è¿›éœ€è¦è°¨æ…

**å‚è€ƒæ¡ˆä¾‹**ï¼š

- [Delta Lakeå®˜æ–¹æ–‡æ¡£](https://delta.io/)
- [æ•°æ®æ¹–æœ€ä½³å®è·µ](https://databricks.com/blog/)
      }
      sale_date: TableColumn {
        column_name: String @value("sale_date")
        column_type: Enum @value("Date")
        is_nullable: Boolean @value(false)
      }
      sale_amount: TableColumn {
        column_name: String @value("sale_amount")
        column_type: Enum @value("Decimal")
        is_nullable: Boolean @value(false)
      }
    }
    partition_columns: List<String> {
      "year"
      "month"
    }
  }
}

```

---

## 3. æ¡ˆä¾‹2ï¼šæ•°æ®æ¹–åˆ°æ•°æ®ä»“åº“è½¬æ¢

### 3.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
å°†æ•°æ®æ¹–ä¸­çš„åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ•°æ®ä»“åº“çš„æ˜Ÿå‹æ¨¡å¼ã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒè‡ªåŠ¨è¯†åˆ«äº‹å®è¡¨å’Œç»´åº¦è¡¨
- æ”¯æŒè‡ªåŠ¨ç”ŸæˆETLæµç¨‹
- æ”¯æŒæ•°æ®è¡€ç¼˜è¿½è¸ª

### 3.2 å®ç°ä»£ç 

```python
def convert_datalake_to_dw(lake_data: DataLakeSchema) -> DataWarehouseSchema:
    """å°†æ•°æ®æ¹–è½¬æ¢ä¸ºæ•°æ®ä»“åº“"""
    dw_schema = DataWarehouseSchema()

    # åˆ†ææ•°æ®è¡¨ï¼Œè¯†åˆ«äº‹å®è¡¨å’Œç»´åº¦è¡¨
    for table in lake_data.data_catalog.data_tables:
        # åˆ¤æ–­æ˜¯å¦ä¸ºäº‹å®è¡¨ï¼ˆåŒ…å«åº¦é‡åˆ—ï¼‰
        measure_columns = [col for col in table.columns if is_measure_column(col)]

        if measure_columns:
            # åˆ›å»ºäº‹å®è¡¨
            fact_table = FactTable()
            fact_table.fact_table_id = table.table_id
            fact_table.fact_table_name = table.table_name
            fact_table.fact_table_type = "Transaction"

            # è½¬æ¢åº¦é‡
            for column in measure_columns:
                measure = Measure()
                measure.measure_id = column.column_id
                measure.measure_name = column.column_name
                measure.measure_type = "Sum"
                measure.data_type = map_column_type_to_measure_type(column.column_type)
                measure.aggregation_function = "SUM"
                fact_table.measures.append(measure)

            # è½¬æ¢ç»´åº¦é”®ï¼ˆä»åˆ†åŒºåˆ—å’Œå…³è”åˆ—ï¼‰
            dimension_keys = set(table.partition_columns)
            for column in table.columns:
                if column.column_name.endswith("_id") and column.column_name not in measure_columns:
                    dimension_keys.add(column.column_name)

            for dim_key in dimension_keys:
                dimension_key = DimensionKey()
                dimension_key.foreign_key_name = dim_key
                dimension_key.dimension_table_id = f"DIM-{dim_key.replace('_id', '')}"
                fact_table.dimension_keys.append(dimension_key)

            dw_schema.star_schema.fact_tables.append(fact_table)
        else:
            # åˆ›å»ºç»´åº¦è¡¨
            dimension_table = DimensionTable()
            dimension_table.dimension_table_id = table.table_id
            dimension_table.dimension_table_name = table.table_name
            dimension_table.dimension_type = "Other"

            # è½¬æ¢å±æ€§
            for column in table.columns:
                attribute = DimensionAttribute()
                attribute.attribute_id = column.column_id
                attribute.attribute_name = column.column_name
                attribute.attribute_type = "Descriptive" if not column.column_name.endswith("_id") else "Surrogate_Key"
                attribute.data_type = map_column_type_to_attribute_type(column.column_type)
                attribute.is_required = not column.is_nullable
                dimension_table.attributes.append(attribute)

            # è®¾ç½®ä¸»é”®
            id_columns = [col for col in table.columns if col.column_name.endswith("_id")]
            if id_columns:
                dimension_table.primary_key = id_columns[0].column_name
            else:
                dimension_table.primary_key = table.columns[0].column_name

            dw_schema.star_schema.dimension_tables.append(dimension_table)

    # è½¬æ¢æ•°æ®è¡€ç¼˜ä¸ºETLæµç¨‹
    for edge in lake_data.data_catalog.data_lineage.lineage_edges:
        from_table = find_table_by_node_id(lake_data, edge.from_node_id)
        to_table = find_table_by_node_id(lake_data, edge.to_node_id)

        if from_table and to_table:
            etl_process = ETLProcess()
            etl_process.process_id = edge.edge_id
            etl_process.source_table = from_table.table_name
            etl_process.target_table = to_table.table_name
            etl_process.transformation_rule = edge.transformation_rule
            etl_process.data_flow_type = edge.data_flow_type
            dw_schema.etl_processes.append(etl_process)

    return dw_schema
```

---

## 4. æ¡ˆä¾‹3ï¼šæ•°æ®ç›®å½•ä¸æ•°æ®è¡€ç¼˜ç³»ç»Ÿ

### 4.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
æ„å»ºæ•°æ®ç›®å½•å’Œæ•°æ®è¡€ç¼˜ç³»ç»Ÿï¼Œæ”¯æŒæ•°æ®å‘ç°ã€æ•°æ®è¡€ç¼˜è¿½è¸ªã€å½±å“åˆ†æã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒæ•°æ®å‘ç°
- æ”¯æŒæ•°æ®è¡€ç¼˜è¿½è¸ª
- æ”¯æŒå½±å“åˆ†æ

### 4.2 å®ç°ä»£ç 

```python
def discover_data_tables(lake_data: DataLakeSchema, source_path: str) -> List[DataTable]:
    """å‘ç°æ•°æ®è¡¨"""
    discovered_tables = []

    # æ‰«ææ•°æ®æºè·¯å¾„
    for source in lake_data.data_catalog.data_sources:
        if source.source_location.startswith(source_path):
            # æ ¹æ®æ•°æ®æºç±»å‹å‘ç°è¡¨
            if source.source_type == "File_System":
                tables = discover_file_system_tables(source.source_location, source.source_format)
            elif source.source_type == "Object_Storage":
                tables = discover_object_storage_tables(source.source_location, source.source_format)
            elif source.source_type == "Database":
                tables = discover_database_tables(source.source_location)

            for table_info in tables:
                table = DataTable()
                table.table_id = f"TBL-{table_info['name']}"
                table.source_id = source.source_id
                table.table_name = table_info['name']
                table.table_path = table_info['path']
                table.table_format = source.source_format
                table.columns = [create_column_from_info(col) for col in table_info['columns']]
                discovered_tables.append(table)

    return discovered_tables

def trace_data_lineage(lake_data: DataLakeSchema, target_table_id: str) -> List[LineagePath]:
    """è¿½æº¯æ•°æ®è¡€ç¼˜"""
    lineage_paths = []

    # æŸ¥æ‰¾ç›®æ ‡è¡¨
    target_node = find_node_by_table_id(lake_data, target_table_id)

    if target_node:
        # é€’å½’æŸ¥æ‰¾ä¸Šæ¸¸èŠ‚ç‚¹
        def find_upstream_nodes(node: LineageNode, path: List[LineageNode]):
            if node.node_type == "Source":
                lineage_paths.append(LineagePath(nodes=path + [node]))
            else:
                # æŸ¥æ‰¾ä¸Šæ¸¸è¾¹
                upstream_edges = [edge for edge in lake_data.data_catalog.data_lineage.lineage_edges
                                 if edge.to_node_id == node.node_id]

                for edge in upstream_edges:
                    upstream_node = find_node_by_id(lake_data, edge.from_node_id)
                    if upstream_node and upstream_node not in path:
                        find_upstream_nodes(upstream_node, path + [node])

        find_upstream_nodes(target_node, [])

    return lineage_paths

def analyze_impact(lake_data: DataLakeSchema, source_table_id: str) -> List[LineagePath]:
    """åˆ†æå½±å“èŒƒå›´"""
    impact_paths = []

    # æŸ¥æ‰¾æºè¡¨
    source_node = find_node_by_table_id(lake_data, source_table_id)

    if source_node:
        # é€’å½’æŸ¥æ‰¾ä¸‹æ¸¸èŠ‚ç‚¹
        def find_downstream_nodes(node: LineageNode, path: List[LineageNode]):
            # æŸ¥æ‰¾ä¸‹æ¸¸è¾¹
            downstream_edges = [edge for edge in lake_data.data_catalog.data_lineage.lineage_edges
                               if edge.from_node_id == node.node_id]

            if not downstream_edges:
                impact_paths.append(LineagePath(nodes=path + [node]))
            else:
                for edge in downstream_edges:
                    downstream_node = find_node_by_id(lake_data, edge.to_node_id)
                    if downstream_node and downstream_node not in path:
                        find_downstream_nodes(downstream_node, path + [node])

        find_downstream_nodes(source_node, [])

    return impact_paths
```

---

## 5. æ¡ˆä¾‹4ï¼šæ•°æ®æ²»ç†ä¸åˆè§„ç³»ç»Ÿ

### 5.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
æ„å»ºæ•°æ®æ²»ç†ä¸åˆè§„ç³»ç»Ÿï¼Œæ”¯æŒæ•°æ®å®‰å…¨ã€æ•°æ®éšç§ã€åˆè§„æ£€æŸ¥ã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒè®¿é—®æ§åˆ¶
- æ”¯æŒæ•°æ®åˆ†ç±»
- æ”¯æŒåˆè§„æ£€æŸ¥

### 5.2 å®ç°ä»£ç 

```python
def classify_data_privacy(lake_data: DataLakeSchema, table_id: str) -> PrivacyClassification:
    """åˆ†ç±»æ•°æ®éšç§"""
    table = find_table(lake_data, table_id)

    classification = PrivacyClassification()
    classification.classification_id = f"CLASS-{table_id}"
    classification.table_id = table_id

    # æ£€æµ‹PIIç±»å‹
    pii_columns = []
    for column in table.columns:
        pii_type = detect_pii_type(column.column_name, column.column_type)
        if pii_type:
            pii_columns.append({
                "column_id": column.column_id,
                "pii_type": pii_type
            })

    # ç¡®å®šéšç§çº§åˆ«
    if pii_columns:
        if any(pii["pii_type"] in ["SSN", "Credit_Card"] for pii in pii_columns):
            classification.privacy_level = "Restricted"
        elif any(pii["pii_type"] in ["Email", "Phone"] for pii in pii_columns):
            classification.privacy_level = "Confidential"
        else:
            classification.privacy_level = "Internal"

        classification.pii_type = pii_columns[0]["pii_type"]
        classification.gdpr_applicable = True
    else:
        classification.privacy_level = "Public"
        classification.gdpr_applicable = False

    return classification

def check_compliance(lake_data: DataLakeSchema, framework_type: str) -> ComplianceCheck:
    """æ£€æŸ¥åˆè§„æ€§"""
    check = ComplianceCheck()
    check.check_id = f"CHECK-{framework_type}-{datetime.now().strftime('%Y%m%d')}"
    check.framework_id = find_framework_id(lake_data, framework_type)
    check.check_name = f"{framework_type} Compliance Check"
    check.check_date = datetime.now().date()

    if framework_type == "GDPR":
        # GDPRåˆè§„æ£€æŸ¥
        violations = []

        for classification in lake_data.data_governance.data_privacy.privacy_classifications:
            if classification.gdpr_applicable:
                # æ£€æŸ¥æ˜¯å¦æœ‰è®¿é—®æ§åˆ¶
                access_controls = [ac for ac in lake_data.data_governance.data_security.access_controls
                                 if ac.resource_id == classification.table_id]

                if not access_controls:
                    violations.append(f"Table {classification.table_id} lacks access control")

                # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®ä¿ç•™ç­–ç•¥
                retention_policies = [p for p in lake_data.data_governance.data_privacy.privacy_policies
                                     if p.policy_type == "Retention"
                                     and classification.table_id in p.applicable_resources]

                if not retention_policies:
                    violations.append(f"Table {classification.table_id} lacks retention policy")

        if violations:
            check.check_result = "Fail"
            check.check_details = "; ".join(violations)
        else:
            check.check_result = "Pass"

    elif framework_type == "HIPAA":
        # HIPAAåˆè§„æ£€æŸ¥
        # ç±»ä¼¼GDPRæ£€æŸ¥é€»è¾‘
        check.check_result = "Pass"

    return check
```

---

## 6. æ¡ˆä¾‹5ï¼šæ•°æ®æ¹–æ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ

### 6.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
æ•°æ®æ¹–æ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿï¼Œæ”¯æŒå…ƒæ•°æ®å­˜å‚¨ã€æŸ¥è¯¢ã€åˆ†æã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

- æ”¯æŒæ•°æ®æ¹–å…ƒæ•°æ®å­˜å‚¨
- æ”¯æŒå…ƒæ•°æ®æŸ¥è¯¢å’Œåˆ†æ
- æ”¯æŒæ•°æ®è´¨é‡ç›‘æ§

### 6.2 å®ç°ä»£ç 

```python
def store_datalake_data(lake_data: DataLakeSchema, conn):
    """å­˜å‚¨æ•°æ®æ¹–æ•°æ®åˆ°PostgreSQL"""
    cursor = conn.cursor()

    # å­˜å‚¨æ•°æ®æº
    for source in lake_data.data_catalog.data_sources:
        cursor.execute("""
            INSERT INTO data_sources
            (source_id, source_name, source_type, source_location, source_format, schema_definition, metadata)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (source_id) DO UPDATE SET
            source_name = EXCLUDED.source_name,
            source_location = EXCLUDED.source_location,
            source_format = EXCLUDED.source_format,
            schema_definition = EXCLUDED.schema_definition,
            metadata = EXCLUDED.metadata,
            updated_at = CURRENT_TIMESTAMP
        """, (source.source_id, source.source_name, source.source_type,
              source.source_location, source.source_format,
              json.dumps(source.schema_definition) if source.schema_definition else None,
              json.dumps(source.metadata) if source.metadata else None))

    # å­˜å‚¨æ•°æ®è¡¨
    for table in lake_data.data_catalog.data_tables:
        cursor.execute("""
            INSERT INTO data_tables
            (table_id, source_id, table_name, table_path, table_format, partition_columns, row_count, size_bytes)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (table_id) DO UPDATE SET
            table_name = EXCLUDED.table_name,
            table_path = EXCLUDED.table_path,
            table_format = EXCLUDED.table_format,
            partition_columns = EXCLUDED.partition_columns,
            row_count = EXCLUDED.row_count,
            size_bytes = EXCLUDED.size_bytes,
            updated_at = CURRENT_TIMESTAMP
        """, (table.table_id, table.source_id, table.table_name,
              table.table_path, table.table_format,
              table.partition_columns, table.row_count, table.size_bytes))

        # å­˜å‚¨è¡¨åˆ—
        for column in table.columns:
            cursor.execute("""
                INSERT INTO table_columns
                (column_id, table_id, column_name, column_type, is_nullable, description)
                VALUES (%s, %s, %s, %s, %s, %s)
                ON CONFLICT (column_id) DO UPDATE SET
                column_name = EXCLUDED.column_name,
                column_type = EXCLUDED.column_type,
                is_nullable = EXCLUDED.is_nullable,
                description = EXCLUDED.description
            """, (column.column_id, table.table_id, column.column_name,
                  column.column_type, column.is_nullable, column.description))

    # å­˜å‚¨æ•°æ®è¡€ç¼˜
    for edge in lake_data.data_catalog.data_lineage.lineage_edges:
        cursor.execute("""
            INSERT INTO data_lineage
            (lineage_id, from_node_id, to_node_id, transformation_rule, data_flow_type)
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT (lineage_id) DO UPDATE SET
            transformation_rule = EXCLUDED.transformation_rule,
            data_flow_type = EXCLUDED.data_flow_type
        """, (edge.edge_id, edge.from_node_id, edge.to_node_id,
              edge.transformation_rule, edge.data_flow_type))

    # å­˜å‚¨æ•°æ®è´¨é‡æŒ‡æ ‡
    for metric in lake_data.data_catalog.data_quality.quality_metrics:
        cursor.execute("""
            INSERT INTO data_quality_metrics
            (metric_id, table_id, metric_name, metric_type, metric_value, threshold, is_passed, check_date)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (metric_id) DO UPDATE SET
            metric_value = EXCLUDED.metric_value,
            is_passed = EXCLUDED.is_passed,
            check_date = EXCLUDED.check_date
        """, (metric.metric_id, metric.table_id, metric.metric_name,
              metric.metric_type, metric.metric_value, metric.threshold,
              metric.is_passed, metric.check_date))

    # å­˜å‚¨è®¿é—®æ§åˆ¶
    for control in lake_data.data_governance.data_security.access_controls:
        cursor.execute("""
            INSERT INTO access_controls
            (control_id, resource_id, resource_type, principal, permission, condition)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (control_id) DO UPDATE SET
            permission = EXCLUDED.permission,
            condition = EXCLUDED.condition
        """, (control.control_id, control.resource_id, control.resource_type,
              control.principal, control.permission, control.condition))

    conn.commit()

def generate_datalake_report(conn):
    """ç”Ÿæˆæ•°æ®æ¹–æŠ¥è¡¨"""
    cursor = conn.cursor()

    # æŸ¥è¯¢æ•°æ®æºæ±‡æ€»
    cursor.execute("""
        SELECT
            ds.source_type,
            COUNT(DISTINCT ds.source_id) as source_count,
            COUNT(DISTINCT dt.table_id) as table_count,
            SUM(dt.row_count) as total_rows,
            SUM(dt.size_bytes) / 1024 / 1024 / 1024 as total_size_gb
        FROM data_sources ds
        LEFT JOIN data_tables dt ON ds.source_id = dt.source_id
        GROUP BY ds.source_type
        ORDER BY source_count DESC
    """)

    source_report = cursor.fetchall()

    # æŸ¥è¯¢æ•°æ®è¡¨æ ¼å¼æ±‡æ€»
    cursor.execute("""
        SELECT
            dt.table_format,
            COUNT(*) as table_count,
            SUM(dt.row_count) as total_rows,
            SUM(dt.size_bytes) / 1024 / 1024 / 1024 as total_size_gb
        FROM data_tables dt
        GROUP BY dt.table_format
        ORDER BY table_count DESC
    """)

    format_report = cursor.fetchall()

    # æŸ¥è¯¢æ•°æ®è´¨é‡æŠ¥å‘Š
    cursor.execute("""
        SELECT
            dt.table_name,
            dqm.metric_type,
            AVG(dqm.metric_value) as avg_metric_value,
            SUM(CASE WHEN dqm.is_passed THEN 1 ELSE 0 END) as passed_count,
            COUNT(*) as total_checks
        FROM data_tables dt
        JOIN data_quality_metrics dqm ON dt.table_id = dqm.table_id
        WHERE dqm.check_date >= CURRENT_DATE - INTERVAL '30 days'
        GROUP BY dt.table_id, dt.table_name, dqm.metric_type
        ORDER BY dt.table_name, dqm.metric_type
    """)

    quality_report = cursor.fetchall()

    return {
        "source_report": source_report,
        "format_report": format_report,
        "quality_report": quality_report
    }
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `04_Transformation.md` - è½¬æ¢ä½“ç³»

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
