# æ•°æ®æ¹–Schemaè½¬æ¢ä½“ç³»

## ğŸ“‘ ç›®å½•

- [æ•°æ®æ¹–Schemaè½¬æ¢ä½“ç³»](#æ•°æ®æ¹–schemaè½¬æ¢ä½“ç³»)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. è½¬æ¢ä½“ç³»æ¦‚è¿°](#1-è½¬æ¢ä½“ç³»æ¦‚è¿°)
    - [1.1 è½¬æ¢ç›®æ ‡](#11-è½¬æ¢ç›®æ ‡)
  - [2. æ•°æ®æ¹–åˆ°æ•°æ®ä»“åº“è½¬æ¢](#2-æ•°æ®æ¹–åˆ°æ•°æ®ä»“åº“è½¬æ¢)
  - [3. æ•°æ®æ¹–åˆ°JSON Schemaè½¬æ¢](#3-æ•°æ®æ¹–åˆ°json-schemaè½¬æ¢)
  - [4. æ•°æ®æ¹–åˆ°Hive Metastoreè½¬æ¢](#4-æ•°æ®æ¹–åˆ°hive-metastoreè½¬æ¢)
  - [5. æ•°æ®æ¹–æ•°æ®å­˜å‚¨ä¸åˆ†æ](#5-æ•°æ®æ¹–æ•°æ®å­˜å‚¨ä¸åˆ†æ)
    - [5.1 PostgreSQLæ•°æ®æ¹–å…ƒæ•°æ®å­˜å‚¨](#51-postgresqlæ•°æ®æ¹–å…ƒæ•°æ®å­˜å‚¨)
    - [5.2 æ•°æ®æ¹–æ•°æ®åˆ†ææŸ¥è¯¢](#52-æ•°æ®æ¹–æ•°æ®åˆ†ææŸ¥è¯¢)

---

## 1. è½¬æ¢ä½“ç³»æ¦‚è¿°

æ•°æ®æ¹–Schemaè½¬æ¢ä½“ç³»æ”¯æŒæ•°æ®æ¹–åˆ°æ•°æ®ä»“åº“ã€JSON Schemaã€Hive Metastoreæ ¼å¼è½¬æ¢ï¼Œä»¥åŠæ•°æ®æ¹–å…ƒæ•°æ®å­˜å‚¨ã€‚

### 1.1 è½¬æ¢ç›®æ ‡

1. **æ•°æ®æ¹–åˆ°æ•°æ®ä»“åº“è½¬æ¢**ï¼šæ•°æ®æ¹–Schemaåˆ°æ•°æ®ä»“åº“æ ¼å¼
2. **æ•°æ®æ¹–åˆ°JSON Schemaè½¬æ¢**ï¼šæ•°æ®æ¹–Schemaåˆ°JSON Schemaæ ¼å¼
3. **æ•°æ®æ¹–åˆ°Hive Metastoreè½¬æ¢**ï¼šæ•°æ®æ¹–Schemaåˆ°Hive Metastoreæ ¼å¼
4. **æ•°æ®æ¹–åˆ°æ•°æ®åº“è½¬æ¢**ï¼šæ•°æ®æ¹–å…ƒæ•°æ®åˆ°PostgreSQLå­˜å‚¨

---

## 2. æ•°æ®æ¹–åˆ°æ•°æ®ä»“åº“è½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- æ•°æ®è¡¨ â†’ æ•°æ®ä»“åº“è¡¨ï¼ˆäº‹å®è¡¨/ç»´åº¦è¡¨ï¼‰
- æ•°æ®åˆ†åŒº â†’ æ•°æ®ä»“åº“åˆ†åŒº
- æ•°æ®è¡€ç¼˜ â†’ ETLæµç¨‹

**è½¬æ¢ç¤ºä¾‹**ï¼š

```python
def convert_datalake_to_datawarehouse(lake_data: DataLakeSchema) -> DataWarehouseSchema:
    """å°†æ•°æ®æ¹–Schemaè½¬æ¢ä¸ºæ•°æ®ä»“åº“æ ¼å¼"""
    dw_schema = DataWarehouseSchema()

    # è½¬æ¢æ•°æ®è¡¨
    for table in lake_data.data_catalog.data_tables:
        # åˆ¤æ–­è¡¨ç±»å‹ï¼ˆäº‹å®è¡¨æˆ–ç»´åº¦è¡¨ï¼‰
        if is_fact_table(table):
            fact_table = FactTable()
            fact_table.fact_table_id = table.table_id
            fact_table.fact_table_name = table.table_name
            fact_table.fact_table_type = "Transaction"

            # è½¬æ¢åº¦é‡
            for column in table.columns:
                if is_measure_column(column):
                    measure = Measure()
                    measure.measure_id = column.column_id
                    measure.measure_name = column.column_name
                    measure.measure_type = "Sum"
                    measure.data_type = map_column_type_to_measure_type(column.column_type)
                    fact_table.measures.append(measure)

            # è½¬æ¢ç»´åº¦é”®
            for partition_key in table.partition_columns:
                dimension_key = DimensionKey()
                dimension_key.foreign_key_name = partition_key
                dimension_key.dimension_table_id = f"DIM-{partition_key}"
                fact_table.dimension_keys.append(dimension_key)

            dw_schema.star_schema.fact_tables.append(fact_table)
        else:
            dimension_table = DimensionTable()
            dimension_table.dimension_table_id = table.table_id
            dimension_table.dimension_table_name = table.table_name
            dimension_table.dimension_type = "Other"

            # è½¬æ¢å±æ€§
            for column in table.columns:
                attribute = DimensionAttribute()
                attribute.attribute_id = column.column_id
                attribute.attribute_name = column.column_name
                attribute.attribute_type = "Descriptive"
                attribute.data_type = map_column_type_to_attribute_type(column.column_type)
                dimension_table.attributes.append(attribute)

            dimension_table.primary_key = table.columns[0].column_name
            dw_schema.star_schema.dimension_tables.append(dimension_table)

    # è½¬æ¢æ•°æ®è¡€ç¼˜ä¸ºETLæµç¨‹
    for edge in lake_data.data_catalog.data_lineage.lineage_edges:
        etl_process = ETLProcess()
        etl_process.process_id = edge.edge_id
        etl_process.source_table = find_table_by_node_id(lake_data, edge.from_node_id)
        etl_process.target_table = find_table_by_node_id(lake_data, edge.to_node_id)
        etl_process.transformation_rule = edge.transformation_rule
        dw_schema.etl_processes.append(etl_process)

    return dw_schema
```

---

## 3. æ•°æ®æ¹–åˆ°JSON Schemaè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- æ•°æ®è¡¨ â†’ JSON Schema Object
- æ•°æ®åˆ— â†’ JSON Schema Property
- æ•°æ®ç±»å‹ â†’ JSON Schema Type

**è½¬æ¢ç¤ºä¾‹**ï¼š

```python
def convert_datalake_to_json_schema(lake_data: DataLakeSchema) -> JSONSchema:
    """å°†æ•°æ®æ¹–Schemaè½¬æ¢ä¸ºJSON Schemaæ ¼å¼"""
    json_schema = {
        "$schema": "http://json-schema.org/draft-07/schema#",
        "type": "object",
        "properties": {}
    }

    # è½¬æ¢æ•°æ®è¡¨
    for table in lake_data.data_catalog.data_tables:
        table_schema = {
            "type": "object",
            "properties": {}
        }

        # è½¬æ¢åˆ—
        for column in table.columns:
            table_schema["properties"][column.column_name] = {
                "type": map_column_type_to_json_type(column.column_type),
                "description": column.description or column.column_name
            }

            if not column.is_nullable:
                if "required" not in table_schema:
                    table_schema["required"] = []
                table_schema["required"].append(column.column_name)

        json_schema["properties"][table.table_name] = table_schema

    return json_schema
```

---

## 4. æ•°æ®æ¹–åˆ°Hive Metastoreè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- æ•°æ®è¡¨ â†’ Hive Table
- æ•°æ®åˆ— â†’ Hive Column
- æ•°æ®åˆ†åŒº â†’ Hive Partition

**è½¬æ¢ç¤ºä¾‹**ï¼š

```python
def convert_datalake_to_hive_metastore(lake_data: DataLakeSchema) -> HiveMetastore:
    """å°†æ•°æ®æ¹–Schemaè½¬æ¢ä¸ºHive Metastoreæ ¼å¼"""
    metastore = HiveMetastore()

    # è½¬æ¢æ•°æ®æºä¸ºæ•°æ®åº“
    databases = {}
    for source in lake_data.data_catalog.data_sources:
        database = HiveDatabase()
        database.name = source.source_name
        database.location = source.source_location
        databases[source.source_id] = database
        metastore.databases.append(database)

    # è½¬æ¢æ•°æ®è¡¨
    for table in lake_data.data_catalog.data_tables:
        hive_table = HiveTable()
        hive_table.database_name = databases[table.source_id].name
        hive_table.table_name = table.table_name
        hive_table.table_type = "EXTERNAL"
        hive_table.location = table.table_path
        hive_table.input_format = map_format_to_hive_input_format(table.table_format)
        hive_table.output_format = map_format_to_hive_output_format(table.table_format)

        # è½¬æ¢åˆ—
        for column in table.columns:
            hive_column = HiveColumn()
            hive_column.name = column.column_name
            hive_column.type = map_column_type_to_hive_type(column.column_type)
            hive_column.comment = column.description
            hive_table.columns.append(hive_column)

        # è½¬æ¢åˆ†åŒº
        if table.partition_columns:
            for partition_key in table.partition_columns:
                partition_column = HivePartitionColumn()
                partition_column.name = partition_key
                partition_column.type = "string"  # é»˜è®¤åˆ†åŒºç±»å‹ä¸ºstring
                hive_table.partition_columns.append(partition_column)

        metastore.tables.append(hive_table)

    return metastore
```

---

## 5. æ•°æ®æ¹–æ•°æ®å­˜å‚¨ä¸åˆ†æ

### 5.1 PostgreSQLæ•°æ®æ¹–å…ƒæ•°æ®å­˜å‚¨

**è¡¨ç»“æ„è®¾è®¡**ï¼š

```sql
-- æ•°æ®æºè¡¨
CREATE TABLE data_sources (
    source_id VARCHAR(50) PRIMARY KEY,
    source_name VARCHAR(200) NOT NULL,
    source_type VARCHAR(20) NOT NULL,
    source_location VARCHAR(500) NOT NULL,
    source_format VARCHAR(20) NOT NULL,
    schema_definition JSONB,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æ•°æ®è¡¨è¡¨
CREATE TABLE data_tables (
    table_id VARCHAR(50) PRIMARY KEY,
    source_id VARCHAR(50) NOT NULL,
    table_name VARCHAR(200) NOT NULL,
    table_path VARCHAR(500) NOT NULL,
    table_format VARCHAR(20) NOT NULL,
    partition_columns TEXT[],
    row_count BIGINT,
    size_bytes BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (source_id) REFERENCES data_sources(source_id)
);

-- è¡¨åˆ—è¡¨
CREATE TABLE table_columns (
    column_id VARCHAR(50) PRIMARY KEY,
    table_id VARCHAR(50) NOT NULL,
    column_name VARCHAR(200) NOT NULL,
    column_type VARCHAR(50) NOT NULL,
    is_nullable BOOLEAN DEFAULT TRUE,
    description TEXT,
    FOREIGN KEY (table_id) REFERENCES data_tables(table_id)
);

-- æ•°æ®è¡€ç¼˜è¡¨
CREATE TABLE data_lineage (
    lineage_id VARCHAR(50) PRIMARY KEY,
    from_node_id VARCHAR(50) NOT NULL,
    to_node_id VARCHAR(50) NOT NULL,
    transformation_rule TEXT,
    data_flow_type VARCHAR(20) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æ•°æ®è´¨é‡æŒ‡æ ‡è¡¨
CREATE TABLE data_quality_metrics (
    metric_id VARCHAR(50) PRIMARY KEY,
    table_id VARCHAR(50) NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    metric_type VARCHAR(20) NOT NULL,
    metric_value DECIMAL(5, 2),
    threshold DECIMAL(5, 2) DEFAULT 90,
    is_passed BOOLEAN,
    check_date DATE NOT NULL,
    FOREIGN KEY (table_id) REFERENCES data_tables(table_id)
);

-- è®¿é—®æ§åˆ¶è¡¨
CREATE TABLE access_controls (
    control_id VARCHAR(50) PRIMARY KEY,
    resource_id VARCHAR(50) NOT NULL,
    resource_type VARCHAR(20) NOT NULL,
    principal VARCHAR(200) NOT NULL,
    permission VARCHAR(20) NOT NULL,
    condition TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_data_tables_source ON data_tables(source_id);
CREATE INDEX idx_table_columns_table ON table_columns(table_id);
CREATE INDEX idx_data_lineage_from ON data_lineage(from_node_id);
CREATE INDEX idx_data_lineage_to ON data_lineage(to_node_id);
CREATE INDEX idx_data_quality_metrics_table ON data_quality_metrics(table_id);
CREATE INDEX idx_access_controls_resource ON access_controls(resource_id);
```

### 5.2 æ•°æ®æ¹–æ•°æ®åˆ†ææŸ¥è¯¢

**æŸ¥è¯¢ç¤ºä¾‹**ï¼š

```python
def analyze_datalake_data(conn):
    """åˆ†ææ•°æ®æ¹–æ•°æ®"""
    cursor = conn.cursor()

    # æŸ¥è¯¢æ•°æ®æºæ±‡æ€»
    cursor.execute("""
        SELECT
            ds.source_type,
            COUNT(DISTINCT ds.source_id) as source_count,
            COUNT(DISTINCT dt.table_id) as table_count,
            SUM(dt.row_count) as total_rows,
            SUM(dt.size_bytes) as total_size_bytes
        FROM data_sources ds
        LEFT JOIN data_tables dt ON ds.source_id = dt.source_id
        GROUP BY ds.source_type
        ORDER BY source_count DESC
    """)

    source_summary = cursor.fetchall()

    # æŸ¥è¯¢æ•°æ®è¡¨æ ¼å¼æ±‡æ€»
    cursor.execute("""
        SELECT
            dt.table_format,
            COUNT(*) as table_count,
            SUM(dt.row_count) as total_rows,
            SUM(dt.size_bytes) as total_size_bytes
        FROM data_tables dt
        GROUP BY dt.table_format
        ORDER BY table_count DESC
    """)

    format_summary = cursor.fetchall()

    # æŸ¥è¯¢æ•°æ®è´¨é‡æ±‡æ€»
    cursor.execute("""
        SELECT
            dt.table_name,
            dqm.metric_type,
            AVG(dqm.metric_value) as avg_metric_value,
            SUM(CASE WHEN dqm.is_passed THEN 1 ELSE 0 END) as passed_count,
            COUNT(*) as total_checks
        FROM data_tables dt
        JOIN data_quality_metrics dqm ON dt.table_id = dqm.table_id
        WHERE dqm.check_date >= CURRENT_DATE - INTERVAL '30 days'
        GROUP BY dt.table_id, dt.table_name, dqm.metric_type
        ORDER BY dt.table_name, dqm.metric_type
    """)

    quality_summary = cursor.fetchall()

    return {
        "source_summary": source_summary,
        "format_summary": format_summary,
        "quality_summary": quality_summary
    }
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `05_Case_Studies.md` - å®è·µæ¡ˆä¾‹

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
