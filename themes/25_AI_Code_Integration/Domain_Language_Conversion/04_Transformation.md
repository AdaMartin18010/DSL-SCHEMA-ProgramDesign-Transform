# DSLåˆ°ä»£ç è½¬æ¢

## ğŸ“‘ ç›®å½•

- [DSLåˆ°ä»£ç è½¬æ¢](#dslåˆ°ä»£ç è½¬æ¢)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. è½¬æ¢æ¦‚è¿°](#1-è½¬æ¢æ¦‚è¿°)
    - [1.1 è½¬æ¢æµç¨‹](#11-è½¬æ¢æµç¨‹)
    - [1.2 è½¬æ¢ç›®æ ‡](#12-è½¬æ¢ç›®æ ‡)
  - [2. è‡ªç„¶è¯­è¨€åˆ°DSLè½¬æ¢](#2-è‡ªç„¶è¯­è¨€åˆ°dslè½¬æ¢)
    - [2.1 è½¬æ¢å·¥å…·](#21-è½¬æ¢å·¥å…·)
    - [2.2 è½¬æ¢ç¤ºä¾‹](#22-è½¬æ¢ç¤ºä¾‹)
  - [3. DSLåˆ°ä»£ç ç”Ÿæˆ](#3-dslåˆ°ä»£ç ç”Ÿæˆ)
    - [3.1 OpenAPIä»£ç ç”Ÿæˆ](#31-openapiä»£ç ç”Ÿæˆ)
    - [3.2 AsyncAPIä»£ç ç”Ÿæˆ](#32-asyncapiä»£ç ç”Ÿæˆ)
    - [3.3 IoTSchemaé€‚é…](#33-iotschemaé€‚é…)
  - [4. è½¬æ¢å·¥å…·](#4-è½¬æ¢å·¥å…·)
    - [4.1 OpenAPI Generator](#41-openapi-generator)
    - [4.2 AsyncAPI Generator](#42-asyncapi-generator)
    - [4.3 è‡ªå®šä¹‰è½¬æ¢å·¥å…·](#43-è‡ªå®šä¹‰è½¬æ¢å·¥å…·)
  - [5. è½¬æ¢å®è·µ](#5-è½¬æ¢å®è·µ)
    - [5.1 å®è·µæµç¨‹](#51-å®è·µæµç¨‹)
    - [5.2 æœ€ä½³å®è·µ](#52-æœ€ä½³å®è·µ)
  - [6. æ•°æ®åº“å­˜å‚¨ä¸åˆ†æ](#6-æ•°æ®åº“å­˜å‚¨ä¸åˆ†æ)
    - [6.1 PostgreSQLæ•°æ®å­˜å‚¨](#61-postgresqlæ•°æ®å­˜å‚¨)
    - [6.2 æ•°æ®åˆ†ææŸ¥è¯¢ç¤ºä¾‹](#62-æ•°æ®åˆ†ææŸ¥è¯¢ç¤ºä¾‹)

---

## 1. è½¬æ¢æ¦‚è¿°

### 1.1 è½¬æ¢æµç¨‹

```text
è‡ªç„¶è¯­è¨€ â†’ DSL â†’ ä»£ç ç”Ÿæˆ â†’ éªŒè¯ â†’ éƒ¨ç½²
```

### 1.2 è½¬æ¢ç›®æ ‡

1. **è‡ªç„¶è¯­è¨€åˆ°DSL**ï¼šç”¨æˆ·æè¿°è½¬æ¢ä¸ºDSLè§„èŒƒ
2. **DSLåˆ°ä»£ç ç”Ÿæˆ**ï¼šDSLè§„èŒƒç”Ÿæˆå¯æ‰§è¡Œä»£ç 
3. **ä»£ç éªŒè¯**ï¼šè‡ªåŠ¨éªŒè¯ç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§

---

## 2. è‡ªç„¶è¯­è¨€åˆ°DSLè½¬æ¢

### 2.1 è½¬æ¢å·¥å…·

**AIæ¨¡å‹**ï¼š

- Claudeï¼ˆAnthropicï¼‰
- GPTï¼ˆOpenAIï¼‰
- Geminiï¼ˆGoogleï¼‰

**è½¬æ¢åœºæ™¯**ï¼š

- ç”¨æˆ·è¾“å…¥ï¼š"åˆ›å»ºä¸€ä¸ªæ”¯æŒæ–‡ä»¶ä¸Šä¼ çš„API"
- AIè¾“å‡ºï¼šè‡ªåŠ¨ç”ŸæˆOpenAPI 3.1è§„èŒƒå¹¶éªŒè¯

### 2.2 è½¬æ¢ç¤ºä¾‹

**è¾“å…¥ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰**ï¼š

```text
åˆ›å»ºä¸€ä¸ªç”¨æˆ·ç®¡ç†APIï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š
1. åˆ›å»ºç”¨æˆ·ï¼ˆPOST /usersï¼‰
2. æŸ¥è¯¢ç”¨æˆ·ï¼ˆGET /users/{id}ï¼‰
3. æ›´æ–°ç”¨æˆ·ï¼ˆPUT /users/{id}ï¼‰
4. åˆ é™¤ç”¨æˆ·ï¼ˆDELETE /users/{id}ï¼‰
```

**è¾“å‡ºï¼ˆOpenAPI DSLï¼‰**ï¼š

```yaml
openapi: 3.1.0
paths:
  /users:
    post:
      summary: åˆ›å»ºç”¨æˆ·
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                name:
                  type: string
                email:
                  type: string
  /users/{id}:
    get:
      summary: æŸ¥è¯¢ç”¨æˆ·
    put:
      summary: æ›´æ–°ç”¨æˆ·
    delete:
      summary: åˆ é™¤ç”¨æˆ·
```

---

## 3. DSLåˆ°ä»£ç ç”Ÿæˆ

### 3.1 OpenAPIä»£ç ç”Ÿæˆ

**OpenAPI Generator**ï¼š

- **è¾“å…¥**ï¼šOpenAPIè§„èŒƒæ–‡ä»¶
- **è¾“å‡º**ï¼šå¤šè¯­è¨€å®¢æˆ·ç«¯ä»£ç ï¼ˆPythonã€Node.jsã€Goã€Javaç­‰ï¼‰
- **ç‰¹æ€§**ï¼šæ”¯æŒæ¨¡æ¿è‡ªå®šä¹‰ã€ä»£ç é£æ ¼é…ç½®

**ç”Ÿæˆç¤ºä¾‹**ï¼š

```bash
openapi-generator generate \
  -i api.yaml \
  -g python \
  -o ./generated/python-client
```

### 3.2 AsyncAPIä»£ç ç”Ÿæˆ

**AsyncAPI Generator**ï¼š

- **è¾“å…¥**ï¼šAsyncAPIè§„èŒƒæ–‡ä»¶
- **è¾“å‡º**ï¼šKafka/AMQPä»£ç æ¨¡æ¿
- **ç‰¹æ€§**ï¼šæ”¯æŒæ¶ˆæ¯å¤„ç†é€»è¾‘ç”Ÿæˆ

**ç”Ÿæˆç¤ºä¾‹**ï¼š

```bash
asyncapi-generator generate \
  -i asyncapi.yaml \
  -g kafka \
  -o ./generated/kafka-client
```

### 3.3 IoTSchemaé€‚é…

**AIå°†è®¾å¤‡åè®®æ˜ å°„åˆ°JSON Schema**ï¼š

- **è¾“å…¥**ï¼šè®¾å¤‡åè®®æè¿°ï¼ˆMQTTä¸»é¢˜ç»“æ„ï¼‰
- **è¾“å‡º**ï¼šIoTSchemaçš„JSON Schema
- **ç¡®ä¿**ï¼šæ•°æ®ä¸€è‡´æ€§ã€ç±»å‹å®‰å…¨

---

## 4. è½¬æ¢å·¥å…·

### 4.1 OpenAPI Generator

**åŠŸèƒ½**ï¼š

- ç”Ÿæˆå¤šè¯­è¨€å®¢æˆ·ç«¯ä»£ç 
- ç”ŸæˆæœåŠ¡å™¨ç«¯ä»£ç 
- ç”ŸæˆAPIæ–‡æ¡£

**æ”¯æŒè¯­è¨€**ï¼š

- Pythonã€Node.jsã€Goã€Javaã€C#ã€PHPç­‰50+ç§è¯­è¨€

### 4.2 AsyncAPI Generator

**åŠŸèƒ½**ï¼š

- ç”Ÿæˆæ¶ˆæ¯é˜Ÿåˆ—å®¢æˆ·ç«¯ä»£ç 
- ç”Ÿæˆæ¶ˆæ¯å¤„ç†é€»è¾‘
- ç”Ÿæˆæµ‹è¯•ä»£ç 

**æ”¯æŒåè®®**ï¼š

- Kafkaã€RabbitMQã€MQTTã€AMQPç­‰

### 4.3 è‡ªå®šä¹‰è½¬æ¢å·¥å…·

**å¼€å‘è‡ªå®šä¹‰è½¬æ¢å™¨**ï¼š

- åŸºäºæ¨¡æ¿å¼•æ“ï¼ˆJinja2ã€Handlebarsï¼‰
- æ”¯æŒè‡ªå®šä¹‰è½¬æ¢è§„åˆ™
- æ”¯æŒå¤šæ ¼å¼è¾“å‡º

---

## 5. è½¬æ¢å®è·µ

### 5.1 å®è·µæµç¨‹

1. **éœ€æ±‚åˆ†æ**ï¼šç†è§£ç”¨æˆ·éœ€æ±‚
2. **DSLç”Ÿæˆ**ï¼šAIç”ŸæˆDSLè§„èŒƒ
3. **è§„èŒƒéªŒè¯**ï¼šéªŒè¯DSLè§„èŒƒæ­£ç¡®æ€§
4. **ä»£ç ç”Ÿæˆ**ï¼šç”Ÿæˆå¯æ‰§è¡Œä»£ç 
5. **ä»£ç æµ‹è¯•**ï¼šè‡ªåŠ¨æµ‹è¯•ç”Ÿæˆä»£ç 
6. **éƒ¨ç½²ä¸Šçº¿**ï¼šéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

### 5.2 æœ€ä½³å®è·µ

- **è¿­ä»£ä¼˜åŒ–**ï¼šæ ¹æ®åé¦ˆä¸æ–­ä¼˜åŒ–è½¬æ¢è§„åˆ™
- **æ¨¡æ¿ç®¡ç†**ï¼šç»Ÿä¸€ç®¡ç†ä»£ç ç”Ÿæˆæ¨¡æ¿
- **ç‰ˆæœ¬æ§åˆ¶**ï¼šå¯¹ç”Ÿæˆçš„ä»£ç è¿›è¡Œç‰ˆæœ¬æ§åˆ¶
- **è‡ªåŠ¨åŒ–æµ‹è¯•**ï¼šè‡ªåŠ¨æµ‹è¯•ç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§

---

## 6. æ•°æ®åº“å­˜å‚¨ä¸åˆ†æ

### 6.1 PostgreSQLæ•°æ®å­˜å‚¨

**è¡¨ç»“æ„è®¾è®¡**ï¼š

```sql
-- è½¬æ¢ä»»åŠ¡è¡¨
CREATE TABLE dsl_conversion_tasks (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(100) UNIQUE NOT NULL,
    source_type VARCHAR(50) NOT NULL,  -- OpenAPI, AsyncAPI, IoTSchema
    target_type VARCHAR(50) NOT NULL,
    source_schema JSONB NOT NULL,
    target_schema JSONB,
    conversion_status VARCHAR(20) DEFAULT 'PENDING',  -- PENDING, PROCESSING, COMPLETED, FAILED
    conversion_result JSONB,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP
);

-- è½¬æ¢è§„åˆ™è¡¨
CREATE TABLE conversion_rules (
    id SERIAL PRIMARY KEY,
    rule_name VARCHAR(200) NOT NULL,
    source_type VARCHAR(50) NOT NULL,
    target_type VARCHAR(50) NOT NULL,
    rule_definition JSONB NOT NULL,
    rule_priority INTEGER DEFAULT 0,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- è½¬æ¢å†å²è¡¨
CREATE TABLE conversion_history (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(100) NOT NULL REFERENCES dsl_conversion_tasks(task_id),
    conversion_step VARCHAR(100) NOT NULL,
    step_input JSONB,
    step_output JSONB,
    execution_time_ms INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_conversion_tasks_status ON dsl_conversion_tasks(conversion_status);
CREATE INDEX idx_conversion_tasks_source_target ON dsl_conversion_tasks(source_type, target_type);
CREATE INDEX idx_conversion_tasks_created_at ON dsl_conversion_tasks(created_at);
CREATE INDEX idx_conversion_rules_source_target ON conversion_rules(source_type, target_type);
CREATE INDEX idx_conversion_history_task_id ON conversion_history(task_id);
```

**Pythonå­˜å‚¨å®ç°**ï¼š

```python
import psycopg2
import json
from datetime import datetime
from typing import Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)

class DSLConversionStorage:
    """DSLè½¬æ¢æ•°æ®å­˜å‚¨ç±»"""

    def __init__(self, db_config: Dict[str, Any]):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        self.conn = psycopg2.connect(
            host=db_config['host'],
            port=db_config['port'],
            database=db_config['database'],
            user=db_config['user'],
            password=db_config['password']
        )
        self.cur = self.conn.cursor()

    def create_conversion_task(self, task_id: str, source_type: str,
                              target_type: str, source_schema: Dict) -> int:
        """åˆ›å»ºè½¬æ¢ä»»åŠ¡"""
        try:
            self.cur.execute("""
                INSERT INTO dsl_conversion_tasks
                (task_id, source_type, target_type, source_schema)
                VALUES (%s, %s, %s, %s::jsonb)
                RETURNING id
            """, (task_id, source_type, target_type, json.dumps(source_schema)))

            task_db_id = self.cur.fetchone()[0]
            self.conn.commit()
            logger.info(f"Created conversion task: {task_id}")
            return task_db_id
        except psycopg2.IntegrityError as e:
            logger.error(f"Task {task_id} already exists: {e}")
            self.conn.rollback()
            raise ValueError(f"Task {task_id} already exists") from e
        except Exception as e:
            logger.error(f"Failed to create conversion task: {e}")
            self.conn.rollback()
            raise

    def update_conversion_result(self, task_id: str, target_schema: Dict,
                                status: str, error_message: Optional[str] = None):
        """æ›´æ–°è½¬æ¢ç»“æœ"""
        try:
            self.cur.execute("""
                UPDATE dsl_conversion_tasks
                SET target_schema = %s::jsonb,
                    conversion_status = %s,
                    error_message = %s,
                    completed_at = CASE WHEN %s = 'COMPLETED' THEN CURRENT_TIMESTAMP ELSE NULL END,
                    updated_at = CURRENT_TIMESTAMP
                WHERE task_id = %s
            """, (json.dumps(target_schema), status, error_message, status, task_id))

            self.conn.commit()
            logger.info(f"Updated conversion task {task_id} with status {status}")
        except Exception as e:
            logger.error(f"Failed to update conversion result: {e}")
            self.conn.rollback()
            raise

    def add_conversion_history(self, task_id: str, step: str,
                              step_input: Dict, step_output: Dict,
                              execution_time_ms: int):
        """æ·»åŠ è½¬æ¢å†å²è®°å½•"""
        try:
            self.cur.execute("""
                INSERT INTO conversion_history
                (task_id, conversion_step, step_input, step_output, execution_time_ms)
                VALUES (%s, %s, %s::jsonb, %s::jsonb, %s)
            """, (task_id, step, json.dumps(step_input),
                  json.dumps(step_output), execution_time_ms))

            self.conn.commit()
        except Exception as e:
            logger.error(f"Failed to add conversion history: {e}")
            self.conn.rollback()
            raise

    def get_conversion_statistics(self, start_time: datetime, end_time: datetime) -> Dict:
        """è·å–è½¬æ¢ç»Ÿè®¡ä¿¡æ¯"""
        try:
            self.cur.execute("""
                SELECT
                    source_type,
                    target_type,
                    conversion_status,
                    COUNT(*) as count,
                    AVG(EXTRACT(EPOCH FROM (completed_at - created_at))) as avg_duration_seconds
                FROM dsl_conversion_tasks
                WHERE created_at >= %s AND created_at <= %s
                GROUP BY source_type, target_type, conversion_status
            """, (start_time, end_time))

            results = []
            for row in self.cur.fetchall():
                results.append({
                    'source_type': row[0],
                    'target_type': row[1],
                    'status': row[2],
                    'count': row[3],
                    'avg_duration_seconds': float(row[4]) if row[4] else None
                })

            return results
        except Exception as e:
            logger.error(f"Failed to get conversion statistics: {e}")
            raise

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.cur.close()
        self.conn.close()
```

### 6.2 æ•°æ®åˆ†ææŸ¥è¯¢ç¤ºä¾‹

**æŸ¥è¯¢è½¬æ¢æˆåŠŸç‡**ï¼š

```python
# æŸ¥è¯¢å„ç±»å‹è½¬æ¢çš„æˆåŠŸç‡
storage.cur.execute("""
    SELECT
        source_type,
        target_type,
        COUNT(*) as total,
        COUNT(CASE WHEN conversion_status = 'COMPLETED' THEN 1 END) as completed,
        ROUND(100.0 * COUNT(CASE WHEN conversion_status = 'COMPLETED' THEN 1 END) / COUNT(*), 2) as success_rate
    FROM dsl_conversion_tasks
    WHERE created_at >= %s
    GROUP BY source_type, target_type
    ORDER BY success_rate DESC
""", (start_time,))
```

**æŸ¥è¯¢è½¬æ¢æ€§èƒ½**ï¼š

```python
# æŸ¥è¯¢å¹³å‡è½¬æ¢æ—¶é—´
storage.cur.execute("""
    SELECT
        source_type,
        target_type,
        AVG(EXTRACT(EPOCH FROM (completed_at - created_at))) as avg_duration,
        MIN(EXTRACT(EPOCH FROM (completed_at - created_at))) as min_duration,
        MAX(EXTRACT(EPOCH FROM (completed_at - created_at))) as max_duration
    FROM dsl_conversion_tasks
    WHERE conversion_status = 'COMPLETED' AND created_at >= %s
    GROUP BY source_type, target_type
""", (start_time,))
```

**æŸ¥è¯¢è½¬æ¢å†å²è¯¦æƒ…**ï¼š

```python
# æŸ¥è¯¢ç‰¹å®šä»»åŠ¡çš„è½¬æ¢å†å²
storage.cur.execute("""
    SELECT
        conversion_step,
        step_input,
        step_output,
        execution_time_ms,
        created_at
    FROM conversion_history
    WHERE task_id = %s
    ORDER BY created_at ASC
""", (task_id,))
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - ä¸‰å¤§Schemaå·®å¼‚åˆ†æ
- `03_Standards.md` - MCPåè®®æ ‡å‡†åŒ–
- `05_Case_Studies.md` - å®è·µæ¡ˆä¾‹

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
