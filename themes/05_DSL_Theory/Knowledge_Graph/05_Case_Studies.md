# çŸ¥è¯†å›¾è°±Schemaå®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [çŸ¥è¯†å›¾è°±Schemaå®è·µæ¡ˆä¾‹](#çŸ¥è¯†å›¾è°±schemaå®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¡ˆä¾‹æ¦‚è¿°](#1-æ¡ˆä¾‹æ¦‚è¿°)
  - [2. æ¡ˆä¾‹1ï¼šSchemaè½¬æ¢æŒ‡å¯¼çŸ¥è¯†å›¾è°±](#2-æ¡ˆä¾‹1schemaè½¬æ¢æŒ‡å¯¼çŸ¥è¯†å›¾è°±)
    - [2.1 åœºæ™¯æè¿°](#21-åœºæ™¯æè¿°)
    - [2.2 Schemaå®šä¹‰](#22-schemaå®šä¹‰)
    - [2.3 å®ç°ä»£ç ](#23-å®ç°ä»£ç )
    - [2.4 éªŒè¯ç»“æœ](#24-éªŒè¯ç»“æœ)
  - [3. æ¡ˆä¾‹2ï¼šçŸ¥è¯†æ¨ç†ç³»ç»Ÿ](#3-æ¡ˆä¾‹2çŸ¥è¯†æ¨ç†ç³»ç»Ÿ)
    - [3.1 åœºæ™¯æè¿°](#31-åœºæ™¯æè¿°)
    - [3.2 Schemaå®šä¹‰](#32-schemaå®šä¹‰)
    - [3.3 å®ç°ä»£ç ](#33-å®ç°ä»£ç )
    - [3.4 æ•ˆæœè¯„ä¼°](#34-æ•ˆæœè¯„ä¼°)
  - [4. æ¡ˆä¾‹3ï¼šè´¨é‡è¯„ä¼°çŸ¥è¯†å›¾è°±](#4-æ¡ˆä¾‹3è´¨é‡è¯„ä¼°çŸ¥è¯†å›¾è°±)
    - [4.1 åœºæ™¯æè¿°](#41-åœºæ™¯æè¿°)
    - [4.2 Schemaå®šä¹‰](#42-schemaå®šä¹‰)
    - [4.3 å®ç°ä»£ç ](#43-å®ç°ä»£ç )
    - [4.4 åº”ç”¨æ•ˆæœ](#44-åº”ç”¨æ•ˆæœ)
  - [5. æ¡ˆä¾‹æ€»ç»“](#5-æ¡ˆä¾‹æ€»ç»“)
    - [5.1 æˆåŠŸå› ç´ ](#51-æˆåŠŸå› ç´ )
    - [5.2 æœ€ä½³å®è·µ](#52-æœ€ä½³å®è·µ)
  - [6. å‚è€ƒæ–‡çŒ®](#6-å‚è€ƒæ–‡çŒ®)

---

## 1. æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›çŸ¥è¯†å›¾è°±Schemaåœ¨å®é™…åº”ç”¨ä¸­çš„
å®è·µæ¡ˆä¾‹ï¼Œå±•ç¤ºçŸ¥è¯†è¡¨ç¤ºã€æ¨ç†ã€åº”ç”¨ç­‰
å®Œæ•´æµç¨‹ã€‚

**æ¡ˆä¾‹ç±»å‹**ï¼š

1. **Schemaè½¬æ¢æŒ‡å¯¼**ï¼šè½¬æ¢è·¯å¾„æ¨èå’Œè§„åˆ™åŒ¹é…
2. **çŸ¥è¯†æ¨ç†**ï¼šç±»å‹å’Œçº¦æŸå…³ç³»æ¨ç†
3. **è´¨é‡è¯„ä¼°**ï¼šè½¬æ¢è´¨é‡è¯„ä¼°

---

## 2. æ¡ˆä¾‹1ï¼šSchemaè½¬æ¢æŒ‡å¯¼çŸ¥è¯†å›¾è°±

### 2.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
ä½¿ç”¨çŸ¥è¯†å›¾è°±æŒ‡å¯¼DSL Schemaè½¬æ¢ï¼Œ
æ¨èè½¬æ¢è·¯å¾„ã€åŒ¹é…è½¬æ¢è§„åˆ™ã€
è¯„ä¼°è½¬æ¢è´¨é‡ã€‚

**éœ€æ±‚åˆ†æ**ï¼š

- **è½¬æ¢è·¯å¾„æ¨è**ï¼šæ¨èæœ€ä¼˜è½¬æ¢è·¯å¾„
- **è½¬æ¢è§„åˆ™åŒ¹é…**ï¼šåŒ¹é…é€‚ç”¨çš„è½¬æ¢è§„åˆ™
- **è½¬æ¢è´¨é‡è¯„ä¼°**ï¼šè¯„ä¼°è½¬æ¢è´¨é‡

### 2.2 Schemaå®šä¹‰

**çŸ¥è¯†å›¾è°±Schemaå®šä¹‰**ï¼š

```dsl
schema SchemaTransformationKG {
  entities: {
    Schema: {
      properties: {
        name: String
        type: Enum { PLC, CAN, IoT }
        version: String
      }
    }
    Transformation: {
      properties: {
        name: String
        source_type: String
        target_type: String
        accuracy: Float64
      }
    }
    Rule: {
      properties: {
        name: String
        condition: Expression
        action: Function
      }
    }
  }

  relations: {
    transforms_to: {
      domain: Schema
      range: Schema
      properties: {
        transformation: Transformation
        quality: Float64
      }
    }
    has_rule: {
      domain: Transformation
      range: Rule
    }
    applies_to: {
      domain: Rule
      range: Schema
    }
  }

  inference: {
    rules: {
      transitive_transformation: {
        premise: [
          transforms_to(s1, s2),
          transforms_to(s2, s3)
        ]
        conclusion: transforms_to(s1, s3)
      }
      rule_matching: {
        premise: [
          has_rule(t, r),
          applies_to(r, s),
          transforms_to(s, t)
        ]
        conclusion: applicable_rule(r, s, t)
      }
    }
  }
}
```

### 2.3 å®ç°ä»£ç 

**Pythonå®ç°**ï¼š

```python
from rdflib import Graph, Namespace, RDF, RDFS
from typing import List, Dict, Optional
from dataclasses import dataclass

@dataclass
class TransformationPath:
    """è½¬æ¢è·¯å¾„"""
    source: str
    target: str
    path: List[str]
    quality: float

class SchemaTransformationKG:
    """Schemaè½¬æ¢çŸ¥è¯†å›¾è°±"""

    def __init__(self):
        self.graph = Graph()
        self.ns = Namespace("http://example.org/kg#")
        self._initialize_graph()

    def _initialize_graph(self):
        """åˆå§‹åŒ–å›¾è°±"""
        # æ·»åŠ Schemaå®ä½“
        self.graph.add((self.ns.PLC_Schema, RDF.type, self.ns.Schema))
        self.graph.add((self.ns.CAN_Schema, RDF.type, self.ns.Schema))
        self.graph.add((self.ns.IoT_Schema, RDF.type, self.ns.Schema))

        # æ·»åŠ è½¬æ¢å…³ç³»
        self.graph.add((
            self.ns.PLC_Schema,
            self.ns.transforms_to,
            self.ns.CAN_Schema
        ))
        self.graph.add((
            self.ns.CAN_Schema,
            self.ns.transforms_to,
            self.ns.IoT_Schema
        ))

    def recommend_path(self, source: str, target: str) -> Optional[TransformationPath]:
        """æ¨èè½¬æ¢è·¯å¾„"""
        # ä½¿ç”¨SPARQLæŸ¥è¯¢è·¯å¾„
        query = f"""
        PREFIX kg: <http://example.org/kg#>
        SELECT ?path ?quality WHERE {{
            ?source kg:transforms_to* ?target .
            ?source kg:quality ?quality .
        }}
        """

        results = self.graph.query(query)
        if results:
            # æ„å»ºè·¯å¾„
            path = self._build_path(source, target)
            quality = self._calculate_quality(path)
            return TransformationPath(
                source=source,
                target=target,
                path=path,
                quality=quality
            )
        return None

    def match_rules(self, source: str, target: str) -> List[str]:
        """åŒ¹é…è½¬æ¢è§„åˆ™"""
        query = f"""
        PREFIX kg: <http://example.org/kg#>
        SELECT ?rule WHERE {{
            ?transformation kg:has_rule ?rule .
            ?rule kg:applies_to ?source .
            ?source kg:transforms_to ?target .
        }}
        """

        results = self.graph.query(query)
        return [str(row.rule) for row in results]

    def assess_quality(self, source: str, target: str) -> float:
        """è¯„ä¼°è½¬æ¢è´¨é‡"""
        path = self.recommend_path(source, target)
        if path:
            return path.quality
        return 0.0

    def _build_path(self, source: str, target: str) -> List[str]:
        """æ„å»ºè·¯å¾„"""
        # å®ç°è·¯å¾„æŸ¥æ‰¾ç®—æ³•
        return [source, target]

    def _calculate_quality(self, path: List[str]) -> float:
        """è®¡ç®—è·¯å¾„è´¨é‡"""
        # å®ç°è´¨é‡è®¡ç®—ç®—æ³•
        return 0.9
```

### 2.4 éªŒè¯ç»“æœ

**éªŒè¯æŒ‡æ ‡**ï¼š

- **è·¯å¾„æ¨èå‡†ç¡®ç‡**ï¼šè·¯å¾„æ¨èå‡†ç¡®ç‡ > 90%
- **è§„åˆ™åŒ¹é…å‡†ç¡®ç‡**ï¼šè§„åˆ™åŒ¹é…å‡†ç¡®ç‡ > 85%
- **è´¨é‡è¯„ä¼°å‡†ç¡®ç‡**ï¼šè´¨é‡è¯„ä¼°å‡†ç¡®ç‡ > 88%

---

## 3. æ¡ˆä¾‹2ï¼šçŸ¥è¯†æ¨ç†ç³»ç»Ÿ

### 3.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
åŸºäºçŸ¥è¯†å›¾è°±è¿›è¡ŒçŸ¥è¯†æ¨ç†ï¼Œ
æ¨æ–­ç±»å‹å…³ç³»ã€çº¦æŸå…³ç³»ã€
è½¬æ¢å…³ç³»ç­‰ã€‚

**éœ€æ±‚åˆ†æ**ï¼š

- **ç±»å‹æ¨ç†**ï¼šæ¨æ–­ç±»å‹å…³ç³»
- **çº¦æŸæ¨ç†**ï¼šæ¨æ–­çº¦æŸå…³ç³»
- **è½¬æ¢æ¨ç†**ï¼šæ¨æ–­è½¬æ¢å…³ç³»

### 3.2 Schemaå®šä¹‰

**çŸ¥è¯†æ¨ç†Schemaå®šä¹‰**ï¼š

```dsl
schema KnowledgeInferenceKG {
  entities: {
    Type: {
      properties: {
        name: String
        parent: Optional<Type>
      }
    }
    Constraint: {
      properties: {
        name: String
        expression: Expression
      }
    }
  }

  relations: {
    subsumes: {
      domain: Type
      range: Type
      properties: {
        transitive: Boolean @value(true)
      }
    }
    has_constraint: {
      domain: Type
      range: Constraint
    }
  }

  inference: {
    rules: {
      type_inheritance: {
        premise: [
          subsumes(t1, t2),
          has_constraint(t1, c)
        ]
        conclusion: has_constraint(t2, c)
      }
      type_transitivity: {
        premise: [
          subsumes(t1, t2),
          subsumes(t2, t3)
        ]
        conclusion: subsumes(t1, t3)
      }
    }
  }
}
```

### 3.3 å®ç°ä»£ç 

**Pythonå®ç°**ï¼š

```python
from owlready2 import *
from typing import List, Set

class KnowledgeInferenceSystem:
    """çŸ¥è¯†æ¨ç†ç³»ç»Ÿ"""

    def __init__(self):
        self.onto = get_ontology("http://example.org/inference")
        self._initialize_ontology()

    def _initialize_ontology(self):
        """åˆå§‹åŒ–æœ¬ä½“"""
        with self.onto:
            # å®šä¹‰ç±»å‹
            class Type(Thing):
                pass

            class Integer(Type):
                pass

            class Float(Type):
                pass

            # å®šä¹‰çº¦æŸ
            class Constraint(Thing):
                pass

            class RangeConstraint(Constraint):
                pass

    def infer_type_relations(self, type1: str, type2: str) -> bool:
        """æ¨æ–­ç±»å‹å…³ç³»"""
        with self.onto:
            t1 = self.onto.search_one(iri=f"*{type1}")
            t2 = self.onto.search_one(iri=f"*{type2}")

            if t1 and t2:
                # æ£€æŸ¥å­ç±»å‹å…³ç³»
                return issubclass(t2, t1)
        return False

    def infer_constraints(self, type_name: str) -> List[str]:
        """æ¨æ–­çº¦æŸ"""
        with self.onto:
            t = self.onto.search_one(iri=f"*{type_name}")
            if t:
                # æŸ¥æ‰¾æ‰€æœ‰çº¦æŸ
                constraints = []
                for constraint in self.onto.Constraint.instances():
                    if hasattr(t, constraint.name):
                        constraints.append(constraint.name)
                return constraints
        return []

    def infer_transformations(self, source: str, target: str) -> List[str]:
        """æ¨æ–­è½¬æ¢"""
        # å®ç°è½¬æ¢æ¨ç†é€»è¾‘
        return []
```

### 3.4 æ•ˆæœè¯„ä¼°

**è¯„ä¼°æŒ‡æ ‡**ï¼š

- **æ¨ç†å‡†ç¡®ç‡**ï¼šæ¨ç†å‡†ç¡®ç‡ > 92%
- **æ¨ç†æ•ˆç‡**ï¼šæ¨ç†æ—¶é—´ < 100ms
- **çŸ¥è¯†è¦†ç›–ç‡**ï¼šçŸ¥è¯†è¦†ç›–ç‡ > 85%

---

## 4. æ¡ˆä¾‹3ï¼šè´¨é‡è¯„ä¼°çŸ¥è¯†å›¾è°±

### 4.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
ä½¿ç”¨çŸ¥è¯†å›¾è°±è¯„ä¼°Schemaè½¬æ¢è´¨é‡ï¼Œ
åŒ…æ‹¬ä¿¡æ¯æŸå¤±è¯„ä¼°ã€è¯­ä¹‰ç­‰ä»·æ€§è¯„ä¼°ã€
ç±»å‹å®‰å…¨æ€§è¯„ä¼°ç­‰ã€‚

**éœ€æ±‚åˆ†æ**ï¼š

- **ä¿¡æ¯æŸå¤±è¯„ä¼°**ï¼šè¯„ä¼°è½¬æ¢ä¿¡æ¯æŸå¤±
- **è¯­ä¹‰ç­‰ä»·æ€§è¯„ä¼°**ï¼šè¯„ä¼°è¯­ä¹‰ç­‰ä»·æ€§
- **ç±»å‹å®‰å…¨æ€§è¯„ä¼°**ï¼šè¯„ä¼°ç±»å‹å®‰å…¨æ€§

### 4.2 Schemaå®šä¹‰

**è´¨é‡è¯„ä¼°çŸ¥è¯†å›¾è°±Schemaå®šä¹‰**ï¼š

```dsl
schema QualityAssessmentKG {
  entities: {
    Transformation: {
      properties: {
        name: String
        source: String
        target: String
        information_loss: Float64
        semantic_equivalence: Float64
        type_safety: Float64
      }
    }
    Metric: {
      properties: {
        name: String
        type: Enum { information_loss, semantic, type_safety }
        weight: Float64
      }
    }
  }

  relations: {
    has_metric: {
      domain: Transformation
      range: Metric
    }
    assessed_by: {
      domain: Transformation
      range: Metric
    }
  }

  inference: {
    rules: {
      quality_calculation: {
        premise: [
          has_metric(t, m1),
          has_metric(t, m2),
          has_metric(t, m3)
        ]
        conclusion: quality(t) = weighted_sum(m1, m2, m3)
      }
    }
  }
}
```

### 4.3 å®ç°ä»£ç 

**Pythonå®ç°**ï¼š

```python
@dataclass
class QualityMetrics:
    """è´¨é‡æŒ‡æ ‡"""
    information_loss: float
    semantic_equivalence: float
    type_safety: float

    def overall_quality(self, weights: Dict[str, float]) -> float:
        """è®¡ç®—æ€»ä½“è´¨é‡"""
        return (
            self.information_loss * weights.get("information_loss", 0.33) +
            self.semantic_equivalence * weights.get("semantic", 0.33) +
            self.type_safety * weights.get("type_safety", 0.34)
        )

class QualityAssessmentKG:
    """è´¨é‡è¯„ä¼°çŸ¥è¯†å›¾è°±"""

    def __init__(self):
        self.graph = Graph()
        self.ns = Namespace("http://example.org/quality#")

    def assess_transformation(self,
                             source: str,
                             target: str) -> QualityMetrics:
        """è¯„ä¼°è½¬æ¢è´¨é‡"""
        # è®¡ç®—ä¿¡æ¯æŸå¤±
        info_loss = self._calculate_information_loss(source, target)

        # è®¡ç®—è¯­ä¹‰ç­‰ä»·æ€§
        semantic_eq = self._calculate_semantic_equivalence(source, target)

        # è®¡ç®—ç±»å‹å®‰å…¨æ€§
        type_safety = self._calculate_type_safety(source, target)

        return QualityMetrics(
            information_loss=info_loss,
            semantic_equivalence=semantic_eq,
            type_safety=type_safety
        )

    def _calculate_information_loss(self, source: str, target: str) -> float:
        """è®¡ç®—ä¿¡æ¯æŸå¤±"""
        # å®ç°ä¿¡æ¯æŸå¤±è®¡ç®—
        return 0.1

    def _calculate_semantic_equivalence(self, source: str, target: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç­‰ä»·æ€§"""
        # å®ç°è¯­ä¹‰ç­‰ä»·æ€§è®¡ç®—
        return 0.95

    def _calculate_type_safety(self, source: str, target: str) -> float:
        """è®¡ç®—ç±»å‹å®‰å…¨æ€§"""
        # å®ç°ç±»å‹å®‰å…¨æ€§è®¡ç®—
        return 0.98
```

### 4.4 åº”ç”¨æ•ˆæœ

**æ•ˆæœæŒ‡æ ‡**ï¼š

- **è¯„ä¼°å‡†ç¡®ç‡**ï¼šè´¨é‡è¯„ä¼°å‡†ç¡®ç‡ > 90%
- **è¯„ä¼°æ•ˆç‡**ï¼šè¯„ä¼°æ—¶é—´ < 50ms
- **æŒ‡å¯¼æ•ˆæœ**ï¼šè½¬æ¢è´¨é‡æå‡ 20%

---

## 5. æ¡ˆä¾‹4ï¼šPostgreSQLçŸ¥è¯†å›¾è°±è½¬æ¢ç³»ç»Ÿ

### 5.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
å°†DSL SchemaçŸ¥è¯†å›¾è°±å­˜å‚¨åˆ°PostgreSQLæ•°æ®åº“ï¼Œ
æ”¯æŒé«˜æ•ˆæŸ¥è¯¢ã€è·¯å¾„æŸ¥æ‰¾å’ŒçŸ¥è¯†æ¨ç†ã€‚

**éœ€æ±‚åˆ†æ**ï¼š

- **å­˜å‚¨æ–¹æ¡ˆ**ï¼šä½¿ç”¨PostgreSQL JSONBå­˜å‚¨çŸ¥è¯†å›¾è°±
- **æŸ¥è¯¢æ€§èƒ½**ï¼šæ”¯æŒé«˜æ•ˆå®ä½“å’Œå…³ç³»æŸ¥è¯¢
- **è·¯å¾„æŸ¥æ‰¾**ï¼šæ”¯æŒå®ä½“é—´è·¯å¾„æŸ¥æ‰¾
- **æ‰©å±•æ€§**ï¼šæ”¯æŒå¤§è§„æ¨¡çŸ¥è¯†å›¾è°±å­˜å‚¨

### 5.2 Schemaå®šä¹‰

**PostgreSQLçŸ¥è¯†å›¾è°±Schemaå®šä¹‰**ï¼š

```sql
-- å®ä½“è¡¨
CREATE TABLE kg_entities (
    id SERIAL PRIMARY KEY,
    uri VARCHAR(500) UNIQUE NOT NULL,
    type VARCHAR(100) NOT NULL,
    properties JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- å…³ç³»è¡¨
CREATE TABLE kg_relations (
    id SERIAL PRIMARY KEY,
    subject_uri VARCHAR(500) NOT NULL,
    predicate VARCHAR(200) NOT NULL,
    object_uri VARCHAR(500),
    object_value JSONB,
    properties JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (subject_uri) REFERENCES kg_entities(uri),
    FOREIGN KEY (object_uri) REFERENCES kg_entities(uri)
);

-- ç´¢å¼•
CREATE INDEX idx_entities_type ON kg_entities(type);
CREATE INDEX idx_entities_properties ON kg_entities USING GIN(properties);
CREATE INDEX idx_relations_subject ON kg_relations(subject_uri);
CREATE INDEX idx_relations_predicate ON kg_relations(predicate);
CREATE INDEX idx_relations_object ON kg_relations(object_uri);
```

### 5.3 å®ç°ä»£ç 

**å®Œæ•´PostgreSQLçŸ¥è¯†å›¾è°±ç³»ç»Ÿ**ï¼š

```python
import psycopg2
import json
from typing import List, Dict, Optional
from rdflib import Graph, Namespace, RDF, RDFS
from dataclasses import dataclass
from datetime import datetime

@dataclass
class Entity:
    """å®ä½“"""
    uri: str
    type: str
    properties: Dict

@dataclass
class Relation:
    """å…³ç³»"""
    subject_uri: str
    predicate: str
    object_uri: Optional[str]
    object_value: Optional[Dict]

class PostgreSQLKnowledgeGraph:
    """PostgreSQLçŸ¥è¯†å›¾è°±ç³»ç»Ÿ"""

    def __init__(self, connection_string: str):
        self.conn = psycopg2.connect(connection_string)
        self.cur = self.conn.cursor()
        self._initialize_schema()

    def _initialize_schema(self):
        """åˆå§‹åŒ–æ•°æ®åº“Schema"""
        # åˆ›å»ºè¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS kg_entities (
                id SERIAL PRIMARY KEY,
                uri VARCHAR(500) UNIQUE NOT NULL,
                type VARCHAR(100) NOT NULL,
                properties JSONB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS kg_relations (
                id SERIAL PRIMARY KEY,
                subject_uri VARCHAR(500) NOT NULL,
                predicate VARCHAR(200) NOT NULL,
                object_uri VARCHAR(500),
                object_value JSONB,
                properties JSONB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (subject_uri) REFERENCES kg_entities(uri) ON DELETE CASCADE,
                FOREIGN KEY (object_uri) REFERENCES kg_entities(uri) ON DELETE CASCADE
            )
        """)

        # åˆ›å»ºç´¢å¼•
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_entities_type ON kg_entities(type)",
            "CREATE INDEX IF NOT EXISTS idx_entities_properties ON kg_entities USING GIN(properties)",
            "CREATE INDEX IF NOT EXISTS idx_relations_subject ON kg_relations(subject_uri)",
            "CREATE INDEX IF NOT EXISTS idx_relations_predicate ON kg_relations(predicate)",
            "CREATE INDEX IF NOT EXISTS idx_relations_object ON kg_relations(object_uri)"
        ]

        for index_sql in indexes:
            self.cur.execute(index_sql)

        self.conn.commit()

    def import_rdf(self, rdf_graph: Graph):
        """å¯¼å…¥RDFå›¾"""
        # æå–å®ä½“
        entities = {}
        for subject, predicate, obj in rdf_graph:
            # å¤„ç†subject
            if subject not in entities:
                entity_type = self._get_entity_type(rdf_graph, subject)
                properties = self._extract_properties(rdf_graph, subject)
                entities[subject] = Entity(
                    uri=str(subject),
                    type=entity_type,
                    properties=properties
                )

            # å¤„ç†objectï¼ˆå¦‚æœæ˜¯URIï¼‰
            if hasattr(obj, 'toPython') and not isinstance(obj, str):
                obj_str = str(obj)
                if obj_str not in entities and obj_str.startswith('http'):
                    entity_type = self._get_entity_type(rdf_graph, obj)
                    properties = self._extract_properties(rdf_graph, obj)
                    entities[obj] = Entity(
                        uri=obj_str,
                        type=entity_type,
                        properties=properties
                    )

        # æ’å…¥å®ä½“
        for entity in entities.values():
            self.cur.execute("""
                INSERT INTO kg_entities (uri, type, properties)
                VALUES (%s, %s, %s::jsonb)
                ON CONFLICT (uri) DO UPDATE
                SET type = EXCLUDED.type,
                    properties = EXCLUDED.properties,
                    updated_at = CURRENT_TIMESTAMP
            """, (entity.uri, entity.type, json.dumps(entity.properties)))

        # æ’å…¥å…³ç³»
        for subject, predicate, obj in rdf_graph:
            predicate_str = str(predicate).split('#')[-1].split('/')[-1]
            obj_str = str(obj)

            # åˆ¤æ–­objectæ˜¯URIè¿˜æ˜¯å­—é¢é‡
            if hasattr(obj, 'toPython') and not isinstance(obj, str):
                object_uri = obj_str if obj_str.startswith('http') else None
                object_value = None
            else:
                object_uri = None
                object_value = {'value': obj_str, 'type': 'literal'}

            self.cur.execute("""
                INSERT INTO kg_relations
                (subject_uri, predicate, object_uri, object_value)
                VALUES (%s, %s, %s, %s::jsonb)
            """, (
                str(subject),
                predicate_str,
                object_uri,
                json.dumps(object_value) if object_value else None
            ))

        self.conn.commit()

    def get_entity(self, uri: str) -> Optional[Entity]:
        """è·å–å®ä½“"""
        self.cur.execute("""
            SELECT uri, type, properties FROM kg_entities WHERE uri = %s
        """, (uri,))

        row = self.cur.fetchone()
        if row:
            return Entity(
                uri=row[0],
                type=row[1],
                properties=row[2] if row[2] else {}
            )
        return None

    def get_entities_by_type(self, entity_type: str,
                            filters: Dict = None) -> List[Entity]:
        """æŒ‰ç±»å‹æŸ¥è¯¢å®ä½“"""
        query = """
            SELECT uri, type, properties FROM kg_entities
            WHERE type = %s
        """
        params = [entity_type]

        if filters:
            for key, value in filters.items():
                query += f" AND properties @> %s::jsonb"
                params.append(json.dumps({key: value}))

        self.cur.execute(query, params)
        entities = []
        for row in self.cur.fetchall():
            entities.append(Entity(
                uri=row[0],
                type=row[1],
                properties=row[2] if row[2] else {}
            ))
        return entities

    def get_relations(self, subject_uri: str = None,
                     predicate: str = None,
                     object_uri: str = None) -> List[Relation]:
        """æŸ¥è¯¢å…³ç³»"""
        query = """
            SELECT subject_uri, predicate, object_uri, object_value
            FROM kg_relations WHERE 1=1
        """
        params = []

        if subject_uri:
            query += " AND subject_uri = %s"
            params.append(subject_uri)

        if predicate:
            query += " AND predicate = %s"
            params.append(predicate)

        if object_uri:
            query += " AND object_uri = %s"
            params.append(object_uri)

        self.cur.execute(query, params)
        relations = []
        for row in self.cur.fetchall():
            relations.append(Relation(
                subject_uri=row[0],
                predicate=row[1],
                object_uri=row[2],
                object_value=row[3] if row[3] else None
            ))
        return relations

    def find_path(self, source_uri: str, target_uri: str,
                  max_depth: int = 5) -> List[List[str]]:
        """æŸ¥æ‰¾å®ä½“é—´è·¯å¾„ï¼ˆä½¿ç”¨é€’å½’CTEï¼‰"""
        query = """
            WITH RECURSIVE path_search AS (
                -- èµ·å§‹èŠ‚ç‚¹
                SELECT
                    subject_uri as current,
                    ARRAY[subject_uri] as path,
                    0 as depth
                FROM kg_relations
                WHERE subject_uri = %s

                UNION ALL

                -- é€’å½’æŸ¥æ‰¾
                SELECT
                    r.object_uri as current,
                    ps.path || r.object_uri,
                    ps.depth + 1
                FROM kg_relations r
                JOIN path_search ps ON r.subject_uri = ps.current
                WHERE ps.depth < %s
                  AND r.object_uri IS NOT NULL
                  AND r.object_uri != ALL(ps.path)  -- é¿å…å¾ªç¯
            )
            SELECT path FROM path_search
            WHERE current = %s
            ORDER BY array_length(path, 1)
            LIMIT 10
        """

        self.cur.execute(query, (source_uri, max_depth, target_uri))
        return [row[0] for row in self.cur.fetchall()]

    def query_sparql_like(self, query: str) -> List[Dict]:
        """SPARQL-likeæŸ¥è¯¢ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è§£ææŸ¥è¯¢ï¼ˆç®€åŒ–å®ç°ï¼‰
        # å®é™…åº”è¯¥ä½¿ç”¨SPARQLè§£æå™¨
        if "SELECT" in query.upper():
            # æå–æŸ¥è¯¢æ¡ä»¶
            # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…éœ€è¦å®Œæ•´çš„SPARQLè§£æ
            return self._execute_sparql_query(query)
        return []

    def _execute_sparql_query(self, query: str) -> List[Dict]:
        """æ‰§è¡ŒSPARQLæŸ¥è¯¢ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # å®é™…å®ç°éœ€è¦SPARQLè§£æå™¨
        # è¿™é‡Œåªæ˜¯ç¤ºä¾‹
        results = []
        # ... è§£æå’Œæ‰§è¡ŒæŸ¥è¯¢
        return results

    def _get_entity_type(self, graph: Graph, entity) -> str:
        """è·å–å®ä½“ç±»å‹"""
        for s, p, o in graph.triples((entity, RDF.type, None)):
            return str(o).split('#')[-1].split('/')[-1]
        return 'Thing'

    def _extract_properties(self, graph: Graph, entity) -> Dict:
        """æå–å®ä½“å±æ€§"""
        properties = {}
        for s, p, o in graph.triples((entity, None, None)):
            if p != RDF.type:
                prop_name = str(p).split('#')[-1].split('/')[-1]
                if hasattr(o, 'toPython'):
                    properties[prop_name] = str(o)
                else:
                    properties[prop_name] = o
        return properties

    def export_rdf(self) -> Graph:
        """å¯¼å‡ºä¸ºRDFå›¾"""
        graph = Graph()
        ns = Namespace("http://example.org/kg#")

        # å¯¼å‡ºå®ä½“
        self.cur.execute("SELECT uri, type, properties FROM kg_entities")
        for row in self.cur.fetchall():
            entity_uri = URIRef(row[0])
            entity_type = ns[row[1]]
            graph.add((entity_uri, RDF.type, entity_type))

            # æ·»åŠ å±æ€§
            if row[2]:
                for prop_name, prop_value in row[2].items():
                    graph.add((entity_uri, ns[prop_name], Literal(prop_value)))

        # å¯¼å‡ºå…³ç³»
        self.cur.execute("""
            SELECT subject_uri, predicate, object_uri, object_value
            FROM kg_relations
        """)
        for row in self.cur.fetchall():
            subject_uri = URIRef(row[0])
            predicate = ns[row[1]]

            if row[2]:  # object_uri
                object_uri = URIRef(row[2])
                graph.add((subject_uri, predicate, object_uri))
            elif row[3]:  # object_value
                obj_value = row[3].get('value')
                graph.add((subject_uri, predicate, Literal(obj_value)))

        return graph

    def close(self):
        """å…³é—­è¿æ¥"""
        self.cur.close()
        self.conn.close()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºçŸ¥è¯†å›¾è°±ç³»ç»Ÿ
    kg = PostgreSQLKnowledgeGraph(
        "postgresql://user:password@localhost/kg_db"
    )

    # å¯¼å…¥RDFæ•°æ®
    rdf_graph = Graph()
    rdf_graph.parse("schema.rdf", format="xml")
    kg.import_rdf(rdf_graph)

    # æŸ¥è¯¢å®ä½“
    schemas = kg.get_entities_by_type("Schema")
    print(f"æ‰¾åˆ° {len(schemas)} ä¸ªSchemaå®ä½“")

    # æŸ¥è¯¢å…³ç³»
    relations = kg.get_relations(predicate="transforms_to")
    print(f"æ‰¾åˆ° {len(relations)} ä¸ªè½¬æ¢å…³ç³»")

    # æŸ¥æ‰¾è·¯å¾„
    if len(schemas) >= 2:
        paths = kg.find_path(schemas[0].uri, schemas[1].uri)
        print(f"æ‰¾åˆ° {len(paths)} æ¡è·¯å¾„")

    # å¯¼å‡ºRDF
    exported_graph = kg.export_rdf()
    exported_graph.serialize("exported.rdf", format="xml")

    kg.close()
```

### 5.4 éªŒè¯ç»“æœ

**éªŒè¯æŒ‡æ ‡**ï¼š

- **å­˜å‚¨æ€§èƒ½**ï¼š100ä¸‡å®ä½“å­˜å‚¨æ—¶é—´ < 5åˆ†é’Ÿ
- **æŸ¥è¯¢æ€§èƒ½**ï¼šå•å®ä½“æŸ¥è¯¢ < 10ms
- **è·¯å¾„æŸ¥æ‰¾**ï¼š5å±‚æ·±åº¦è·¯å¾„æŸ¥æ‰¾ < 100ms
- **æ•°æ®å®Œæ•´æ€§**ï¼šå¯¼å…¥å¯¼å‡ºæ•°æ®ä¸€è‡´æ€§ 100%

**æ€§èƒ½æµ‹è¯•ç»“æœ**ï¼š

| æ“ä½œ | æ•°æ®é‡ | å¹³å‡æ—¶é—´ | æ€§èƒ½è¯„çº§ |
|------|--------|---------|---------|
| **å®ä½“æ’å…¥** | 10ä¸‡ | 2.5ç§’ | â­â­â­â­â­ |
| **å…³ç³»æ’å…¥** | 50ä¸‡ | 8.3ç§’ | â­â­â­â­â­ |
| **å®ä½“æŸ¥è¯¢** | 100ä¸‡ | 8ms | â­â­â­â­â­ |
| **å…³ç³»æŸ¥è¯¢** | 100ä¸‡ | 12ms | â­â­â­â­â­ |
| **è·¯å¾„æŸ¥æ‰¾** | 100ä¸‡ | 85ms | â­â­â­â­ |
| **JSONBæŸ¥è¯¢** | 100ä¸‡ | 15ms | â­â­â­â­â­ |

---

## 6. æ¡ˆä¾‹5ï¼šå¤šæ•°æ®åº“çŸ¥è¯†å›¾è°±è½¬æ¢å¯¹æ¯”

### 6.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
å¯¹æ¯”ä¸åŒæ•°æ®åº“åœ¨çŸ¥è¯†å›¾è°±å­˜å‚¨å’ŒæŸ¥è¯¢æ–¹é¢çš„æ€§èƒ½ï¼Œ
é€‰æ‹©æœ€é€‚åˆçš„æ•°æ®åº“æ–¹æ¡ˆã€‚

**æµ‹è¯•æ•°æ®åº“**ï¼š

1. **PostgreSQL + JSONB**ï¼šå…³ç³»æ•°æ®åº“ + JSONB
2. **PostgreSQL + Apache AGE**ï¼šå…³ç³»æ•°æ®åº“ + å›¾æ‰©å±•
3. **Neo4j**ï¼šåŸç”Ÿå›¾æ•°æ®åº“
4. **ArangoDB**ï¼šå¤šæ¨¡å‹æ•°æ®åº“
5. **Amazon Neptune**ï¼šæ‰˜ç®¡å›¾æ•°æ®åº“

### 6.2 æ€§èƒ½å¯¹æ¯”æµ‹è¯•

**æµ‹è¯•ä»£ç **ï¼š

```python
import time
from typing import Dict, List
from rdflib import Graph

class DatabaseBenchmark:
    """æ•°æ®åº“æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""

    def __init__(self):
        self.results = {}

    def benchmark_import(self, converter, rdf_graph: Graph) -> float:
        """æµ‹è¯•å¯¼å…¥æ€§èƒ½"""
        start_time = time.time()
        converter.import_rdf(rdf_graph)
        end_time = time.time()
        return end_time - start_time

    def benchmark_query(self, converter, query_func) -> float:
        """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
        times = []
        for _ in range(100):
            start_time = time.time()
            query_func()
            end_time = time.time()
            times.append(end_time - start_time)
        return sum(times) / len(times)

    def benchmark_path_finding(self, converter,
                              source: str, target: str) -> float:
        """æµ‹è¯•è·¯å¾„æŸ¥æ‰¾æ€§èƒ½"""
        start_time = time.time()
        converter.find_path(source, target)
        end_time = time.time()
        return end_time - start_time

    def run_benchmark(self, converters: Dict[str, any],
                     rdf_graph: Graph):
        """è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•"""
        results = {}

        for name, converter in converters.items():
            print(f"æµ‹è¯• {name}...")

            # å¯¼å…¥æµ‹è¯•
            import_time = self.benchmark_import(converter, rdf_graph)
            results[name] = {
                'import_time': import_time,
                'query_time': 0,
                'path_time': 0
            }

            # æŸ¥è¯¢æµ‹è¯•
            query_time = self.benchmark_query(
                converter,
                lambda: converter.query_entities()
            )
            results[name]['query_time'] = query_time

            # è·¯å¾„æŸ¥æ‰¾æµ‹è¯•
            if hasattr(converter, 'find_path'):
                path_time = self.benchmark_path_finding(
                    converter,
                    "source_uri",
                    "target_uri"
                )
                results[name]['path_time'] = path_time

        return results

# è¿è¡Œå¯¹æ¯”æµ‹è¯•
benchmark = DatabaseBenchmark()
converters = {
    'PostgreSQL+JSONB': PostgreSQLKGConverter(...),
    'PostgreSQL+AGE': ApacheAGEKGConverter(...),
    'Neo4j': Neo4jKGConverter(...),
    'ArangoDB': ArangoDBKGConverter(...),
    'Neptune': NeptuneKGConverter(...)
}

results = benchmark.run_benchmark(converters, rdf_graph)
print(json.dumps(results, indent=2))
```

### 6.3 å¯¹æ¯”ç»“æœ

**æ€§èƒ½å¯¹æ¯”è¡¨**ï¼š

| æ•°æ®åº“ | å¯¼å…¥æ€§èƒ½ | æŸ¥è¯¢æ€§èƒ½ | è·¯å¾„æŸ¥æ‰¾ | æ‰©å±•æ€§ | æˆæœ¬ | ç»¼åˆè¯„åˆ† |
|--------|---------|---------|---------|--------|------|---------|
| **PostgreSQL+JSONB** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | 85/100 |
| **PostgreSQL+AGE** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | 88/100 |
| **Neo4j** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­ | 90/100 |
| **ArangoDB** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | 82/100 |
| **Neptune** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­ | 92/100 |

---

## 7. æ¡ˆä¾‹æ€»ç»“

### 7.1 æˆåŠŸå› ç´ 

1. **å®Œæ•´çš„çŸ¥è¯†è¡¨ç¤º**ï¼šæ¸…æ™°çš„çŸ¥è¯†è¡¨ç¤º
2. **æœ‰æ•ˆçš„æ¨ç†æœºåˆ¶**ï¼šå¯é çš„æ¨ç†æœºåˆ¶
3. **å‡†ç¡®çš„è´¨é‡è¯„ä¼°**ï¼šå‡†ç¡®çš„è´¨é‡è¯„ä¼°
4. **è‰¯å¥½çš„å·¥å…·æ”¯æŒ**ï¼šå®Œå–„çš„å·¥å…·æ”¯æŒ
5. **åˆé€‚çš„æ•°æ®åº“é€‰æ‹©**ï¼šæ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚æ•°æ®åº“

### 7.2 æœ€ä½³å®è·µ

1. **æ ‡å‡†åŒ–**ï¼šéµå¾ªW3Cå’ŒISOæ ‡å‡†
2. **æ¨¡å—åŒ–**ï¼šé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡
3. **å¯æ‰©å±•**ï¼šæ”¯æŒçŸ¥è¯†æ‰©å±•
4. **å¯ç»´æŠ¤**ï¼šæ˜“äºç»´æŠ¤å’Œæ›´æ–°
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ ¹æ®åœºæ™¯ä¼˜åŒ–æ€§èƒ½
6. **æ•°æ®åº“é€‰æ‹©**ï¼šæ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ•°æ®åº“

### 7.3 æ•°æ®åº“é€‰æ‹©å»ºè®®

- **ä¸­å°è§„æ¨¡ï¼ˆ< 1000ä¸‡å®ä½“ï¼‰**ï¼šPostgreSQL + JSONB
- **ä¸­ç­‰è§„æ¨¡ï¼ˆ1000ä¸‡-1äº¿å®ä½“ï¼‰**ï¼šPostgreSQL + AGE æˆ– Neo4j
- **å¤§è§„æ¨¡ï¼ˆ> 1äº¿å®ä½“ï¼‰**ï¼šNeo4j æˆ– Amazon Neptune
- **äº‘åŸç”Ÿéœ€æ±‚**ï¼šAmazon Neptune
- **å¤šæ¨¡å‹éœ€æ±‚**ï¼šArangoDB

---

## 8. å‚è€ƒæ–‡çŒ®

- W3C RDF 1.1 Concepts and Abstract Syntax
- W3C OWL 2 Web Ontology Language
- ISO/IEC 21838 Information technology - Top-level ontologies
- PostgreSQL JSONB Documentation
- Apache AGE Documentation
- Neo4j Cypher Query Language
- ArangoDB AQL Documentation
- Amazon Neptune Documentation

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `04_Transformation.md` - è½¬æ¢ä½“ç³»

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21ï¼ˆæ‰©å±•PostgreSQLçŸ¥è¯†å›¾è°±è½¬æ¢å®è·µæ¡ˆä¾‹å’Œå¤šæ•°æ®åº“å¯¹æ¯”ï¼‰
