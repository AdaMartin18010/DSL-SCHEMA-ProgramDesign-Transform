# å¯è§‚æµ‹æ€§Schemaå®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [å¯è§‚æµ‹æ€§Schemaå®è·µæ¡ˆä¾‹](#å¯è§‚æµ‹æ€§schemaå®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¡ˆä¾‹æ¦‚è¿°](#1-æ¡ˆä¾‹æ¦‚è¿°)
  - [2. æ¡ˆä¾‹1ï¼šä¼ä¸šå¾®æœåŠ¡OTLPå¯è§‚æµ‹æ€§ç³»ç»Ÿ](#2-æ¡ˆä¾‹1ä¼ä¸šå¾®æœåŠ¡otlpå¯è§‚æµ‹æ€§ç³»ç»Ÿ)
    - [2.1 ä¸šåŠ¡èƒŒæ™¯](#21-ä¸šåŠ¡èƒŒæ™¯)
    - [2.2 æŠ€æœ¯æŒ‘æˆ˜](#22-æŠ€æœ¯æŒ‘æˆ˜)
    - [2.3 è§£å†³æ–¹æ¡ˆ](#23-è§£å†³æ–¹æ¡ˆ)
    - [2.4 å®Œæ•´ä»£ç å®ç°](#24-å®Œæ•´ä»£ç å®ç°)
    - [2.5 æ•ˆæœè¯„ä¼°](#25-æ•ˆæœè¯„ä¼°)
  - [3. æ¡ˆä¾‹2ï¼šIoTè®¾å¤‡Prometheusç›‘æ§ç³»ç»Ÿ](#3-æ¡ˆä¾‹2iotè®¾å¤‡prometheusç›‘æ§ç³»ç»Ÿ)
    - [3.1 ä¸šåŠ¡èƒŒæ™¯](#31-ä¸šåŠ¡èƒŒæ™¯)
    - [3.2 æŠ€æœ¯æŒ‘æˆ˜](#32-æŠ€æœ¯æŒ‘æˆ˜)
    - [3.3 è§£å†³æ–¹æ¡ˆ](#33-è§£å†³æ–¹æ¡ˆ)
    - [3.4 å®Œæ•´ä»£ç å®ç°](#34-å®Œæ•´ä»£ç å®ç°)
    - [3.5 æ•ˆæœè¯„ä¼°](#35-æ•ˆæœè¯„ä¼°)
  - [4. æ¡ˆä¾‹3ï¼šå¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ](#4-æ¡ˆä¾‹3å¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ)
    - [4.1 ä¸šåŠ¡èƒŒæ™¯](#41-ä¸šåŠ¡èƒŒæ™¯)
    - [4.2 æŠ€æœ¯æŒ‘æˆ˜](#42-æŠ€æœ¯æŒ‘æˆ˜)
    - [4.3 è§£å†³æ–¹æ¡ˆ](#43-è§£å†³æ–¹æ¡ˆ)
    - [4.4 å®Œæ•´ä»£ç å®ç°](#44-å®Œæ•´ä»£ç å®ç°)
    - [4.5 æ•ˆæœè¯„ä¼°](#45-æ•ˆæœè¯„ä¼°)
  - [5. æ¡ˆä¾‹4ï¼šæ™ºèƒ½å‘Šè­¦ä¸æ ¹å› åˆ†æç³»ç»Ÿ](#5-æ¡ˆä¾‹4æ™ºèƒ½å‘Šè­¦ä¸æ ¹å› åˆ†æç³»ç»Ÿ)
    - [5.1 ä¸šåŠ¡èƒŒæ™¯](#51-ä¸šåŠ¡èƒŒæ™¯)
    - [5.2 æŠ€æœ¯æŒ‘æˆ˜](#52-æŠ€æœ¯æŒ‘æˆ˜)
    - [5.3 è§£å†³æ–¹æ¡ˆ](#53-è§£å†³æ–¹æ¡ˆ)
    - [5.4 å®Œæ•´ä»£ç å®ç°](#54-å®Œæ•´ä»£ç å®ç°)
    - [5.5 æ•ˆæœè¯„ä¼°](#55-æ•ˆæœè¯„ä¼°)
  - [6. æ¡ˆä¾‹5ï¼šå¯è§‚æµ‹æ€§æ•°æ®å¯è§†åŒ–å¹³å°](#6-æ¡ˆä¾‹5å¯è§‚æµ‹æ€§æ•°æ®å¯è§†åŒ–å¹³å°)
    - [6.1 ä¸šåŠ¡èƒŒæ™¯](#61-ä¸šåŠ¡èƒŒæ™¯)
    - [6.2 æŠ€æœ¯æŒ‘æˆ˜](#62-æŠ€æœ¯æŒ‘æˆ˜)
    - [6.3 è§£å†³æ–¹æ¡ˆ](#63-è§£å†³æ–¹æ¡ˆ)
    - [6.4 å®Œæ•´ä»£ç å®ç°](#64-å®Œæ•´ä»£ç å®ç°)
    - [6.5 æ•ˆæœè¯„ä¼°](#65-æ•ˆæœè¯„ä¼°)

---

## 1. æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›å¯è§‚æµ‹æ€§Schemaåœ¨å®é™…ä¼ä¸šåº”ç”¨ä¸­çš„å®è·µæ¡ˆä¾‹ï¼Œæ¶µç›–å¾®æœåŠ¡OTLPå¯è§‚æµ‹æ€§ã€IoTè®¾å¤‡Prometheusç›‘æ§ã€å¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨ä¸åˆ†æã€æ™ºèƒ½å‘Šè­¦ä¸æ ¹å› åˆ†æã€æ•°æ®å¯è§†åŒ–ç­‰çœŸå®åœºæ™¯ã€‚

**æ¡ˆä¾‹ç±»å‹**ï¼š

1. **å¾®æœåŠ¡OTLPå¯è§‚æµ‹æ€§ç³»ç»Ÿ**ï¼šä½¿ç”¨OpenTelemetryæ”¶é›†æŒ‡æ ‡ã€æ—¥å¿—å’Œè¿½è¸ªæ•°æ®
2. **IoTè®¾å¤‡Prometheusç›‘æ§ç³»ç»Ÿ**ï¼šä½¿ç”¨Prometheusç›‘æ§å¤§è§„æ¨¡IoTè®¾å¤‡
3. **å¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ**ï¼šæ—¶åºæ•°æ®å­˜å‚¨ã€å®æ—¶åˆ†æä¸èšåˆ
4. **æ™ºèƒ½å‘Šè­¦ä¸æ ¹å› åˆ†æç³»ç»Ÿ**ï¼šåŸºäºAIçš„å¼‚å¸¸æ£€æµ‹ä¸æ•…éšœå®šä½
5. **å¯è§‚æµ‹æ€§æ•°æ®å¯è§†åŒ–å¹³å°**ï¼šç»Ÿä¸€ä»ªè¡¨ç›˜ä¸å¯è§†åŒ–åˆ†æ

**å‚è€ƒä¼ä¸šæ¡ˆä¾‹**ï¼š

- **OpenTelemetry**ï¼šOpenTelemetryæ ‡å‡†
- **Prometheus**ï¼šPrometheusç›‘æ§ç³»ç»Ÿ
- **Grafana**ï¼šå¯è§†åŒ–ä¸å‘Šè­¦å¹³å°
- **Uber**ï¼šå¤§è§„æ¨¡å¾®æœåŠ¡å¯è§‚æµ‹æ€§å®è·µ
- **Netflix**ï¼šåˆ†å¸ƒå¼è¿½è¸ªä¸æ•…éšœåˆ†æ

---

## 2. æ¡ˆä¾‹1ï¼šä¼ä¸šå¾®æœåŠ¡OTLPå¯è§‚æµ‹æ€§ç³»ç»Ÿ

### 2.1 ä¸šåŠ¡èƒŒæ™¯

**ä¼ä¸šèƒŒæ™¯**ï¼š
æŸå¤§å‹ç”µå•†å¹³å°ï¼ˆä»¥ä¸‹ç®€ç§°"Aå…¬å¸"ï¼‰ï¼Œæ‹¥æœ‰è¶…è¿‡500ä¸ªå¾®æœåŠ¡ï¼Œæ—¥æ´»è·ƒç”¨æˆ·æ•°è¾¾2000ä¸‡ï¼Œæ—¥å‡è®¢å•é‡è¶…è¿‡500ä¸‡ç¬”ã€‚å¹³å°é‡‡ç”¨äº‘åŸç”Ÿæ¶æ„ï¼Œéƒ¨ç½²åœ¨Kubernetesé›†ç¾¤ä¸Šï¼Œä¸šåŠ¡æ¶µç›–ç”µå•†æ ¸å¿ƒäº¤æ˜“ã€æ”¯ä»˜ã€ç‰©æµã€å®¢æœç­‰å¤šä¸ªé¢†åŸŸã€‚

**ä¸šåŠ¡ç—›ç‚¹**ï¼š

1. **ç›‘æ§ç›²åŒºä¸¥é‡**ï¼šä»…æœ‰60%çš„æœåŠ¡æ¥å…¥ç›‘æ§ï¼Œæ ¸å¿ƒäº¤æ˜“é“¾è·¯å­˜åœ¨ç›‘æ§ç›²åŒºï¼Œæ•…éšœå‘ç”Ÿæ—¶æ— æ³•å¿«é€Ÿå®šä½é—®é¢˜æœåŠ¡
2. **æ•…éšœå®šä½æ•ˆç‡ä½**ï¼šå¹³å‡æ•…éšœå®šä½æ—¶é—´ï¼ˆMTTRï¼‰é«˜è¾¾30åˆ†é’Ÿï¼Œä¸¥é‡å½±å“ç”¨æˆ·ä½“éªŒå’Œä¸šåŠ¡è¿ç»­æ€§
3. **æ•°æ®å­¤å²›é—®é¢˜**ï¼šæŒ‡æ ‡ã€æ—¥å¿—ã€è¿½è¸ªæ•°æ®åˆ†æ•£åœ¨ä¸åŒç³»ç»Ÿï¼Œç¼ºä¹ç»Ÿä¸€å…³è”åˆ†æèƒ½åŠ›
4. **è·¨æœåŠ¡è¿½è¸ªå›°éš¾**ï¼šåˆ†å¸ƒå¼äº‹åŠ¡è·¨è¶Š20+æœåŠ¡ï¼Œæ— æ³•å®Œæ•´è¿½è¸ªè¯·æ±‚é“¾è·¯
5. **ç¼ºä¹ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§**ï¼šä»…å…³æ³¨ç³»ç»ŸæŒ‡æ ‡ï¼Œç¼ºä¹è®¢å•æˆåŠŸç‡ã€æ”¯ä»˜è½¬åŒ–ç‡ç­‰æ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡ç›‘æ§

**ä¸šåŠ¡ç›®æ ‡**ï¼š

- å®ç°95%ä»¥ä¸ŠæœåŠ¡çš„å…¨é¢ç›‘æ§è¦†ç›–
- å°†MTTRä»30åˆ†é’Ÿé™ä½è‡³5åˆ†é’Ÿä»¥å†…
- å»ºç«‹ç»Ÿä¸€çš„æŒ‡æ ‡-æ—¥å¿—-è¿½è¸ªå…³è”åˆ†æä½“ç³»
- å®ç°å…¨é“¾è·¯åˆ†å¸ƒå¼è¿½è¸ª
- æ„å»ºæ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡ç›‘æ§ä½“ç³»

### 2.2 æŠ€æœ¯æŒ‘æˆ˜

1. **æµ·é‡æ•°æ®é‡‡é›†æŒ‘æˆ˜**ï¼šæ—¥å‡äº§ç”Ÿ10TBç›‘æ§æ•°æ®ï¼Œå¦‚ä½•é«˜æ•ˆé‡‡é›†ä¸”ä¸å½±å“ä¸šåŠ¡æ€§èƒ½
2. **å¤šè¯­è¨€SDKç»Ÿä¸€**ï¼šJavaã€Goã€Pythonã€Node.jså¤šè¯­è¨€æœåŠ¡éœ€è¦ç»Ÿä¸€çš„åŸ‹ç‚¹æ–¹æ¡ˆ
3. **é‡‡æ ·ç­–ç•¥è®¾è®¡**ï¼šå…¨é‡é‡‡é›†æˆæœ¬è¿‡é«˜ï¼Œéœ€è¦æ™ºèƒ½é‡‡æ ·ç­–ç•¥ä¿è¯å…³é”®æ•°æ®ä¸ä¸¢å¤±
4. **æ•°æ®å…³è”å¤æ‚æ€§**ï¼šå¦‚ä½•å°†æŒ‡æ ‡ã€æ—¥å¿—ã€è¿½è¸ªæ•°æ®é€šè¿‡ç»Ÿä¸€ä¸Šä¸‹æ–‡å…³è”
5. **å®æ—¶æ€§è¦æ±‚**ï¼šæ ¸å¿ƒæŒ‡æ ‡éœ€è¦ç§’çº§å»¶è¿Ÿï¼Œå¯¹æ•°æ®å¤„ç†é“¾è·¯æå‡ºé«˜è¦æ±‚

### 2.3 è§£å†³æ–¹æ¡ˆ

**æ¶æ„è®¾è®¡**ï¼š

- é‡‡ç”¨OpenTelemetryæ ‡å‡†é‡‡é›†Metricsã€Logsã€Tracesä¸‰ç±»æ•°æ®
- éƒ¨ç½²OpenTelemetry Collectorè¿›è¡Œæ•°æ®æ”¶é›†ã€å¤„ç†å’Œå¯¼å‡º
- ä½¿ç”¨Jaegerå­˜å‚¨è¿½è¸ªæ•°æ®ï¼ŒVictoriaMetricså­˜å‚¨æŒ‡æ ‡æ•°æ®
- æ„å»ºç»Ÿä¸€æ ‡ç­¾ä½“ç³»ï¼ˆservice.nameã€deployment.environmentã€host.nameç­‰ï¼‰
- åŸºäºTraceIDã€SpanIDå®ç°æ•°æ®å…³è”

### 2.4 å®Œæ•´ä»£ç å®ç°

**å¾®æœåŠ¡OTLPå¯è§‚æµ‹æ€§ç³»ç»Ÿå®Œæ•´å®ç°ï¼ˆçº¦450è¡Œï¼‰**ï¼š

```python
#!/usr/bin/env python3
"""
ä¼ä¸šçº§å¾®æœåŠ¡OTLPå¯è§‚æµ‹æ€§ç³»ç»Ÿ
åŠŸèƒ½ï¼šæŒ‡æ ‡æ”¶é›†ã€æ—¥å¿—è®°å½•ã€é“¾è·¯è¿½è¸ªã€æ•°æ®å…³è”åˆ†æ
"""

import json
import time
import uuid
import random
import logging
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from enum import Enum
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
import threading

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MetricType(str, Enum):
    """æŒ‡æ ‡ç±»å‹æšä¸¾"""
    COUNTER = "Counter"      # ç´¯ç§¯è®¡æ•°å™¨
    GAUGE = "Gauge"          # ç¬æ—¶å€¼
    HISTOGRAM = "Histogram"  # ç›´æ–¹å›¾
    SUMMARY = "Summary"      # æ‘˜è¦ç»Ÿè®¡


class SpanKind(str, Enum):
    """Spanç±»å‹æšä¸¾"""
    INTERNAL = "INTERNAL"
    SERVER = "SERVER"
    CLIENT = "CLIENT"
    PRODUCER = "PRODUCER"
    CONSUMER = "CONSUMER"


class StatusCode(str, Enum):
    """çŠ¶æ€ç æšä¸¾"""
    UNSET = "UNSET"
    OK = "OK"
    ERROR = "ERROR"


@dataclass
class Resource:
    """èµ„æºä¿¡æ¯ - æ ‡è¯†ç›‘æ§å®ä½“"""
    service_name: str
    service_version: str
    deployment_environment: str
    host_name: str = ""
    attributes: Dict[str, str] = field(default_factory=dict)

    def to_dict(self) -> Dict:
        return {
            "service.name": self.service_name,
            "service.version": self.service_version,
            "deployment.environment": self.deployment_environment,
            "host.name": self.host_name,
            **self.attributes
        }


@dataclass
class Metric:
    """æŒ‡æ ‡æ•°æ®æ¨¡å‹"""
    name: str
    type: MetricType
    unit: str
    value: float
    labels: Dict[str, str] = field(default_factory=dict)
    timestamp: Optional[datetime] = None
    resource: Optional[Resource] = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

    def to_otlp_format(self) -> Dict:
        """è½¬æ¢ä¸ºOTLPæ ¼å¼"""
        return {
            "name": self.name,
            "type": self.type.value,
            "unit": self.unit,
            "value": self.value,
            "labels": self.labels,
            "timestamp": self.timestamp.isoformat() if self.timestamp else None,
            "resource": self.resource.to_dict() if self.resource else {}
        }


@dataclass
class Span:
    """è¿½è¸ªSpanæ•°æ®æ¨¡å‹"""
    span_id: str
    trace_id: str
    parent_span_id: Optional[str]
    name: str
    kind: SpanKind
    start_time: datetime
    end_time: Optional[datetime] = None
    attributes: Dict[str, Any] = field(default_factory=dict)
    status: StatusCode = StatusCode.UNSET
    events: List[Dict] = field(default_factory=list)
    resource: Optional[Resource] = None

    def end(self, status: StatusCode = StatusCode.OK):
        """ç»“æŸSpan"""
        self.end_time = datetime.now()
        self.status = status

    def add_event(self, name: str, attributes: Dict = None):
        """æ·»åŠ äº‹ä»¶"""
        self.events.append({
            "name": name,
            "timestamp": datetime.now().isoformat(),
            "attributes": attributes or {}
        })

    def duration_ms(self) -> float:
        """è·å–SpanæŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰"""
        if self.end_time:
            return (self.end_time - self.start_time).total_seconds() * 1000
        return 0.0


@dataclass
class LogRecord:
    """æ—¥å¿—è®°å½•æ•°æ®æ¨¡å‹"""
    log_id: str
    trace_id: Optional[str]
    span_id: Optional[str]
    severity: str  # DEBUG, INFO, WARN, ERROR, FATAL
    body: str
    timestamp: datetime
    attributes: Dict[str, Any] = field(default_factory=dict)
    resource: Optional[Resource] = None

    def to_otlp_format(self) -> Dict:
        """è½¬æ¢ä¸ºOTLPæ ¼å¼"""
        return {
            "logId": self.log_id,
            "traceId": self.trace_id,
            "spanId": self.span_id,
            "severity": self.severity,
            "body": self.body,
            "timestamp": self.timestamp.isoformat(),
            "attributes": self.attributes,
            "resource": self.resource.to_dict() if self.resource else {}
        }


class Tracer:
    """è¿½è¸ªå™¨ - ç®¡ç†Spanåˆ›å»ºå’Œä¸Šä¸‹æ–‡ä¼ æ’­"""

    def __init__(self, resource: Resource):
        self.resource = resource
        self._current_span: Optional[Span] = None
        self._span_stack: List[Span] = []

    def start_span(self, name: str, kind: SpanKind = SpanKind.INTERNAL,
                   parent_span_id: Optional[str] = None) -> Span:
        """å¼€å§‹ä¸€ä¸ªæ–°çš„Span"""
        span = Span(
            span_id=str(uuid.uuid4().hex)[:16],
            trace_id=self._get_or_create_trace_id(),
            parent_span_id=parent_span_id or (self._current_span.span_id if self._current_span else None),
            name=name,
            kind=kind,
            start_time=datetime.now(),
            resource=self.resource
        )
        self._span_stack.append(span)
        self._current_span = span
        return span

    def end_span(self, span: Span, status: StatusCode = StatusCode.OK):
        """ç»“æŸSpan"""
        span.end(status)
        if span in self._span_stack:
            self._span_stack.remove(span)
        self._current_span = self._span_stack[-1] if self._span_stack else None

    def _get_or_create_trace_id(self) -> str:
        """è·å–æˆ–åˆ›å»ºTrace ID"""
        if self._current_span:
            return self._current_span.trace_id
        return str(uuid.uuid4().hex)[:32]

    def get_current_context(self) -> Dict:
        """è·å–å½“å‰è¿½è¸ªä¸Šä¸‹æ–‡"""
        if self._current_span:
            return {
                "trace_id": self._current_span.trace_id,
                "span_id": self._current_span.span_id
            }
        return {}


class Meter:
    """è®¡é‡å™¨ - ç®¡ç†æŒ‡æ ‡æ”¶é›†"""

    def __init__(self, resource: Resource):
        self.resource = resource
        self._counters: Dict[str, float] = defaultdict(float)
        self._gauges: Dict[str, float] = {}
        self._histograms: Dict[str, List[float]] = defaultdict(list)

    def create_counter(self, name: str, unit: str = "1") -> 'Counter':
        """åˆ›å»ºè®¡æ•°å™¨"""
        return Counter(name, unit, self.resource, self._counters)

    def create_gauge(self, name: str, unit: str = "1") -> 'Gauge':
        """åˆ›å»ºä»ªè¡¨ç›˜"""
        return Gauge(name, unit, self.resource, self._gauges)

    def create_histogram(self, name: str, unit: str = "ms") -> 'Histogram':
        """åˆ›å»ºç›´æ–¹å›¾"""
        return Histogram(name, unit, self.resource, self._histograms)


class Counter:
    """è®¡æ•°å™¨å®ç°"""

    def __init__(self, name: str, unit: str, resource: Resource, storage: Dict):
        self.name = name
        self.unit = unit
        self.resource = resource
        self._storage = storage

    def add(self, value: float, labels: Dict[str, str] = None):
        """å¢åŠ å€¼"""
        label_key = json.dumps(labels or {}, sort_keys=True)
        key = f"{self.name}:{label_key}"
        self._storage[key] += value

    def get_value(self, labels: Dict[str, str] = None) -> float:
        """è·å–å½“å‰å€¼"""
        label_key = json.dumps(labels or {}, sort_keys=True)
        key = f"{self.name}:{label_key}"
        return self._storage.get(key, 0.0)


class Gauge:
    """ä»ªè¡¨ç›˜å®ç°"""

    def __init__(self, name: str, unit: str, resource: Resource, storage: Dict):
        self.name = name
        self.unit = unit
        self.resource = resource
        self._storage = storage

    def set(self, value: float, labels: Dict[str, str] = None):
        """è®¾ç½®å€¼"""
        label_key = json.dumps(labels or {}, sort_keys=True)
        key = f"{self.name}:{label_key}"
        self._storage[key] = value

    def get_value(self, labels: Dict[str, str] = None) -> float:
        """è·å–å½“å‰å€¼"""
        label_key = json.dumps(labels or {}, sort_keys=True)
        key = f"{self.name}:{label_key}"
        return self._storage.get(key, 0.0)


class Histogram:
    """ç›´æ–¹å›¾å®ç°"""

    def __init__(self, name: str, unit: str, resource: Resource, storage: Dict):
        self.name = name
        self.unit = unit
        self.resource = resource
        self._storage = storage

    def record(self, value: float, labels: Dict[str, str] = None):
        """è®°å½•å€¼"""
        label_key = json.dumps(labels or {}, sort_keys=True)
        key = f"{self.name}:{label_key}"
        self._storage[key].append(value)

    def get_statistics(self, labels: Dict[str, str] = None) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        label_key = json.dumps(labels or {}, sort_keys=True)
        key = f"{self.name}:{label_key}"
        values = self._storage.get(key, [])
        if not values:
            return {"count": 0, "sum": 0, "min": 0, "max": 0, "avg": 0}
        return {
            "count": len(values),
            "sum": sum(values),
            "min": min(values),
            "max": max(values),
            "avg": sum(values) / len(values)
        }


class ObservabilityCollector:
    """å¯è§‚æµ‹æ€§æ•°æ®æ”¶é›†å™¨ - ç»Ÿä¸€æ”¶é›†æŒ‡æ ‡ã€æ—¥å¿—ã€è¿½è¸ª"""

    def __init__(self, service_name: str, service_version: str, environment: str = "production"):
        self.resource = Resource(
            service_name=service_name,
            service_version=service_version,
            deployment_environment=environment,
            host_name=uuid.uuid4().hex[:8]
        )
        self.tracer = Tracer(self.resource)
        self.meter = Meter(self.resource)

        self._metrics: List[Metric] = []
        self._spans: List[Span] = []
        self._logs: List[LogRecord] = []
        self._lock = threading.Lock()

        # å¯åŠ¨åå°å¯¼å‡ºçº¿ç¨‹
        self._executor = ThreadPoolExecutor(max_workers=2)
        self._running = True
        self._export_interval = 10  # ç§’
        threading.Thread(target=self._periodic_export, daemon=True).start()

    def record_log(self, severity: str, body: str, attributes: Dict = None):
        """è®°å½•æ—¥å¿—"""
        context = self.tracer.get_current_context()
        log = LogRecord(
            log_id=str(uuid.uuid4()),
            trace_id=context.get("trace_id"),
            span_id=context.get("span_id"),
            severity=severity,
            body=body,
            timestamp=datetime.now(),
            attributes=attributes or {},
            resource=self.resource
        )
        with self._lock:
            self._logs.append(log)
        logger.info(f"[{severity}] {body}")

    def record_metric(self, name: str, metric_type: MetricType, value: float,
                     unit: str = "1", labels: Dict = None):
        """è®°å½•æŒ‡æ ‡"""
        metric = Metric(
            name=name,
            type=metric_type,
            unit=unit,
            value=value,
            labels={"service_name": self.resource.service_name, **(labels or {})},
            timestamp=datetime.now(),
            resource=self.resource
        )
        with self._lock:
            self._metrics.append(metric)

    def start_span(self, name: str, kind: SpanKind = SpanKind.INTERNAL) -> Span:
        """å¼€å§‹Span"""
        return self.tracer.start_span(name, kind)

    def end_span(self, span: Span, status: StatusCode = StatusCode.OK):
        """ç»“æŸSpanå¹¶ä¿å­˜"""
        self.tracer.end_span(span, status)
        with self._lock:
            self._spans.append(span)

    def trace_function(self, name: str, kind: SpanKind = SpanKind.INTERNAL):
        """å‡½æ•°è¿½è¸ªè£…é¥°å™¨"""
        def decorator(func: Callable):
            def wrapper(*args, **kwargs):
                span = self.start_span(name, kind)
                try:
                    result = func(*args, **kwargs)
                    span.add_event("function.completed", {"result": str(result)[:100]})
                    self.end_span(span, StatusCode.OK)
                    return result
                except Exception as e:
                    span.add_event("function.error", {"error": str(e)})
                    self.end_span(span, StatusCode.ERROR)
                    self.record_log("ERROR", f"Function {name} failed: {str(e)}")
                    raise
            return wrapper
        return decorator

    def _periodic_export(self):
        """å®šæœŸå¯¼å‡ºæ•°æ®"""
        while self._running:
            time.sleep(self._export_interval)
            self._export_data()

    def _export_data(self):
        """å¯¼å‡ºæ•°æ®åˆ°å­˜å‚¨"""
        with self._lock:
            metrics = self._metrics.copy()
            spans = self._spans.copy()
            logs = self._logs.copy()
            self._metrics.clear()
            self._spans.clear()
            self._logs.clear()

        # æ¨¡æ‹Ÿå¯¼å‡ºåˆ°åç«¯å­˜å‚¨
        logger.info(f"Exported {len(metrics)} metrics, {len(spans)} spans, {len(logs)} logs")

    def get_observability_summary(self) -> Dict:
        """è·å–å¯è§‚æµ‹æ€§æ‘˜è¦"""
        with self._lock:
            return {
                "resource": self.resource.to_dict(),
                "pending_metrics": len(self._metrics),
                "pending_spans": len(self._spans),
                "pending_logs": len(self._logs),
                "metric_types": defaultdict(int)
            }

    def shutdown(self):
        """å…³é—­æ”¶é›†å™¨"""
        self._running = False
        self._export_data()
        self._executor.shutdown(wait=True)


# ============ ä¸šåŠ¡åœºæ™¯æ¼”ç¤º ============

def simulate_ecommerce_service():
    """æ¨¡æ‹Ÿç”µå•†æœåŠ¡åœºæ™¯"""
    collector = ObservabilityCollector(
        service_name="order-service",
        service_version="v2.3.1",
        environment="production"
    )

    # åˆ›å»ºæŒ‡æ ‡
    request_counter = collector.meter.create_counter("http_requests_total")
    latency_histogram = collector.meter.create_histogram("http_request_duration_ms")
    active_orders = collector.meter.create_gauge("active_orders")

    @collector.trace_function("create_order", SpanKind.SERVER)
    def create_order(user_id: str, amount: float):
        """åˆ›å»ºè®¢å•ä¸šåŠ¡å‡½æ•°"""
        collector.record_log("INFO", f"Creating order for user {user_id}", {"amount": amount})

        # æ¨¡æ‹Ÿä¸šåŠ¡å¤„ç†
        processing_time = random.uniform(50, 200)
        time.sleep(processing_time / 1000)

        # è®°å½•æŒ‡æ ‡
        request_counter.add(1, {"method": "POST", "endpoint": "/orders"})
        latency_histogram.record(processing_time, {"endpoint": "/orders"})
        active_orders.set(random.randint(100, 500))

        order_id = str(uuid.uuid4())[:8]
        collector.record_log("INFO", f"Order {order_id} created successfully")

        return {"order_id": order_id, "status": "created"}

    @collector.trace_function("process_payment", SpanKind.CLIENT)
    def process_payment(order_id: str, amount: float):
        """å¤„ç†æ”¯ä»˜"""
        span = collector.start_span("call_payment_gateway", SpanKind.CLIENT)
        try:
            # æ¨¡æ‹Ÿè°ƒç”¨æ”¯ä»˜æœåŠ¡
            time.sleep(random.uniform(20, 80) / 1000)
            span.add_event("payment_gateway.called", {"order_id": order_id})
            collector.end_span(span, StatusCode.OK)
            return {"status": "paid", "transaction_id": str(uuid.uuid4())[:12]}
        except Exception as e:
            collector.end_span(span, StatusCode.ERROR)
            raise

    # æ¨¡æ‹Ÿ10ä¸ªè¯·æ±‚
    collector.record_log("INFO", "Starting order service simulation")
    for i in range(10):
        try:
            result = create_order(f"user_{i}", random.uniform(50, 500))
            process_payment(result["order_id"], 100.0)
        except Exception as e:
            collector.record_log("ERROR", f"Request failed: {e}")

    # è¾“å‡ºç»Ÿè®¡
    summary = collector.get_observability_summary()
    print("\n" + "="*60)
    print("å¯è§‚æµ‹æ€§æ•°æ®æ‘˜è¦:")
    print(f"  æœåŠ¡: {summary['resource']['service.name']}")
    print(f"  å¾…å¯¼å‡ºæŒ‡æ ‡: {summary['pending_metrics']}")
    print(f"  å¾…å¯¼å‡ºSpan: {summary['pending_spans']}")
    print(f"  å¾…å¯¼å‡ºæ—¥å¿—: {summary['pending_logs']}")

    # è¾“å‡ºæŒ‡æ ‡ç»Ÿè®¡
    print("\næŒ‡æ ‡ç»Ÿè®¡:")
    print(f"  è¯·æ±‚æ€»æ•°: {request_counter.get_value({'method': 'POST', 'endpoint': '/orders'})}")
    print(f"  å»¶è¿Ÿç»Ÿè®¡: {latency_histogram.get_statistics({'endpoint': '/orders'})}")
    print(f"  å½“å‰æ´»è·ƒè®¢å•: {active_orders.get_value()}")

    collector.shutdown()


if __name__ == '__main__':
    simulate_ecommerce_service()
```

### 2.5 æ•ˆæœè¯„ä¼°

**æ€§èƒ½æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | æ”¹è¿›å‰ | æ”¹è¿›å | æå‡å¹…åº¦ |
|------|--------|--------|----------|
| ç›‘æ§è¦†ç›–ç‡ | 60% | 98% | +38% |
| å¹³å‡æ•…éšœå®šä½æ—¶é—´(MTTR) | 30åˆ†é’Ÿ | 4.2åˆ†é’Ÿ | -86% |
| å‘Šè­¦å‡†ç¡®ç‡ | 45% | 89% | +44% |
| æ•°æ®å…³è”å®Œæ•´åº¦ | 30% | 95% | +65% |
| æ ¸å¿ƒé“¾è·¯è¿½è¸ªæˆåŠŸç‡ | 0% | 99.5% | +99.5% |
| ç›‘æ§æ•°æ®å»¶è¿Ÿ | 5åˆ†é’Ÿ | 3ç§’ | -99% |

**ä¸šåŠ¡ä»·å€¼**ï¼š

1. **ç›´æ¥ç»æµæ”¶ç›Š**ï¼š
   - å¹´åº¦æ•…éšœæŸå¤±å‡å°‘çº¦1200ä¸‡å…ƒï¼ˆæ•…éšœå¤„ç†æ—¶é—´ç¼©çŸ­80%ï¼Œä¸šåŠ¡ä¸­æ–­æ—¶é—´å‡å°‘ï¼‰
   - è¿ç»´äººåŠ›æˆæœ¬èŠ‚çœçº¦300ä¸‡å…ƒ/å¹´ï¼ˆè‡ªåŠ¨åŒ–ç›‘æ§å‡å°‘äººå·¥å·¡æ£€å·¥ä½œé‡ï¼‰

2. **è¿è¥æ•ˆç‡æå‡**ï¼š
   - è¿ç»´å›¢é˜Ÿäººå‡å¯ç®¡ç†æœåŠ¡æ•°ä»15ä¸ªæå‡è‡³50ä¸ªï¼ˆ+233%ï¼‰
   - æ•…éšœå“åº”SLAè¾¾æˆç‡ä»75%æå‡è‡³99.2%
   - å‘å¸ƒå›æ»šå†³ç­–æ—¶é—´ä»å¹³å‡15åˆ†é’Ÿé™è‡³2åˆ†é’Ÿ

3. **ç”¨æˆ·ä½“éªŒæ”¹å–„**ï¼š
   - ç³»ç»Ÿå¯ç”¨æ€§ä»99.5%æå‡è‡³99.95%
   - é¡µé¢åŠ è½½è¶…æ—¶æŠ•è¯‰å‡å°‘65%
   - å¤§ä¿ƒæœŸé—´ç³»ç»Ÿç¨³å®šæ€§ä¿éšœèƒ½åŠ›æ˜¾è‘—æå‡

4. **æŠ€æœ¯å€ºåŠ¡å‡å°‘**ï¼š
   - ç»Ÿä¸€äº†å¤šè¯­è¨€æœåŠ¡çš„ç›‘æ§æ–¹æ¡ˆï¼Œæ¶ˆé™¤æŠ€æœ¯æ ˆå·®å¼‚å¸¦æ¥çš„ç›‘æ§ç›²åŒº
   - å»ºç«‹äº†å¯è§‚æµ‹æ€§æœ€ä½³å®è·µï¼Œæ–°æœåŠ¡æ¥å…¥æ—¶é—´ä»3å¤©ç¼©çŸ­è‡³2å°æ—¶

**ç»éªŒæ•™è®­**ï¼š

1. **æ¸è¿›å¼æ¨è¿›ç­–ç•¥**ï¼šä»æ ¸å¿ƒäº¤æ˜“é“¾è·¯å¼€å§‹é€æ­¥æ‰©å±•ï¼Œæ¯”ä¸€æ¬¡æ€§å…¨é‡æ¨è¿›æˆåŠŸç‡æ›´é«˜
2. **é‡‡æ ·ç­–ç•¥çš„é‡è¦æ€§**ï¼šç”Ÿäº§ç¯å¢ƒé‡‡ç”¨è‡ªé€‚åº”é‡‡æ ·ï¼ˆAdaptive Samplingï¼‰ï¼Œåœ¨ä¿è¯å…³é”®æ•°æ®å®Œæ•´æ€§çš„åŒæ—¶é™ä½å­˜å‚¨æˆæœ¬60%
3. **æ•°æ®å…³è”è§„èŒƒ**ï¼šç»Ÿä¸€ä½¿ç”¨TraceIDä½œä¸ºå…³è”é”®ï¼Œå»ºç«‹æ ‡ç­¾å‘½åè§„èŒƒï¼Œé¿å…åæœŸæ•°æ®å…³è”å›°éš¾
4. **æ€§èƒ½å½±å“ç›‘æ§**ï¼šAgentæœ¬èº«éœ€è¦è¢«ç›‘æ§ï¼ŒåˆæœŸæ›¾å› Agent CPUå ç”¨è¿‡é«˜å½±å“ä¸šåŠ¡ï¼Œåé€šè¿‡ä¼˜åŒ–è§£å†³
5. **ç»„ç»‡ååŒ**ï¼šå¯è§‚æµ‹æ€§ä¸ä»…æ˜¯æŠ€æœ¯é—®é¢˜ï¼Œéœ€è¦å¼€å‘ã€è¿ç»´ã€SREå›¢é˜Ÿå…±åŒåˆ¶å®šSLOå’Œå‘Šè­¦ç­–ç•¥

---

## 3. æ¡ˆä¾‹2ï¼šIoTè®¾å¤‡Prometheusç›‘æ§ç³»ç»Ÿ

### 3.1 ä¸šåŠ¡èƒŒæ™¯

**ä¼ä¸šèƒŒæ™¯**ï¼š
æŸæ™ºæ…§åŸå¸‚è¿è¥å•†ï¼ˆä»¥ä¸‹ç®€ç§°"Bå…¬å¸"ï¼‰ï¼Œç®¡ç†ç€è¶…è¿‡50ä¸‡å°IoTè®¾å¤‡ï¼ŒåŒ…æ‹¬æ™ºèƒ½è·¯ç¯ã€ç¯å¢ƒä¼ æ„Ÿå™¨ã€äº¤é€šç›‘æ§æ‘„åƒå¤´ã€æ™ºèƒ½ç”µè¡¨ç­‰ã€‚è¿™äº›è®¾å¤‡åˆ†å¸ƒåœ¨å…¨å›½300å¤šä¸ªåŸå¸‚ï¼Œæ¯å¤©äº§ç”Ÿçº¦20äº¿æ¡ç›‘æ§æ•°æ®ï¼Œæ•°æ®å³°å€¼æ—¶å¯è¾¾50ä¸‡æ¡/ç§’ã€‚

**ä¸šåŠ¡ç—›ç‚¹**ï¼š

1. **è®¾å¤‡ç›‘æ§ç›²åŒº**ï¼šä»…30%çš„å…³é”®è®¾å¤‡æ¥å…¥ç›‘æ§ï¼Œå¤§é‡è®¾å¤‡å¤„äº"é»‘ç›’"çŠ¶æ€
2. **æ•…éšœå‘ç°æ»å**ï¼šè®¾å¤‡æ•…éšœå¹³å‡å‘ç°æ—¶é—´é•¿è¾¾2å°æ—¶ï¼Œå¯¼è‡´è·¯ç¯ç†„ç­ã€ä¼ æ„Ÿå™¨å¤±æ•ˆç­‰é—®é¢˜å½±å“å¸‚æ°‘ç”Ÿæ´»
3. **æ•°æ®è§„æ¨¡æŒ‘æˆ˜**ï¼šè®¾å¤‡æ•°é‡åºå¤§ï¼Œä¼ ç»Ÿç›‘æ§ç³»ç»Ÿæ— æ³•æ”¯æ’‘æµ·é‡æŒ‡æ ‡é‡‡é›†å’Œå­˜å‚¨
4. **ç½‘ç»œä¸ç¨³å®š**ï¼šè®¾å¤‡é€šè¿‡4G/5Gç½‘ç»œè¿æ¥ï¼Œç½‘ç»œæŠ–åŠ¨é¢‘ç¹ï¼Œæ•°æ®é‡‡é›†æ˜“ä¸¢å¤±
5. **å¼‚æ„è®¾å¤‡ç®¡ç†**ï¼šè®¾å¤‡å‚å•†ä¼—å¤šï¼Œé€šä¿¡åè®®å„å¼‚ï¼ˆMQTTã€CoAPã€HTTPï¼‰ï¼Œéš¾ä»¥ç»Ÿä¸€ç®¡ç†

**ä¸šåŠ¡ç›®æ ‡**ï¼š

- å®ç°100%å…³é”®è®¾å¤‡ç›‘æ§è¦†ç›–
- å°†è®¾å¤‡æ•…éšœå‘ç°æ—¶é—´ç¼©çŸ­è‡³5åˆ†é’Ÿä»¥å†…
- æ”¯æŒ50ä¸‡+è®¾å¤‡åŒæ—¶åœ¨çº¿ç›‘æ§
- å»ºç«‹ç»Ÿä¸€çš„è®¾å¤‡æŒ‡æ ‡é‡‡é›†æ ‡å‡†
- å®ç°è¾¹ç¼˜è®¡ç®—+äº‘ç«¯åˆ†æçš„æ··åˆæ¶æ„

### 3.2 æŠ€æœ¯æŒ‘æˆ˜

1. **æµ·é‡è®¾å¤‡æ¥å…¥**ï¼š50ä¸‡å°è®¾å¤‡åŒæ—¶ä¸ŠæŠ¥æŒ‡æ ‡ï¼Œå¯¹é‡‡é›†ç³»ç»Ÿååé‡æå‡ºæé«˜è¦æ±‚
2. **ç½‘ç»œä¸ç¨³å®š**ï¼šå¼±ç½‘ç¯å¢ƒä¸‹å¦‚ä½•ä¿è¯æ•°æ®å¯é ä¼ è¾“
3. **æŒ‡æ ‡åŸºæ•°çˆ†ç‚¸**ï¼šæ¯ä¸ªè®¾å¤‡æ•°åä¸ªæŒ‡æ ‡ï¼Œæ ‡ç­¾ç»„åˆå¯¼è‡´æŒ‡æ ‡åŸºæ•°è¾¾åƒä¸‡çº§
4. **å®æ—¶æ€§è¦æ±‚**ï¼šè®¾å¤‡å¼‚å¸¸éœ€è¦ç§’çº§å‘ç°ï¼Œå¯¹æ•°æ®å¤„ç†å»¶è¿Ÿæ•æ„Ÿ
5. **è¾¹ç¼˜è®¡ç®—æ¶æ„**ï¼šå¦‚ä½•åœ¨è¾¹ç¼˜èŠ‚ç‚¹è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼Œå‡å°‘äº‘ç«¯å¸¦å®½å‹åŠ›

### 3.3 è§£å†³æ–¹æ¡ˆ

**æ¶æ„è®¾è®¡**ï¼š

- é‡‡ç”¨Prometheus + Thanosæ¶æ„å®ç°é«˜å¯ç”¨æ—¶åºæ•°æ®å­˜å‚¨
- è¾¹ç¼˜éƒ¨ç½²Prometheus Agentè¿›è¡Œæœ¬åœ°é‡‡é›†å’Œé¢„å¤„ç†
- ä½¿ç”¨MQTT Brokerä½œä¸ºè®¾å¤‡æ•°æ®æ¥å…¥å±‚
- æŒ‡æ ‡æ ‡ç­¾è§„èŒƒåŒ–ï¼šdevice_idã€device_typeã€locationã€manufacturer
- å‘Šè­¦è§„åˆ™åˆ†çº§ï¼šè¾¹ç¼˜å‘Šè­¦ï¼ˆæœ¬åœ°å¤„ç†ï¼‰+ ä¸­å¿ƒå‘Šè­¦ï¼ˆå…¨å±€åˆ†æï¼‰

### 3.4 å®Œæ•´ä»£ç å®ç°

**IoTè®¾å¤‡Prometheusç›‘æ§ç³»ç»Ÿå®Œæ•´å®ç°ï¼ˆçº¦480è¡Œï¼‰**ï¼š

```python
#!/usr/bin/env python3
"""
IoTè®¾å¤‡Prometheusç›‘æ§ç³»ç»Ÿ
åŠŸèƒ½ï¼šè®¾å¤‡æŒ‡æ ‡é‡‡é›†ã€è¾¹ç¼˜é¢„å¤„ç†ã€è¿œç¨‹å†™å…¥ã€å‘Šè­¦æ£€æµ‹
"""

import json
import time
import random
import socket
import struct
import logging
from typing import Dict, List, Optional, Set, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from collections import defaultdict
from enum import Enum
from threading import Thread, Lock, Event
import queue

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DeviceStatus(str, Enum):
    """è®¾å¤‡çŠ¶æ€"""
    ONLINE = "online"
    OFFLINE = "offline"
    WARNING = "warning"
    ERROR = "error"
    MAINTENANCE = "maintenance"


class MetricType(str, Enum):
    """æŒ‡æ ‡ç±»å‹"""
    GAUGE = "gauge"
    COUNTER = "counter"
    HISTOGRAM = "histogram"


@dataclass
class Device:
    """IoTè®¾å¤‡æ¨¡å‹"""
    device_id: str
    device_type: str  # streetlight, sensor, camera, meter
    manufacturer: str
    location: Dict[str, str]  # city, district, lat, lng
    firmware_version: str
    labels: Dict[str, str] = field(default_factory=dict)
    last_seen: Optional[datetime] = None
    status: DeviceStatus = DeviceStatus.OFFLINE

    def get_labels_dict(self) -> Dict[str, str]:
        """è·å–å®Œæ•´çš„æ ‡ç­¾å­—å…¸"""
        return {
            "device_id": self.device_id,
            "device_type": self.device_type,
            "manufacturer": self.manufacturer,
            "city": self.location.get("city", "unknown"),
            "district": self.location.get("district", "unknown"),
            "firmware_version": self.firmware_version,
            **self.labels
        }


@dataclass
class MetricSample:
    """PrometheusæŒ‡æ ‡æ ·æœ¬"""
    name: str
    value: float
    labels: Dict[str, str]
    timestamp: datetime
    metric_type: MetricType = MetricType.GAUGE
    help_text: str = ""

    def to_prometheus_format(self) -> str:
        """è½¬æ¢ä¸ºPrometheus expositionæ ¼å¼"""
        label_str = ",".join([f'{k}="{v}"' for k, v in sorted(self.labels.items())])
        if label_str:
            label_str = "{" + label_str + "}"
        timestamp_ms = int(self.timestamp.timestamp() * 1000)
        return f"{self.name}{label_str} {self.value} {timestamp_ms}"


@dataclass
class AlertRule:
    """å‘Šè­¦è§„åˆ™"""
    name: str
    expr: str  # è¡¨è¾¾å¼ï¼Œå¦‚ "temperature > 80"
    duration: int  # æŒç»­æ—¶é—´(ç§’)
    severity: str  # critical, warning, info
    summary: str
    description: str

    def evaluate(self, metric_value: float, threshold: float) -> bool:
        """è¯„ä¼°å‘Šè­¦æ¡ä»¶"""
        if ">" in self.expr:
            return metric_value > threshold
        elif "<" in self.expr:
            return metric_value < threshold
        elif "==" in self.expr:
            return metric_value == threshold
        return False


@dataclass
class Alert:
    """å‘Šè­¦å®ä¾‹"""
    alert_id: str
    rule_name: str
    device_id: str
    severity: str
    status: str  # firing, resolved
    starts_at: datetime
    ends_at: Optional[datetime] = None
    value: float = 0.0
    labels: Dict[str, str] = field(default_factory=dict)
    annotations: Dict[str, str] = field(default_factory=dict)


class DeviceRegistry:
    """è®¾å¤‡æ³¨å†Œä¸­å¿ƒ"""

    def __init__(self):
        self._devices: Dict[str, Device] = {}
        self._lock = Lock()
        self._type_index: Dict[str, Set[str]] = defaultdict(set)
        self._city_index: Dict[str, Set[str]] = defaultdict(set)

    def register(self, device: Device):
        """æ³¨å†Œè®¾å¤‡"""
        with self._lock:
            self._devices[device.device_id] = device
            self._type_index[device.device_type].add(device.device_id)
            self._city_index[device.location.get("city", "unknown")].add(device.device_id)
        logger.info(f"Device registered: {device.device_id}")

    def get_device(self, device_id: str) -> Optional[Device]:
        """è·å–è®¾å¤‡"""
        with self._lock:
            return self._devices.get(device_id)

    def get_devices_by_type(self, device_type: str) -> List[Device]:
        """æŒ‰ç±»å‹è·å–è®¾å¤‡"""
        with self._lock:
            return [self._devices[did] for did in self._type_index.get(device_type, [])]

    def update_status(self, device_id: str, status: DeviceStatus):
        """æ›´æ–°è®¾å¤‡çŠ¶æ€"""
        with self._lock:
            if device_id in self._devices:
                self._devices[device_id].status = status
                self._devices[device_id].last_seen = datetime.now()

    def get_all_devices(self) -> List[Device]:
        """è·å–æ‰€æœ‰è®¾å¤‡"""
        with self._lock:
            return list(self._devices.values())

    def get_offline_devices(self, timeout_seconds: int = 300) -> List[Device]:
        """è·å–ç¦»çº¿è®¾å¤‡"""
        now = datetime.now()
        offline_devices = []
        with self._lock:
            for device in self._devices.values():
                if device.last_seen is None or (now - device.last_seen).seconds > timeout_seconds:
                    offline_devices.append(device)
        return offline_devices


class MetricsCollector:
    """æŒ‡æ ‡é‡‡é›†å™¨ - æ¨¡æ‹Ÿä»IoTè®¾å¤‡é‡‡é›†æŒ‡æ ‡"""

    def __init__(self, registry: DeviceRegistry):
        self.registry = registry
        self._running = False
        self._collect_interval = 15  # é‡‡é›†é—´éš”(ç§’)
        self._samples_queue: queue.Queue = queue.Queue(maxsize=100000)
        self._metric_definitions = {
            "device_temperature": {"type": MetricType.GAUGE, "unit": "celsius", "help": "Device temperature"},
            "device_humidity": {"type": MetricType.GAUGE, "unit": "percent", "help": "Environment humidity"},
            "device_uptime": {"type": MetricType.COUNTER, "unit": "seconds", "help": "Device uptime"},
            "device_cpu_usage": {"type": MetricType.GAUGE, "unit": "percent", "help": "CPU usage"},
            "device_memory_usage": {"type": MetricType.GAUGE, "unit": "percent", "help": "Memory usage"},
            "device_network_signal": {"type": MetricType.GAUGE, "unit": "dbm", "help": "Network signal strength"},
            "device_power_level": {"type": MetricType.GAUGE, "unit": "percent", "help": "Battery/power level"},
            "device_request_count": {"type": MetricType.COUNTER, "unit": "1", "help": "Request count"},
        }

    def start(self):
        """å¯åŠ¨é‡‡é›†"""
        self._running = True
        Thread(target=self._collect_loop, daemon=True).start()
        logger.info("Metrics collector started")

    def stop(self):
        """åœæ­¢é‡‡é›†"""
        self._running = False

    def _collect_loop(self):
        """é‡‡é›†å¾ªç¯"""
        while self._running:
            devices = self.registry.get_all_devices()
            for device in devices:
                samples = self._generate_device_metrics(device)
                for sample in samples:
                    try:
                        self._samples_queue.put(sample, timeout=1)
                    except queue.Full:
                        logger.warning("Metrics queue full, dropping sample")
            time.sleep(self._collect_interval)

    def _generate_device_metrics(self, device: Device) -> List[MetricSample]:
        """ç”Ÿæˆè®¾å¤‡æŒ‡æ ‡ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        now = datetime.now()
        samples = []
        labels = device.get_labels_dict()

        # æ ¹æ®è®¾å¤‡ç±»å‹ç”Ÿæˆä¸åŒçš„æŒ‡æ ‡
        if device.device_type == "streetlight":
            samples.append(MetricSample(
                name="streetlight_brightness",
                value=random.uniform(60, 100),
                labels=labels,
                timestamp=now,
                metric_type=MetricType.GAUGE,
                help_text="Streetlight brightness level"
            ))
            samples.append(MetricSample(
                name="streetlight_power_consumption",
                value=random.uniform(50, 150),
                labels=labels,
                timestamp=now,
                metric_type=MetricType.GAUGE,
                help_text="Power consumption in watts"
            ))

        elif device.device_type == "sensor":
            samples.append(MetricSample(
                name="device_temperature",
                value=random.uniform(20, 40),
                labels=labels,
                timestamp=now,
                metric_type=MetricType.GAUGE
            ))
            samples.append(MetricSample(
                name="device_humidity",
                value=random.uniform(30, 80),
                labels=labels,
                timestamp=now,
                metric_type=MetricType.GAUGE
            ))
            samples.append(MetricSample(
                name="device_pm25",
                value=random.uniform(0, 150),
                labels=labels,
                timestamp=now,
                metric_type=MetricType.GAUGE,
                help_text="PM2.5 concentration"
            ))

        # é€šç”¨æŒ‡æ ‡
        samples.append(MetricSample(
            name="device_uptime",
            value=random.randint(3600, 86400 * 30),
            labels=labels,
            timestamp=now,
            metric_type=MetricType.COUNTER
        ))
        samples.append(MetricSample(
            name="device_network_signal",
            value=random.uniform(-90, -50),
            labels=labels,
            timestamp=now,
            metric_type=MetricType.GAUGE
        ))
        samples.append(MetricSample(
            name="device_power_level",
            value=random.uniform(20, 100),
            labels=labels,
            timestamp=now,
            metric_type=MetricType.GAUGE
        ))

        return samples

    def get_samples_batch(self, batch_size: int = 1000) -> List[MetricSample]:
        """æ‰¹é‡è·å–æ ·æœ¬"""
        samples = []
        for _ in range(batch_size):
            try:
                samples.append(self._samples_queue.get_nowait())
            except queue.Empty:
                break
        return samples


class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""

    def __init__(self, registry: DeviceRegistry):
        self.registry = registry
        self._rules: List[AlertRule] = []
        self._active_alerts: Dict[str, Alert] = {}
        self._alert_history: List[Alert] = []
        self._lock = Lock()
        self._eval_interval = 30  # è¯„ä¼°é—´éš”(ç§’)
        self._running = False

    def add_rule(self, rule: AlertRule):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        self._rules.append(rule)
        logger.info(f"Alert rule added: {rule.name}")

    def start(self):
        """å¯åŠ¨å‘Šè­¦è¯„ä¼°"""
        self._running = True
        Thread(target=self._eval_loop, daemon=True).start()

    def stop(self):
        """åœæ­¢å‘Šè­¦è¯„ä¼°"""
        self._running = False

    def _eval_loop(self):
        """è¯„ä¼°å¾ªç¯"""
        while self._running:
            self._evaluate_rules()
            time.sleep(self._eval_interval)

    def _evaluate_rules(self):
        """è¯„ä¼°æ‰€æœ‰è§„åˆ™"""
        # æ¨¡æ‹ŸåŸºäºå½“å‰æŒ‡æ ‡çš„å‘Šè­¦è¯„ä¼°
        pass

    def evaluate_metric(self, device_id: str, metric_name: str, value: float):
        """è¯„ä¼°å•ä¸ªæŒ‡æ ‡"""
        for rule in self._rules:
            if metric_name in rule.expr:
                threshold = self._extract_threshold(rule.expr)
                if rule.evaluate(value, threshold):
                    self._fire_alert(rule, device_id, value)

    def _extract_threshold(self, expr: str) -> float:
        """ä»è¡¨è¾¾å¼ä¸­æå–é˜ˆå€¼"""
        import re
        match = re.search(r'\d+\.?\d*', expr)
        return float(match.group()) if match else 0.0

    def _fire_alert(self, rule: AlertRule, device_id: str, value: float):
        """è§¦å‘å‘Šè­¦"""
        alert_id = f"{rule.name}:{device_id}"
        with self._lock:
            if alert_id not in self._active_alerts:
                alert = Alert(
                    alert_id=alert_id,
                    rule_name=rule.name,
                    device_id=device_id,
                    severity=rule.severity,
                    status="firing",
                    starts_at=datetime.now(),
                    value=value,
                    labels={"device_id": device_id},
                    annotations={
                        "summary": rule.summary,
                        "description": rule.description
                    }
                )
                self._active_alerts[alert_id] = alert
                logger.warning(f"ALERT FIRING: {rule.name} for device {device_id}, value={value:.2f}")

    def resolve_alert(self, alert_id: str):
        """è§£å†³å‘Šè­¦"""
        with self._lock:
            if alert_id in self._active_alerts:
                alert = self._active_alerts.pop(alert_id)
                alert.status = "resolved"
                alert.ends_at = datetime.now()
                self._alert_history.append(alert)
                logger.info(f"ALERT RESOLVED: {alert.rule_name} for device {alert.device_id}")

    def get_active_alerts(self) -> List[Alert]:
        """è·å–æ´»è·ƒå‘Šè­¦"""
        with self._lock:
            return list(self._active_alerts.values())


class PrometheusRemoteWriter:
    """Prometheusè¿œç¨‹å†™å…¥å™¨ - å°†æŒ‡æ ‡å‘é€åˆ°è¿œç¨‹å­˜å‚¨"""

    def __init__(self, endpoint: str = "http://localhost:9090/api/v1/write"):
        self.endpoint = endpoint
        self._batch_size = 1000
        self._flush_interval = 10
        self._running = False
        self._buffer: List[MetricSample] = []
        self._lock = Lock()
        self._samples_sent = 0

    def start(self):
        """å¯åŠ¨å†™å…¥å™¨"""
        self._running = True
        Thread(target=self._flush_loop, daemon=True).start()

    def stop(self):
        """åœæ­¢å†™å…¥å™¨"""
        self._running = False
        self._flush()

    def write(self, sample: MetricSample):
        """å†™å…¥æ ·æœ¬"""
        with self._lock:
            self._buffer.append(sample)
            if len(self._buffer) >= self._batch_size:
                self._flush()

    def write_batch(self, samples: List[MetricSample]):
        """æ‰¹é‡å†™å…¥"""
        with self._lock:
            self._buffer.extend(samples)
            if len(self._buffer) >= self._batch_size:
                self._flush()

    def _flush_loop(self):
        """å®šæœŸåˆ·æ–°å¾ªç¯"""
        while self._running:
            time.sleep(self._flush_interval)
            self._flush()

    def _flush(self):
        """åˆ·æ–°ç¼“å†²åŒºåˆ°è¿œç¨‹å­˜å‚¨"""
        with self._lock:
            if not self._buffer:
                return
            batch = self._buffer[:self._batch_size]
            self._buffer = self._buffer[self._batch_size:]

        # æ¨¡æ‹Ÿå‘é€
        self._samples_sent += len(batch)
        logger.info(f"Flushed {len(batch)} samples to remote storage. Total: {self._samples_sent}")


class IoTMonitoringSystem:
    """IoTç›‘æ§ç³»ç»Ÿä¸»ç±»"""

    def __init__(self):
        self.registry = DeviceRegistry()
        self.collector = MetricsCollector(self.registry)
        self.alert_manager = AlertManager(self.registry)
        self.remote_writer = PrometheusRemoteWriter()
        self._running = False

    def initialize_devices(self, count: int = 100):
        """åˆå§‹åŒ–æ¨¡æ‹Ÿè®¾å¤‡"""
        device_types = ["streetlight", "sensor", "camera", "meter"]
        manufacturers = ["Huawei", "Dahua", "Hikvision", "Siemens"]
        cities = ["Beijing", "Shanghai", "Shenzhen", "Guangzhou", "Hangzhou"]

        for i in range(count):
            device = Device(
                device_id=f"DEV{str(i).zfill(6)}",
                device_type=random.choice(device_types),
                manufacturer=random.choice(manufacturers),
                location={
                    "city": random.choice(cities),
                    "district": f"District_{random.randint(1, 10)}",
                    "lat": str(random.uniform(30.0, 40.0)),
                    "lng": str(random.uniform(115.0, 125.0))
                },
                firmware_version=f"v{random.randint(1, 5)}.{random.randint(0, 9)}"
            )
            self.registry.register(device)
        logger.info(f"Initialized {count} devices")

    def setup_alert_rules(self):
        """è®¾ç½®å‘Šè­¦è§„åˆ™"""
        rules = [
            AlertRule(
                name="HighTemperature",
                expr="device_temperature > 80",
                duration=300,
                severity="critical",
                summary="Device temperature too high",
                description="Device {{ $labels.device_id }} temperature is {{ $value }} C"
            ),
            AlertRule(
                name="LowBattery",
                expr="device_power_level < 20",
                duration=60,
                severity="warning",
                summary="Device battery low",
                description="Device {{ $labels.device_id }} battery level is {{ $value }}%"
            ),
            AlertRule(
                name="WeakSignal",
                expr="device_network_signal < -85",
                duration=180,
                severity="warning",
                summary="Device network signal weak",
                description="Device {{ $labels.device_id }} signal strength is {{ $value }} dBm"
            ),
            AlertRule(
                name="DeviceOffline",
                expr="up == 0",
                duration=300,
                severity="critical",
                summary="Device offline",
                description="Device {{ $labels.device_id }} has been offline for more than 5 minutes"
            )
        ]
        for rule in rules:
            self.alert_manager.add_rule(rule)

    def start(self):
        """å¯åŠ¨ç³»ç»Ÿ"""
        self._running = True
        self.collector.start()
        self.alert_manager.start()
        self.remote_writer.start()

        # å¯åŠ¨æŒ‡æ ‡å¤„ç†çº¿ç¨‹
        Thread(target=self._process_metrics, daemon=True).start()
        logger.info("IoT Monitoring System started")

    def stop(self):
        """åœæ­¢ç³»ç»Ÿ"""
        self._running = False
        self.collector.stop()
        self.alert_manager.stop()
        self.remote_writer.stop()

    def _process_metrics(self):
        """å¤„ç†æŒ‡æ ‡ - è·å–é‡‡é›†çš„æŒ‡æ ‡å¹¶å‘é€åˆ°è¿œç¨‹å­˜å‚¨"""
        while self._running:
            samples = self.collector.get_samples_batch(batch_size=1000)
            if samples:
                self.remote_writer.write_batch(samples)

                # è¯„ä¼°å‘Šè­¦
                for sample in samples:
                    if sample.name in ["device_temperature", "device_power_level", "device_network_signal"]:
                        self.alert_manager.evaluate_metric(
                            sample.labels.get("device_id", ""),
                            sample.name,
                            sample.value
                        )
            time.sleep(1)

    def get_system_status(self) -> Dict:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        devices = self.registry.get_all_devices()
        offline_devices = self.registry.get_offline_devices()
        active_alerts = self.alert_manager.get_active_alerts()

        return {
            "total_devices": len(devices),
            "offline_devices": len(offline_devices),
            "online_rate": f"{(len(devices) - len(offline_devices)) / len(devices) * 100:.1f}%" if devices else "0%",
            "active_alerts": len(active_alerts),
            "critical_alerts": len([a for a in active_alerts if a.severity == "critical"]),
            "samples_sent": self.remote_writer._samples_sent
        }


# ============ æ¼”ç¤º ============

def demo_iot_monitoring():
    """æ¼”ç¤ºIoTç›‘æ§ç³»ç»Ÿ"""
    system = IoTMonitoringSystem()

    # åˆå§‹åŒ–100ä¸ªè®¾å¤‡
    system.initialize_devices(count=100)

    # è®¾ç½®å‘Šè­¦è§„åˆ™
    system.setup_alert_rules()

    # å¯åŠ¨ç³»ç»Ÿ
    system.start()

    try:
        # è¿è¡Œ60ç§’
        for i in range(6):
            time.sleep(10)
            status = system.get_system_status()
            print("\n" + "="*60)
            print(f"ç³»ç»Ÿè¿è¡ŒçŠ¶æ€ (t={i*10}s):")
            print(f"  è®¾å¤‡æ€»æ•°: {status['total_devices']}")
            print(f"  ç¦»çº¿è®¾å¤‡: {status['offline_devices']}")
            print(f"  åœ¨çº¿ç‡: {status['online_rate']}")
            print(f"  æ´»è·ƒå‘Šè­¦: {status['active_alerts']}")
            print(f"  ä¸¥é‡å‘Šè­¦: {status['critical_alerts']}")
            print(f"  å·²å‘é€æ ·æœ¬: {status['samples_sent']}")

            # æ˜¾ç¤ºæ´»è·ƒå‘Šè­¦
            alerts = system.alert_manager.get_active_alerts()
            if alerts:
                print("\n  å½“å‰å‘Šè­¦:")
                for alert in alerts[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"    - [{alert.severity.upper()}] {alert.rule_name}: {alert.annotations.get('summary', '')}")

    finally:
        system.stop()
        print("\n" + "="*60)
        print("ç³»ç»Ÿå·²åœæ­¢")


if __name__ == '__main__':
    demo_iot_monitoring()
```

### 3.5 æ•ˆæœè¯„ä¼°

**æ€§èƒ½æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | æ”¹è¿›å‰ | æ”¹è¿›å | æå‡å¹…åº¦ |
|------|--------|--------|----------|
| è®¾å¤‡ç›‘æ§è¦†ç›–ç‡ | 30% | 100% | +70% |
| æ•…éšœå‘ç°æ—¶é—´ | 2å°æ—¶ | 3.5åˆ†é’Ÿ | -97% |
| ç³»ç»Ÿæ‰¿è½½è®¾å¤‡æ•° | 5ä¸‡ | 60ä¸‡+ | +1100% |
| æ•°æ®é‡‡é›†æˆåŠŸç‡ | 85% | 99.7% | +14.7% |
| å‘Šè­¦å‡†ç¡®ç‡ | 35% | 92% | +57% |
| è¾¹ç¼˜æ•°æ®å¤„ç†å»¶è¿Ÿ | - | <100ms | - |

**ä¸šåŠ¡ä»·å€¼**ï¼š

1. **ç›´æ¥ç»æµæ”¶ç›Š**ï¼š
   - å¹´åº¦è®¾å¤‡ç»´æŠ¤æˆæœ¬é™ä½2800ä¸‡å…ƒï¼ˆé¢„æµ‹æ€§ç»´æŠ¤å‡å°‘ç´§æ€¥ç»´ä¿®ï¼‰
   - èƒ½æºæµªè´¹å‡å°‘15%ï¼Œå¹´èŠ‚çœç”µè´¹çº¦600ä¸‡å…ƒï¼ˆæ™ºèƒ½è·¯ç¯æŒ‰éœ€è°ƒèŠ‚ï¼‰
   - è®¾å¤‡ç›—çªƒå’ŒæŸåæŸå¤±é™ä½40%ï¼Œå¹´å‡å°‘æŸå¤±çº¦400ä¸‡å…ƒ

2. **è¿è¥æ•ˆç‡æå‡**ï¼š
   - è¿ç»´å›¢é˜Ÿäººæ•ˆæå‡400%ï¼ˆäººå‡ç®¡ç†è®¾å¤‡æ•°ä»500æå‡è‡³2000ï¼‰
   - å·¡æ£€å·¥ä½œè‡ªåŠ¨åŒ–ç‡85%ï¼Œé‡Šæ”¾äººåŠ›æŠ•å…¥é«˜ä»·å€¼å·¥ä½œ
   - è®¾å¤‡æ•…éšœå·¥å•è‡ªåŠ¨æ´¾å•å‡†ç¡®ç‡è¾¾95%

3. **æœåŠ¡è´¨é‡æ”¹å–„**ï¼š
   - å¸‚æ°‘æŠ•è¯‰ç‡ä¸‹é™60%ï¼ˆè·¯ç¯ã€ä¼ æ„Ÿå™¨æ•…éšœå¿«é€Ÿä¿®å¤ï¼‰
   - ç¯å¢ƒç›‘æµ‹æ•°æ®å®æ—¶æ€§æå‡ï¼Œç©ºæ°”è´¨é‡é¢„è­¦æå‰30åˆ†é’Ÿ
   - äº¤é€šæ‹¥å µæ£€æµ‹å‡†ç¡®ç‡æå‡è‡³95%

4. **æ•°æ®èµ„äº§ä»·å€¼**ï¼š
   - ç§¯ç´¯è®¾å¤‡è¿è¡Œå¤§æ•°æ®ï¼Œæ”¯æŒè®¾å¤‡é€‰å‹ä¼˜åŒ–
   - åŸºäºå†å²æ•°æ®é¢„æµ‹è®¾å¤‡å¯¿å‘½ï¼Œå¤‡ä»¶åº“å­˜ä¼˜åŒ–èŠ‚çœæˆæœ¬

**ç»éªŒæ•™è®­**ï¼š

1. **è¾¹ç¼˜è®¡ç®—çš„é‡è¦æ€§**ï¼š50%çš„æ•°æ®åœ¨è¾¹ç¼˜èŠ‚ç‚¹é¢„å¤„ç†ï¼Œå‡å°‘äº‘ç«¯å¸¦å®½æˆæœ¬70%ï¼ŒåŒæ—¶é™ä½å»¶è¿Ÿ
2. **ç½‘ç»œå®¹é”™è®¾è®¡**ï¼šè®¾å¤‡å¼±ç½‘ç¯å¢ƒä¸‹é‡‡ç”¨æœ¬åœ°ç¼“å­˜+æ–­ç‚¹ç»­ä¼ ï¼Œä¿è¯æ•°æ®å®Œæ•´æ€§
3. **æŒ‡æ ‡åŸºæ•°æ§åˆ¶**ï¼šåˆæœŸæ ‡ç­¾è®¾è®¡è¿‡äºå®½æ¾å¯¼è‡´æŒ‡æ ‡åŸºæ•°çˆ†ç‚¸ï¼ŒåæœŸé€šè¿‡æ ‡ç­¾è§„èŒƒåŒ–è§£å†³
4. **è®¾å¤‡ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼šè®¾å¤‡ä»æ³¨å†Œåˆ°æŠ¥åºŸçš„å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†è‡³å…³é‡è¦ï¼Œé¿å…åƒµå°¸è®¾å¤‡å ç”¨èµ„æº
5. **å¤šäº‘éƒ¨ç½²ç­–ç•¥**ï¼šé‡‡ç”¨å¤šäº‘æ¶æ„é¿å…å•ç‚¹æ•…éšœï¼Œç¡®ä¿ç›‘æ§ç³»ç»Ÿè‡ªèº«é«˜å¯ç”¨


---

## 4. æ¡ˆä¾‹3ï¼šå¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ

### 4.1 ä¸šåŠ¡èƒŒæ™¯

**ä¼ä¸šèƒŒæ™¯**ï¼š
æŸé‡‘èç§‘æŠ€å…¬å¸ï¼ˆä»¥ä¸‹ç®€ç§°"Cå…¬å¸"ï¼‰ï¼Œæ—¥å‡äº¤æ˜“é‡è¶…è¿‡1äº¿ç¬”ï¼Œå³°å€¼TPSè¾¾10ä¸‡ã€‚ç³»ç»Ÿæ¯å¤©äº§ç”Ÿçº¦50TBå¯è§‚æµ‹æ€§æ•°æ®ï¼ŒåŒ…æ‹¬æŒ‡æ ‡ã€æ—¥å¿—ã€è¿½è¸ªä¸‰ç±»æ•°æ®ï¼Œéœ€è¦æ”¯æŒå®æ—¶æŸ¥è¯¢ã€å†å²åˆ†æå’Œåˆè§„å®¡è®¡ã€‚

**ä¸šåŠ¡ç—›ç‚¹**ï¼š

1. **æ•°æ®å­˜å‚¨æˆæœ¬é«˜æ˜‚**ï¼šåŸå§‹æ•°æ®å­˜å‚¨æˆæœ¬æ¯æœˆè¶…è¿‡200ä¸‡å…ƒï¼Œä¸”å¿«é€Ÿå¢é•¿
2. **æŸ¥è¯¢æ€§èƒ½å·®**ï¼šå¤æ‚æŸ¥è¯¢ç»å¸¸è¶…æ—¶ï¼Œå¹³å‡æŸ¥è¯¢å»¶è¿Ÿè¶…è¿‡10ç§’
3. **æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†ç¼ºå¤±**ï¼šç¼ºä¹è‡ªåŠ¨å½’æ¡£å’Œæ¸…ç†æœºåˆ¶ï¼Œæ•°æ®æ— é™å¢é•¿
4. **å¤šæ•°æ®æºåˆ†æå›°éš¾**ï¼šæŒ‡æ ‡ã€æ—¥å¿—ã€è¿½è¸ªæ•°æ®å­˜å‚¨åœ¨ä¸åŒç³»ç»Ÿï¼Œå…³è”åˆ†æå¤æ‚
5. **åˆè§„å®¡è®¡å‹åŠ›**ï¼šé‡‘èæ•°æ®éœ€è¦ä¿ç•™5å¹´ï¼Œä¼ ç»Ÿå­˜å‚¨æ–¹æ¡ˆæ— æ³•æ»¡è¶³

**ä¸šåŠ¡ç›®æ ‡**ï¼š

- é™ä½å­˜å‚¨æˆæœ¬50%ä»¥ä¸Š
- å°†æŸ¥è¯¢å»¶è¿Ÿæ§åˆ¶åœ¨1ç§’ä»¥å†…
- å®ç°è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†
- å»ºç«‹ç»Ÿä¸€çš„æ•°æ®åˆ†æå¹³å°
- æ»¡è¶³é‡‘èè¡Œä¸šåˆè§„å®¡è®¡è¦æ±‚

### 4.2 æŠ€æœ¯æŒ‘æˆ˜

1. **æµ·é‡æ•°æ®å†™å…¥**ï¼šæ—¥å‡50TBæ•°æ®ï¼Œå³°å€¼å†™å…¥è¾¾100MB/s
2. **å†·çƒ­æ•°æ®åˆ†ç¦»**ï¼š95%æŸ¥è¯¢é’ˆå¯¹æœ€è¿‘7å¤©æ•°æ®ï¼Œéœ€è¦æ™ºèƒ½åˆ†å±‚å­˜å‚¨
3. **å®æ—¶ä¸ç¦»çº¿å¹³è¡¡**ï¼šå®æ—¶æŸ¥è¯¢ä¸ç¦»çº¿åˆ†æéœ€è¦ä¸åŒçš„å­˜å‚¨æ ¼å¼
4. **æ•°æ®å‹ç¼©æ•ˆç‡**ï¼šéœ€è¦åœ¨æŸ¥è¯¢æ€§èƒ½å’Œå‹ç¼©ç‡ä¹‹é—´æ‰¾åˆ°å¹³è¡¡
5. **å¤šç§Ÿæˆ·éš”ç¦»**ï¼šä¸åŒä¸šåŠ¡çº¿æ•°æ®éœ€è¦ç‰©ç†æˆ–é€»è¾‘éš”ç¦»

### 4.3 è§£å†³æ–¹æ¡ˆ

**æ¶æ„è®¾è®¡**ï¼š

- é‡‡ç”¨åˆ†å±‚å­˜å‚¨ï¼šçƒ­æ•°æ®ï¼ˆSSDï¼Œ7å¤©ï¼‰+ æ¸©æ•°æ®ï¼ˆHDDï¼Œ30å¤©ï¼‰+ å†·æ•°æ®ï¼ˆå¯¹è±¡å­˜å‚¨ï¼Œ5å¹´ï¼‰
- ä½¿ç”¨ClickHouseå­˜å‚¨æŒ‡æ ‡æ•°æ®ï¼ŒElasticsearchå­˜å‚¨æ—¥å¿—ï¼ŒCephå­˜å‚¨è¿½è¸ªæ•°æ®
- å®ç°æ•°æ®è‡ªåŠ¨é™çº§ï¼š7å¤©â†’30å¤©â†’1å¹´â†’5å¹´ï¼Œä¸åŒå±‚çº§ä¸åŒå‹ç¼©ç­–ç•¥
- å»ºç«‹ç»Ÿä¸€æŸ¥è¯¢å±‚ï¼Œè‡ªåŠ¨è·¯ç”±åˆ°åˆé€‚çš„å­˜å‚¨åç«¯
- é‡‡ç”¨åˆ—å¼å­˜å‚¨å’Œæ™ºèƒ½å‹ç¼©ï¼Œé™ä½å­˜å‚¨æˆæœ¬70%

### 4.4 å®Œæ•´ä»£ç å®ç°

**å¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿå®Œæ•´å®ç°ï¼ˆçº¦500è¡Œï¼‰**ï¼š

```python
#!/usr/bin/env python3
"""
å¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨ä¸åˆ†æç³»ç»Ÿ
åŠŸèƒ½ï¼šåˆ†å±‚å­˜å‚¨ã€æ•°æ®å‹ç¼©ã€ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€ç»Ÿä¸€æŸ¥è¯¢
"""

import json
import gzip
import time
import hashlib
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from enum import Enum
from collections import defaultdict
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import struct

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DataTier(str, Enum):
    """æ•°æ®å­˜å‚¨å±‚çº§"""
    HOT = "hot"       # SSD, æœ€è¿‘7å¤©
    WARM = "warm"     # HDD, 7-30å¤©
    COLD = "cold"     # å¯¹è±¡å­˜å‚¨, 30å¤©-1å¹´
    ARCHIVE = "archive"  # å†·å½’æ¡£, 1-5å¹´


class DataType(str, Enum):
    """æ•°æ®ç±»å‹"""
    METRIC = "metric"
    LOG = "log"
    TRACE = "trace"


@dataclass
class StoragePolicy:
    """å­˜å‚¨ç­–ç•¥"""
    tier: DataTier
    retention_days: int
    compression: str  # none, gzip, zstd, snappy
    replication_factor: int
    index_fields: List[str] = field(default_factory=list)


@dataclass
class TimeSeriesPoint:
    """æ—¶åºæ•°æ®ç‚¹"""
    metric_name: str
    timestamp: datetime
    value: float
    labels: Dict[str, str] = field(default_factory=dict)
    data_type: DataType = DataType.METRIC

    def to_storage_format(self) -> bytes:
        """è½¬æ¢ä¸ºå­˜å‚¨æ ¼å¼"""
        data = {
            "n": self.metric_name,
            "t": int(self.timestamp.timestamp()),
            "v": self.value,
            "l": self.labels
        }
        return json.dumps(data, separators=(',', ':')).encode('utf-8')

    @classmethod
    def from_storage_format(cls, data: bytes) -> 'TimeSeriesPoint':
        """ä»å­˜å‚¨æ ¼å¼è§£æ"""
        obj = json.loads(data.decode('utf-8'))
        return cls(
            metric_name=obj["n"],
            timestamp=datetime.fromtimestamp(obj["t"]),
            value=obj["v"],
            labels=obj["l"]
        )


@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®"""
    log_id: str
    timestamp: datetime
    level: str
    message: str
    service: str
    trace_id: Optional[str]
    span_id: Optional[str]
    labels: Dict[str, str] = field(default_factory=dict)

    def to_storage_format(self) -> bytes:
        """è½¬æ¢ä¸ºå­˜å‚¨æ ¼å¼"""
        data = {
            "i": self.log_id,
            "t": int(self.timestamp.timestamp()),
            "l": self.level,
            "m": self.message,
            "s": self.service,
            "tid": self.trace_id,
            "sid": self.span_id,
            "labels": self.labels
        }
        return json.dumps(data, separators=(',', ':')).encode('utf-8')


class CompressionEngine:
    """å‹ç¼©å¼•æ“"""

    SUPPORTED_ALGORITHMS = ['none', 'gzip', 'zstd']

    @staticmethod
    def compress(data: bytes, algorithm: str = 'gzip') -> Tuple[bytes, str]:
        """å‹ç¼©æ•°æ®"""
        if algorithm == 'none':
            return data, 'none'
        elif algorithm == 'gzip':
            return gzip.compress(data, compresslevel=6), 'gzip'
        elif algorithm == 'zstd':
            return gzip.compress(data, compresslevel=3), 'zstd'
        return data, 'none'

    @staticmethod
    def decompress(data: bytes, algorithm: str) -> bytes:
        """è§£å‹ç¼©æ•°æ®"""
        if algorithm == 'none':
            return data
        elif algorithm in ['gzip', 'zstd']:
            return gzip.decompress(data)
        return data


class StorageBackend:
    """å­˜å‚¨åç«¯æŠ½è±¡åŸºç±»"""

    def __init__(self, name: str, tier: DataTier):
        self.name = name
        self.tier = tier
        self._storage: Dict[str, bytes] = {}
        self._metadata: Dict[str, Dict] = {}
        self._lock = threading.RLock()
        self._size_bytes = 0

    def write(self, key: str, data: bytes, metadata: Dict = None) -> bool:
        """å†™å…¥æ•°æ®"""
        with self._lock:
            self._storage[key] = data
            self._metadata[key] = metadata or {}
            self._size_bytes += len(data)
        return True

    def read(self, key: str) -> Optional[bytes]:
        """è¯»å–æ•°æ®"""
        with self._lock:
            return self._storage.get(key)

    def delete(self, key: str) -> bool:
        """åˆ é™¤æ•°æ®"""
        with self._lock:
            if key in self._storage:
                self._size_bytes -= len(self._storage[key])
                del self._storage[key]
                del self._metadata[key]
                return True
        return False

    def query(self, prefix: str = None, start_time: datetime = None,
              end_time: datetime = None) -> List[Dict]:
        """æŸ¥è¯¢æ•°æ®"""
        results = []
        with self._lock:
            for key, meta in self._metadata.items():
                if prefix and not key.startswith(prefix):
                    continue
                if start_time and meta.get('timestamp', datetime.now()) < start_time:
                    continue
                if end_time and meta.get('timestamp', datetime.now()) > end_time:
                    continue
                results.append({"key": key, "metadata": meta})
        return results

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            return {
                "name": self.name,
                "tier": self.tier.value,
                "total_keys": len(self._storage),
                "size_bytes": self._size_bytes,
                "size_mb": self._size_bytes / 1024 / 1024
            }


class TieredStorageManager:
    """åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨"""

    def __init__(self):
        self.backends: Dict[DataTier, StorageBackend] = {
            DataTier.HOT: StorageBackend("hot_ssd", DataTier.HOT),
            DataTier.WARM: StorageBackend("warm_hdd", DataTier.WARM),
            DataTier.COLD: StorageBackend("cold_object", DataTier.COLD),
            DataTier.ARCHIVE: StorageBackend("archive_tape", DataTier.ARCHIVE)
        }

        self.policies: Dict[DataType, Dict[DataTier, StoragePolicy]] = {
            DataType.METRIC: {
                DataTier.HOT: StoragePolicy(DataTier.HOT, 7, 'none', 2, ['metric_name', 'timestamp']),
                DataTier.WARM: StoragePolicy(DataTier.WARM, 30, 'zstd', 2, ['metric_name']),
                DataTier.COLD: StoragePolicy(DataTier.COLD, 365, 'zstd', 1, ['metric_name']),
                DataTier.ARCHIVE: StoragePolicy(DataTier.ARCHIVE, 1825, 'gzip', 1, [])
            },
            DataType.LOG: {
                DataTier.HOT: StoragePolicy(DataTier.HOT, 3, 'none', 2, ['service', 'level', 'timestamp']),
                DataTier.WARM: StoragePolicy(DataTier.WARM, 14, 'gzip', 2, ['service', 'level']),
                DataTier.COLD: StoragePolicy(DataTier.COLD, 90, 'gzip', 1, ['service']),
                DataTier.ARCHIVE: StoragePolicy(DataTier.ARCHIVE, 1825, 'gzip', 1, [])
            },
            DataType.TRACE: {
                DataTier.HOT: StoragePolicy(DataTier.HOT, 1, 'none', 2, ['trace_id', 'service']),
                DataTier.WARM: StoragePolicy(DataTier.WARM, 7, 'zstd', 2, ['trace_id']),
                DataTier.COLD: StoragePolicy(DataTier.COLD, 30, 'zstd', 1, ['service']),
                DataTier.ARCHIVE: StoragePolicy(DataTier.ARCHIVE, 365, 'gzip', 1, [])
            }
        }

        self.compression = CompressionEngine()
        self._tier_downgrade_interval = 3600  # 1å°æ—¶æ£€æŸ¥ä¸€æ¬¡
        self._running = False
        self._executor = ThreadPoolExecutor(max_workers=4)

    def store(self, data_type: DataType, key: str, data: bytes,
              timestamp: datetime, metadata: Dict = None) -> bool:
        """å­˜å‚¨æ•°æ® - è‡ªåŠ¨è·¯ç”±åˆ°åˆé€‚çš„å±‚çº§"""
        tier = self._determine_tier(data_type, timestamp)
        policy = self.policies[data_type][tier]

        compressed_data, algo = self.compression.compress(data, policy.compression)

        meta = {
            "data_type": data_type.value,
            "original_size": len(data),
            "compressed_size": len(compressed_data),
            "compression_ratio": len(data) / len(compressed_data) if len(compressed_data) > 0 else 1,
            "compression_algorithm": algo,
            "timestamp": timestamp,
            "tier": tier.value,
            **(metadata or {})
        }

        backend = self.backends[tier]
        return backend.write(key, compressed_data, meta)

    def _determine_tier(self, data_type: DataType, timestamp: datetime) -> DataTier:
        """ç¡®å®šæ•°æ®åº”å­˜å‚¨çš„å±‚çº§"""
        age_days = (datetime.now() - timestamp).days

        for tier in [DataTier.HOT, DataTier.WARM, DataTier.COLD]:
            policy = self.policies[data_type][tier]
            if age_days < policy.retention_days:
                return tier
        return DataTier.ARCHIVE

    def retrieve(self, key: str, data_type: DataType) -> Optional[bytes]:
        """æ£€ç´¢æ•°æ® - è‡ªåŠ¨åœ¨æ‰€æœ‰å±‚çº§æŸ¥æ‰¾"""
        for tier in DataTier:
            backend = self.backends[tier]
            data = backend.read(key)
            if data is not None:
                meta = backend._metadata.get(key, {})
                algo = meta.get('compression_algorithm', 'none')
                return self.compression.decompress(data, algo)
        return None

    def query_by_time_range(self, data_type: DataType, start: datetime,
                           end: datetime, filters: Dict = None) -> List[Dict]:
        """æŒ‰æ—¶é—´èŒƒå›´æŸ¥è¯¢"""
        results = []

        tiers_to_query = set()
        for tier in DataTier:
            policy = self.policies[data_type][tier]
            tier_start = datetime.now() - timedelta(days=policy.retention_days)
            if end >= tier_start:
                tiers_to_query.add(tier)

        futures = []
        for tier in tiers_to_query:
            backend = self.backends[tier]
            future = self._executor.submit(backend.query, None, start, end)
            futures.append((tier, future))

        for tier, future in futures:
            try:
                tier_results = future.result(timeout=5)
                for item in tier_results:
                    item['tier'] = tier.value
                results.extend(tier_results)
            except Exception as e:
                logger.error(f"Query failed for tier {tier}: {e}")

        return results

    def start_lifecycle_manager(self):
        """å¯åŠ¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
        self._running = True
        threading.Thread(target=self._lifecycle_loop, daemon=True).start()
        logger.info("Lifecycle manager started")

    def stop_lifecycle_manager(self):
        """åœæ­¢ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
        self._running = False

    def _lifecycle_loop(self):
        """ç”Ÿå‘½å‘¨æœŸç®¡ç†å¾ªç¯"""
        while self._running:
            self._execute_tier_downgrade()
            self._execute_data_expiration()
            time.sleep(self._tier_downgrade_interval)

    def _execute_tier_downgrade(self):
        """æ‰§è¡Œå±‚çº§é™çº§"""
        logger.info("Executing tier downgrade...")
        for data_type in DataType:
            for source_tier in [DataTier.HOT, DataTier.WARM, DataTier.COLD]:
                policy = self.policies[data_type][source_tier]
                backend = self.backends[source_tier]

                with backend._lock:
                    keys_to_downgrade = []
                    for key, meta in backend._metadata.items():
                        if meta.get('data_type') != data_type.value:
                            continue
                        timestamp = meta.get('timestamp')
                        if timestamp and (datetime.now() - timestamp).days >= policy.retention_days:
                            keys_to_downgrade.append(key)

                for key in keys_to_downgrade[:100]:
                    self._downgrade_data(key, data_type, source_tier)

    def _downgrade_data(self, key: str, data_type: DataType, from_tier: DataTier):
        """é™çº§æ•°æ®åˆ°æ›´ä½å±‚çº§"""
        source_backend = self.backends[from_tier]
        data = source_backend.read(key)
        meta = source_backend._metadata.get(key, {})

        if data is None:
            return

        algo = meta.get('compression_algorithm', 'none')
        original_data = self.compression.decompress(data, algo)
        timestamp = meta.get('timestamp', datetime.now())

        self.store(data_type, key, original_data, timestamp,
                  {k: v for k, v in meta.items() if k not in ['tier', 'compression_algorithm']})

        source_backend.delete(key)
        logger.debug(f"Downgraded {key} from {from_tier.value}")

    def _execute_data_expiration(self):
        """æ‰§è¡Œæ•°æ®è¿‡æœŸåˆ é™¤"""
        for data_type in DataType:
            archive_policy = self.policies[data_type][DataTier.ARCHIVE]
            backend = self.backends[DataTier.ARCHIVE]

            with backend._lock:
                keys_to_delete = []
                for key, meta in backend._metadata.items():
                    timestamp = meta.get('timestamp')
                    if timestamp and (datetime.now() - timestamp).days >= archive_policy.retention_days:
                        keys_to_delete.append(key)

                for key in keys_to_delete:
                    backend.delete(key)
                    logger.info(f"Expired and deleted: {key}")

    def get_storage_stats(self) -> Dict:
        """è·å–å­˜å‚¨ç»Ÿè®¡"""
        stats = {
            "backends": {},
            "total_keys": 0,
            "total_size_mb": 0,
            "compression_stats": defaultdict(lambda: {"original": 0, "compressed": 0})
        }

        for tier, backend in self.backends.items():
            backend_stats = backend.get_stats()
            stats["backends"][tier.value] = backend_stats
            stats["total_keys"] += backend_stats["total_keys"]
            stats["total_size_mb"] += backend_stats["size_mb"]

            with backend._lock:
                for meta in backend._metadata.values():
                    algo = meta.get('compression_algorithm', 'none')
                    stats["compression_stats"][algo]["original"] += meta.get('original_size', 0)
                    stats["compression_stats"][algo]["compressed"] += meta.get('compressed_size', 0)

        return stats


class QueryEngine:
    """æŸ¥è¯¢å¼•æ“ - ç»Ÿä¸€æŸ¥è¯¢æ¥å£"""

    def __init__(self, storage_manager: TieredStorageManager):
        self.storage = storage_manager
        self._query_cache: Dict[str, Any] = {}
        self._cache_ttl = 60

    def query_metrics(self, metric_name: str, start: datetime, end: datetime,
                     labels: Dict = None, aggregation: str = None) -> Dict:
        """æŸ¥è¯¢æŒ‡æ ‡æ•°æ®"""
        cache_key = f"metric:{metric_name}:{start.isoformat()}:{end.isoformat()}"
        if cache_key in self._query_cache:
            return self._query_cache[cache_key]

        results = self.storage.query_by_time_range(DataType.METRIC, start, end)

        points = []
        for item in results:
            data = self.storage.retrieve(item['key'], DataType.METRIC)
            if data:
                point = TimeSeriesPoint.from_storage_format(data)
                if point.metric_name == metric_name:
                    if labels and not all(point.labels.get(k) == v for k, v in labels.items()):
                        continue
                    points.append(point)

        points.sort(key=lambda p: p.timestamp)

        values = [p.value for p in points]
        result = {
            "metric_name": metric_name,
            "points_count": len(points),
            "time_range": {"start": start.isoformat(), "end": end.isoformat()},
            "statistics": {
                "min": min(values) if values else 0,
                "max": max(values) if values else 0,
                "avg": sum(values) / len(values) if values else 0,
                "sum": sum(values)
            },
            "series": [{"timestamp": p.timestamp.isoformat(), "value": p.value, "labels": p.labels}
                      for p in points[:1000]]
        }

        self._query_cache[cache_key] = result
        return result

    def query_logs(self, service: str, level: str = None,
                  start: datetime = None, end: datetime = None,
                  keyword: str = None, limit: int = 100) -> List[Dict]:
        """æŸ¥è¯¢æ—¥å¿—"""
        results = self.storage.query_by_time_range(DataType.LOG, start or datetime.now() - timedelta(hours=1),
                                                   end or datetime.now())
        logs = []
        for item in results:
            data = self.storage.retrieve(item['key'], DataType.LOG)
            if data:
                log_obj = json.loads(data.decode('utf-8'))
                if log_obj.get('s') == service:
                    if level and log_obj.get('l') != level:
                        continue
                    if keyword and keyword not in log_obj.get('m', ''):
                        continue
                    logs.append({
                        "timestamp": datetime.fromtimestamp(log_obj['t']).isoformat(),
                        "level": log_obj['l'],
                        "message": log_obj['m'],
                        "service": log_obj['s'],
                        "trace_id": log_obj.get('tid')
                    })
        return logs[:limit]

    def get_trace(self, trace_id: str) -> Optional[Dict]:
        """è·å–å®Œæ•´è°ƒç”¨é“¾"""
        for tier in DataTier:
            backend = self.storage.backends[tier]
            results = backend.query(prefix=f"trace:{trace_id}")
            if results:
                spans = []
                for item in results:
                    data = backend.read(item['key'])
                    if data:
                        algo = item['metadata'].get('compression_algorithm', 'none')
                        decompressed = CompressionEngine.decompress(data, algo)
                        spans.append(json.loads(decompressed.decode('utf-8')))
                return {"trace_id": trace_id, "spans": spans}
        return None


# ============ æ¼”ç¤º ============

def demo_storage_system():
    """æ¼”ç¤ºå­˜å‚¨ç³»ç»Ÿ"""
    storage = TieredStorageManager()
    query_engine = QueryEngine(storage)
    storage.start_lifecycle_manager()

    try:
        now = datetime.now()

        logger.info("Writing recent data (hot tier)...")
        for i in range(1000):
            point = TimeSeriesPoint(
                metric_name="cpu_usage",
                timestamp=now - timedelta(minutes=i),
                value=random.uniform(20, 80),
                labels={"host": f"server-{i % 10}", "datacenter": "dc1"}
            )
            key = f"metric:{point.metric_name}:{point.timestamp.timestamp()}:{i}"
            storage.store(DataType.METRIC, key, point.to_storage_format(), point.timestamp)

        logger.info("Writing 15-day-old data (warm tier)...")
        for i in range(500):
            point = TimeSeriesPoint(
                metric_name="cpu_usage",
                timestamp=now - timedelta(days=15, minutes=i),
                value=random.uniform(20, 80),
                labels={"host": f"server-{i % 10}", "datacenter": "dc1"}
            )
            key = f"metric:{point.metric_name}:{point.timestamp.timestamp()}:{i}"
            storage.store(DataType.METRIC, key, point.to_storage_format(), point.timestamp)

        logger.info("Writing log data...")
        for i in range(500):
            log = LogEntry(
                log_id=f"log-{i}",
                timestamp=now - timedelta(minutes=i),
                level=random.choice(["INFO", "WARN", "ERROR"]),
                message=f"Operation completed: task_{i}",
                service="payment-service",
                trace_id=str(random.randint(10000, 99999)),
                span_id=str(random.randint(10000, 99999))
            )
            key = f"log:{log.service}:{log.timestamp.timestamp()}:{i}"
            storage.store(DataType.LOG, key, log.to_storage_format(), log.timestamp)

        time.sleep(1)
        print("\n" + "="*60)
        print("å­˜å‚¨ç³»ç»Ÿç»Ÿè®¡:")
        stats = storage.get_storage_stats()
        for tier, tier_stats in stats["backends"].items():
            print(f"  [{tier.upper()}]")
            print(f"    é”®æ•°é‡: {tier_stats['total_keys']}")
            print(f"    å­˜å‚¨å¤§å°: {tier_stats['size_mb']:.2f} MB")

        print(f"\næ€»é”®æ•°é‡: {stats['total_keys']}")
        print(f"æ€»å­˜å‚¨å¤§å°: {stats['total_size_mb']:.2f} MB")

        print("\n" + "="*60)
        print("æŸ¥è¯¢æ¼”ç¤º:")

        result = query_engine.query_metrics(
            "cpu_usage",
            now - timedelta(hours=1),
            now,
            labels={"datacenter": "dc1"}
        )
        print(f"\næŒ‡æ ‡æŸ¥è¯¢ç»“æœ:")
        print(f"  æ•°æ®ç‚¹æ•°: {result['points_count']}")
        print(f"  å¹³å‡å€¼: {result['statistics']['avg']:.2f}")
        print(f"  æœ€å¤§å€¼: {result['statistics']['max']:.2f}")

        logs = query_engine.query_logs(
            service="payment-service",
            level="ERROR",
            start=now - timedelta(hours=1),
            limit=5
        )
        print(f"\næ—¥å¿—æŸ¥è¯¢ç»“æœ (ERRORçº§åˆ«): {len(logs)} æ¡")
        for log in logs[:3]:
            print(f"  [{log['level']}] {log['message'][:50]}...")

    finally:
        storage.stop_lifecycle_manager()
        print("\n" + "="*60)
        print("å­˜å‚¨ç³»ç»Ÿå·²åœæ­¢")


if __name__ == '__main__':
    import random
    demo_storage_system()
```

### 4.5 æ•ˆæœè¯„ä¼°

**æ€§èƒ½æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | æ”¹è¿›å‰ | æ”¹è¿›å | æå‡å¹…åº¦ |
|------|--------|--------|----------|
| æœˆåº¦å­˜å‚¨æˆæœ¬ | 200ä¸‡å…ƒ | 75ä¸‡å…ƒ | -62.5% |
| å¹³å‡æŸ¥è¯¢å»¶è¿Ÿ | 12ç§’ | 0.8ç§’ | -93% |
| P99æŸ¥è¯¢å»¶è¿Ÿ | 45ç§’ | 2.5ç§’ | -94% |
| æ•°æ®å‹ç¼©ç‡ | - | 75% | - |
| æ•°æ®å¯ç”¨æ€§ | 99.5% | 99.99% | +0.49% |
| æ•°æ®æ¢å¤æ—¶é—´ | 24å°æ—¶ | 30åˆ†é’Ÿ | -98% |

**ä¸šåŠ¡ä»·å€¼**ï¼š

1. **ç›´æ¥æˆæœ¬èŠ‚çœ**ï¼š
   - å¹´åº¦å­˜å‚¨æˆæœ¬èŠ‚çœ1500ä¸‡å…ƒï¼ˆä»2400ä¸‡é™è‡³900ä¸‡ï¼‰
   - æŸ¥è¯¢èµ„æºæ¶ˆè€—é™ä½70%ï¼ŒèŠ‚çœè®¡ç®—æˆæœ¬çº¦200ä¸‡å…ƒ/å¹´
   - è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†èŠ‚çœè¿ç»´äººåŠ›æˆæœ¬150ä¸‡å…ƒ/å¹´

2. **æ•ˆç‡æå‡**ï¼š
   - æ•…éšœåˆ†ææŸ¥è¯¢å“åº”æ—¶é—´ä»åˆ†é’Ÿçº§é™è‡³ç§’çº§
   - æ”¯æŒå¹¶å‘æŸ¥è¯¢ç”¨æˆ·æ•°ä»50æå‡è‡³500
   - æ•°æ®æ£€ç´¢æˆåŠŸç‡ä»85%æå‡è‡³99.9%

3. **åˆè§„ä¿éšœ**ï¼š
   - æ»¡è¶³é‡‘èç›‘ç®¡5å¹´æ•°æ®ç•™å­˜è¦æ±‚
   - å®¡è®¡æ•°æ®æŸ¥è¯¢æ—¶é—´ä»æ•°å°æ—¶ç¼©çŸ­è‡³åˆ†é’Ÿçº§
   - æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡ç‡100%

4. **æ¶æ„ä¼˜åŠ¿**ï¼š
   - å†·çƒ­æ•°æ®è‡ªåŠ¨åˆ†å±‚ï¼Œæ— éœ€äººå·¥å¹²é¢„
   - å­˜å‚¨å®¹é‡å¯çº¿æ€§æ‰©å±•è‡³PBçº§
   - æ”¯æŒå¤šç§Ÿæˆ·æ•°æ®éš”ç¦»

**ç»éªŒæ•™è®­**ï¼š

1. **å‹ç¼©ç­–ç•¥é€‰æ‹©**ï¼šä¸åŒç±»å‹æ•°æ®é‡‡ç”¨ä¸åŒå‹ç¼©ç®—æ³•ï¼Œæ—¶åºæ•°æ®é€‚åˆZSTDï¼Œæ—¥å¿—é€‚åˆGZIP
2. **ç´¢å¼•è®¾è®¡å…³é”®**ï¼šåˆç†çš„ç´¢å¼•å¯ä»¥å°†æŸ¥è¯¢æ€§èƒ½æå‡100å€ï¼Œä½†ç´¢å¼•æœ¬èº«ä¹Ÿæœ‰å­˜å‚¨æˆæœ¬
3. **é¢„èšåˆç­–ç•¥**ï¼šé«˜é¢‘æŸ¥è¯¢çš„èšåˆç»“æœå¯ä»¥é¢„å…ˆè®¡ç®—å¹¶ç¼“å­˜ï¼Œå¤§å¹…é™ä½å®æ—¶è®¡ç®—å‹åŠ›
4. **æ•°æ®é‡‡æ ·**ï¼šå†å²æ•°æ®å¯ä»¥é‡‡ç”¨é™é‡‡æ ·ç­–ç•¥ï¼Œ30å¤©å‰çš„æ•°æ®ä»ç§’çº§é™è‡³åˆ†é’Ÿçº§ç²’åº¦
5. **å¤šäº‘å¤‡ä»½**ï¼šå…³é”®æ•°æ®é‡‡ç”¨å¤šäº‘å¤‡ä»½ç­–ç•¥ï¼Œé¿å…å•äº‘æ•…éšœå¯¼è‡´æ•°æ®ä¸¢å¤±


---

## 5. æ¡ˆä¾‹4ï¼šæ™ºèƒ½å‘Šè­¦ä¸æ ¹å› åˆ†æç³»ç»Ÿ

### 5.1 ä¸šåŠ¡èƒŒæ™¯

**ä¼ä¸šèƒŒæ™¯**ï¼š
æŸäº‘æœåŠ¡æä¾›å•†ï¼ˆä»¥ä¸‹ç®€ç§°"Då…¬å¸"ï¼‰ï¼Œä¸ºè¶…è¿‡10ä¸‡å®¶ä¼ä¸šæä¾›äº‘è®¡ç®—æœåŠ¡ï¼Œç®¡ç†ç€æ•°åä¸‡è™šæ‹Ÿæœºå’Œæ•°ç™¾ä¸‡å®¹å™¨ã€‚è¿ç»´å›¢é˜Ÿæ¯å¤©æ¥æ”¶è¶…è¿‡5000æ¡å‘Šè­¦ï¼Œå‘Šè­¦é£æš´é¢‘å‘ï¼Œè¿ç»´äººå‘˜ç–²äºåº”å¯¹ã€‚

**ä¸šåŠ¡ç—›ç‚¹**ï¼š

1. **å‘Šè­¦é£æš´**ï¼šæ•…éšœæ—¶çŸ­æ—¶é—´å†…äº§ç”Ÿæ•°ç™¾æ¡ç›¸å…³å‘Šè­¦ï¼Œæ·¹æ²¡çœŸæ­£é‡è¦çš„ä¿¡æ¯
2. **è¯¯æŠ¥ç‡é«˜**ï¼š70%çš„å‘Šè­¦æ— éœ€äººå·¥å¤„ç†ï¼Œæµªè´¹è¿ç»´èµ„æº
3. **æ ¹å› å®šä½æ…¢**ï¼šå¹³å‡éœ€è¦45åˆ†é’Ÿæ‰èƒ½ç¡®å®šæ•…éšœæ ¹å› 
4. **å‘Šè­¦å…³è”å›°éš¾**ï¼šè·¨ç³»ç»Ÿã€è·¨æœåŠ¡çš„å‘Šè­¦ç¼ºä¹è‡ªåŠ¨å…³è”èƒ½åŠ›
5. **ç¼ºä¹é¢„æµ‹èƒ½åŠ›**ï¼šåªèƒ½è¢«åŠ¨å“åº”æ•…éšœï¼Œæ— æ³•æå‰é¢„è­¦

**ä¸šåŠ¡ç›®æ ‡**ï¼š

- å°†å‘Šè­¦æ•°é‡å‡å°‘80%ï¼ˆé€šè¿‡èšåˆå’Œé™å™ªï¼‰
- å‘Šè­¦å‡†ç¡®ç‡æå‡è‡³90%ä»¥ä¸Š
- æ ¹å› å®šä½æ—¶é—´ç¼©çŸ­è‡³10åˆ†é’Ÿä»¥å†…
- å®ç°æ™ºèƒ½å‘Šè­¦æŠ‘åˆ¶å’Œèšåˆ
- å…·å¤‡æ•…éšœé¢„æµ‹èƒ½åŠ›ï¼Œæå‰30åˆ†é’Ÿé¢„è­¦

### 5.2 æŠ€æœ¯æŒ‘æˆ˜

1. **å‘Šè­¦å®æ—¶å¤„ç†**ï¼šæ¯ç§’æ•°åƒæ¡å‘Šè­¦æµå…¥ï¼Œéœ€è¦æ¯«ç§’çº§å¤„ç†å»¶è¿Ÿ
2. **åŠ¨æ€é˜ˆå€¼è®¾å®š**ï¼šé™æ€é˜ˆå€¼éš¾ä»¥é€‚åº”ä¸šåŠ¡æ³¢åŠ¨ï¼Œéœ€è¦è‡ªé€‚åº”é˜ˆå€¼
3. **å› æœå…³ç³»å»ºæ¨¡**ï¼šå¦‚ä½•æ„å»ºæœåŠ¡ä¾èµ–å›¾è°±ï¼Œè¯†åˆ«å‘Šè­¦ä¼ æ’­è·¯å¾„
4. **å™ªå£°è¿‡æ»¤**ï¼šåŒºåˆ†çœŸå®å‘Šè­¦å’Œå™ªå£°ï¼ˆå¦‚è®¡åˆ’å†…ç»´æŠ¤ã€æµ‹è¯•å‘Šè­¦ï¼‰
5. **é¢„æµ‹æ¨¡å‹å‡†ç¡®æ€§**ï¼šå¦‚ä½•åœ¨å¤§è§„æ¨¡ç¯å¢ƒä¸‹ä¿æŒé¢„æµ‹å‡†ç¡®ç‡

### 5.3 è§£å†³æ–¹æ¡ˆ

**æ¶æ„è®¾è®¡**ï¼š

- é‡‡ç”¨æµå¼å¤„ç†æ¶æ„å®æ—¶å¤„ç†å‘Šè­¦æµ
- æ„å»ºæœåŠ¡ä¾èµ–å›¾è°±ï¼ŒåŸºäºå›¾ç®—æ³•è¿›è¡Œå‘Šè­¦å…³è”
- ä½¿ç”¨LSTMç¥ç»ç½‘ç»œè¿›è¡Œæ—¶åºå¼‚å¸¸æ£€æµ‹
- å®ç°åŠ¨æ€é˜ˆå€¼ç®—æ³•ï¼ˆåŸºäºå†å²æ•°æ®è‡ªé€‚åº”è°ƒæ•´ï¼‰
- é›†æˆçŸ¥è¯†å›¾è°±ï¼Œæ”¯æŒæ ¹å› æ¨ç†

### 5.4 å®Œæ•´ä»£ç å®ç°

**æ™ºèƒ½å‘Šè­¦ä¸æ ¹å› åˆ†æç³»ç»Ÿå®Œæ•´å®ç°ï¼ˆçº¦520è¡Œï¼‰**ï¼š

```python
#!/usr/bin/env python3
"""
æ™ºèƒ½å‘Šè­¦ä¸æ ¹å› åˆ†æç³»ç»Ÿ
åŠŸèƒ½ï¼šå‘Šè­¦èšåˆã€é™å™ªã€æ ¹å› åˆ†æã€è¶‹åŠ¿é¢„æµ‹
"""

import json
import time
import uuid
import heapq
import random
import logging
from typing import Dict, List, Optional, Set, Tuple, Any
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import defaultdict, deque
from enum import Enum
import threading
from threading import Lock

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class AlertSeverity(str, Enum):
    """å‘Šè­¦ä¸¥é‡çº§åˆ«"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class AlertStatus(str, Enum):
    """å‘Šè­¦çŠ¶æ€"""
    FIRING = "firing"
    ACKNOWLEDGED = "acknowledged"
    RESOLVED = "resolved"
    SUPPRESSED = "suppressed"


@dataclass
class Alert:
    """å‘Šè­¦æ•°æ®æ¨¡å‹"""
    alert_id: str
    name: str
    severity: AlertSeverity
    status: AlertStatus
    service: str
    instance: str
    message: str
    value: float
    threshold: float
    starts_at: datetime
    ends_at: Optional[datetime] = None
    labels: Dict[str, str] = field(default_factory=dict)
    annotations: Dict[str, str] = field(default_factory=dict)
    correlation_id: Optional[str] = None
    root_cause_score: float = 0.0

    def duration_seconds(self) -> int:
        """å‘Šè­¦æŒç»­æ—¶é—´"""
        end = self.ends_at or datetime.now()
        return int((end - self.starts_at).total_seconds())


@dataclass
class AlertGroup:
    """å‘Šè­¦ç»„ - èšåˆç›¸å…³å‘Šè­¦"""
    group_id: str
    name: str
    alerts: List[Alert] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    common_labels: Dict[str, str] = field(default_factory=dict)
    severity: AlertSeverity = AlertSeverity.MEDIUM
    root_cause_alert: Optional[Alert] = None

    def add_alert(self, alert: Alert):
        """æ·»åŠ å‘Šè­¦åˆ°ç»„"""
        self.alerts.append(alert)
        self.updated_at = datetime.now()
        alert.correlation_id = self.group_id

        severity_order = [AlertSeverity.INFO, AlertSeverity.LOW,
                         AlertSeverity.MEDIUM, AlertSeverity.HIGH, AlertSeverity.CRITICAL]
        if severity_order.index(alert.severity) > severity_order.index(self.severity):
            self.severity = alert.severity


class ServiceDependencyGraph:
    """æœåŠ¡ä¾èµ–å›¾è°±"""

    def __init__(self):
        self._nodes: Set[str] = set()
        self._edges: Dict[str, Set[str]] = defaultdict(set)
        self._reverse_edges: Dict[str, Set[str]] = defaultdict(set)
        self._edge_weights: Dict[Tuple[str, str], float] = {}
        self._lock = Lock()

    def add_service(self, service_name: str):
        """æ·»åŠ æœåŠ¡èŠ‚ç‚¹"""
        with self._lock:
            self._nodes.add(service_name)

    def add_dependency(self, from_service: str, to_service: str, weight: float = 1.0):
        """æ·»åŠ ä¾èµ–å…³ç³»"""
        with self._lock:
            self._nodes.add(from_service)
            self._nodes.add(to_service)
            self._edges[from_service].add(to_service)
            self._reverse_edges[to_service].add(from_service)
            self._edge_weights[(from_service, to_service)] = weight

    def get_upstream(self, service: str) -> List[Tuple[str, float]]:
        """è·å–ä¸Šæ¸¸ä¾èµ–ï¼ˆè¢«è¯¥æœåŠ¡ä¾èµ–çš„æœåŠ¡ï¼‰"""
        with self._lock:
            return [(s, self._edge_weights.get((service, s), 1.0))
                   for s in self._edges.get(service, [])]

    def get_downstream(self, service: str) -> List[Tuple[str, float]]:
        """è·å–ä¸‹æ¸¸ä¾èµ–ï¼ˆä¾èµ–è¯¥æœåŠ¡çš„æœåŠ¡ï¼‰"""
        with self._lock:
            return [(s, self._edge_weights.get((s, service), 1.0))
                   for s in self._reverse_edges.get(service, [])]

    def find_impact_radius(self, service: str, max_depth: int = 3) -> Dict[str, float]:
        """æŸ¥æ‰¾å½±å“åŠå¾„å†…çš„æ‰€æœ‰æœåŠ¡åŠå…¶å½±å“æƒé‡"""
        impacted = {service: 1.0}
        queue = deque([(service, 1.0, 0)])
        visited = {service}

        while queue:
            current, weight, depth = queue.popleft()
            if depth >= max_depth:
                continue

            for downstream, edge_weight in self.get_downstream(current):
                if downstream not in visited:
                    visited.add(downstream)
                    new_weight = weight * edge_weight * 0.8
                    impacted[downstream] = new_weight
                    queue.append((downstream, new_weight, depth + 1))

        return impacted

    def calculate_root_cause_score(self, service: str, all_alerts: List[Alert]) -> float:
        """è®¡ç®—æœåŠ¡çš„æ ¹å› å¯èƒ½æ€§è¯„åˆ†"""
        score = 0.0

        upstream = self.get_upstream(service)
        upstream_alerts = sum(1 for a in all_alerts if a.service in [s for s, _ in upstream])
        score += upstream_alerts * 0.3

        downstream = self.get_downstream(service)
        score += len(downstream) * 0.2

        service_alert = next((a for a in all_alerts if a.service == service), None)
        if service_alert:
            earliest = min(a.starts_at for a in all_alerts)
            if service_alert.starts_at == earliest:
                score += 0.5

        return min(score, 1.0)


class AlertAggregator:
    """å‘Šè­¦èšåˆå™¨"""

    def __init__(self, dependency_graph: ServiceDependencyGraph):
        self.graph = dependency_graph
        self._groups: Dict[str, AlertGroup] = {}
        self._alert_to_group: Dict[str, str] = {}
        self._lock = Lock()
        self._group_window = 300
        self._similarity_threshold = 0.8

    def process_alert(self, alert: Alert) -> Optional[AlertGroup]:
        """å¤„ç†æ–°å‘Šè­¦ï¼Œè¿”å›èšåˆåçš„ç»„"""
        with self._lock:
            for group in self._groups.values():
                if self._should_merge(alert, group):
                    group.add_alert(alert)
                    self._alert_to_group[alert.alert_id] = group.group_id
                    return group

            group_id = str(uuid.uuid4())[:8]
            group = AlertGroup(
                group_id=group_id,
                name=self._generate_group_name(alert),
                common_labels=self._extract_common_labels(alert),
                severity=alert.severity
            )
            group.add_alert(alert)
            self._groups[group_id] = group
            self._alert_to_group[alert.alert_id] = group_id
            return group

    def _should_merge(self, alert: Alert, group: AlertGroup) -> bool:
        """åˆ¤æ–­å‘Šè­¦æ˜¯å¦åº”è¯¥åˆå¹¶åˆ°ç»„"""
        if (alert.starts_at - group.updated_at).seconds > self._group_window:
            return False

        for existing_alert in group.alerts:
            if alert.service == existing_alert.service:
                return True
            if alert.service in [s for s, _ in self.graph.get_upstream(existing_alert.service)]:
                return True
            if existing_alert.service in [s for s, _ in self.graph.get_upstream(alert.service)]:
                return True

        similarity = self._calculate_similarity(alert.labels, group.common_labels)
        return similarity >= self._similarity_threshold

    def _calculate_similarity(self, labels1: Dict, labels2: Dict) -> float:
        """è®¡ç®—æ ‡ç­¾ç›¸ä¼¼åº¦"""
        if not labels1 or not labels2:
            return 0.0
        common_keys = set(labels1.keys()) & set(labels2.keys())
        if not common_keys:
            return 0.0
        matching = sum(1 for k in common_keys if labels1.get(k) == labels2.get(k))
        return matching / len(common_keys)

    def _generate_group_name(self, alert: Alert) -> str:
        """ç”Ÿæˆç»„å"""
        return f"{alert.service}_{alert.name}_{alert.starts_at.strftime('%H%M')}"

    def _extract_common_labels(self, alert: Alert) -> Dict:
        """æå–å…¬å…±æ ‡ç­¾"""
        return {k: v for k, v in alert.labels.items()
                if k in ['datacenter', 'cluster', 'namespace', 'app']}

    def analyze_root_cause(self, group_id: str) -> Optional[Alert]:
        """åˆ†æå‘Šè­¦ç»„çš„æ ¹å› """
        with self._lock:
            group = self._groups.get(group_id)
            if not group or len(group.alerts) < 2:
                return None

            max_score = 0.0
            root_cause = None

            for alert in group.alerts:
                score = self.graph.calculate_root_cause_score(alert.service, group.alerts)
                alert.root_cause_score = score
                if score > max_score:
                    max_score = score
                    root_cause = alert

            group.root_cause_alert = root_cause
            return root_cause

    def get_active_groups(self) -> List[AlertGroup]:
        """è·å–æ´»è·ƒå‘Šè­¦ç»„"""
        with self._lock:
            cutoff = datetime.now() - timedelta(seconds=self._group_window * 2)
            return [g for g in self._groups.values() if g.updated_at > cutoff]

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            return {
                "total_groups": len(self._groups),
                "active_groups": len(self.get_active_groups()),
                "total_alerts": len(self._alert_to_group),
                "avg_group_size": sum(len(g.alerts) for g in self._groups.values()) / len(self._groups) if self._groups else 0
            }


class AlertSuppressor:
    """å‘Šè­¦æŠ‘åˆ¶å™¨"""

    def __init__(self):
        self._suppression_rules: List[Dict] = []
        self._maintenance_windows: List[Dict] = []
        self._recent_alerts: deque = deque(maxlen=1000)
        self._lock = Lock()

    def add_suppression_rule(self, name: str, condition: Dict, duration: int):
        """æ·»åŠ æŠ‘åˆ¶è§„åˆ™"""
        rule = {
            "name": name,
            "condition": condition,
            "duration": duration,
            "created_at": datetime.now()
        }
        with self._lock:
            self._suppression_rules.append(rule)
        logger.info(f"Suppression rule added: {name}")

    def add_maintenance_window(self, service: str, start: datetime, end: datetime, reason: str):
        """æ·»åŠ ç»´æŠ¤çª—å£"""
        window = {
            "service": service,
            "start": start,
            "end": end,
            "reason": reason
        }
        with self._lock:
            self._maintenance_windows.append(window)

    def should_suppress(self, alert: Alert) -> Tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æŠ‘åˆ¶å‘Šè­¦"""
        with self._lock:
            now = datetime.now()
            for window in self._maintenance_windows:
                if (alert.service == window["service"] and
                    window["start"] <= now <= window["end"]):
                    return True, f"Maintenance window: {window['reason']}"

            for rule in self._suppression_rules:
                match = all(alert.labels.get(k) == v or alert.service == v
                           for k, v in rule["condition"].items())
                if match:
                    age = (now - rule["created_at"]).total_seconds()
                    if age < rule["duration"]:
                        return True, f"Suppression rule: {rule['name']}"

            for recent in self._recent_alerts:
                if (recent["name"] == alert.name and
                    recent["service"] == alert.service and
                    (now - recent["time"]).seconds < 300):
                    return True, "Duplicate alert (deduplication)"

            self._recent_alerts.append({
                "name": alert.name,
                "service": alert.service,
                "time": now
            })

        return False, ""


class AlertPredictor:
    """å‘Šè­¦é¢„æµ‹å™¨ - åŸºäºæ—¶åºæ•°æ®é¢„æµ‹æ½œåœ¨æ•…éšœ"""

    def __init__(self):
        self._metric_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1440))
        self._prediction_models: Dict[str, Any] = {}
        self._lock = Lock()

    def feed_metric(self, metric_name: str, service: str, value: float, timestamp: datetime):
        """å–‚å…¥æ—¶åºæ•°æ®"""
        key = f"{service}:{metric_name}"
        with self._lock:
            self._metric_history[key].append({
                "timestamp": timestamp,
                "value": value
            })

    def predict(self, metric_name: str, service: str, threshold: float) -> Optional[Dict]:
        """é¢„æµ‹æœªæ¥è¶‹åŠ¿"""
        key = f"{service}:{metric_name}"
        with self._lock:
            history = list(self._metric_history[key])

        if len(history) < 60:
            return None

        values = [h["value"] for h in history[-60:]]
        n = len(values)

        x = list(range(n))
        x_mean = sum(x) / n
        y_mean = sum(values) / n

        numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
        denominator = sum((xi - x_mean) ** 2 for xi in x)

        if denominator == 0:
            slope = 0
        else:
            slope = numerator / denominator

        intercept = y_mean - slope * x_mean

        future_values = [slope * (n + i) + intercept for i in range(1, 6)]

        will_breach = any(v > threshold for v in future_values)
        time_to_breach = None

        if will_breach and slope > 0:
            for i, v in enumerate(future_values):
                if v > threshold:
                    time_to_breach = (i + 1) * 60
                    break

        return {
            "service": service,
            "metric": metric_name,
            "current_value": values[-1],
            "predicted_value": future_values[-1],
            "trend": "increasing" if slope > 0.01 else "decreasing" if slope < -0.01 else "stable",
            "will_breach_threshold": will_breach,
            "time_to_breach_seconds": time_to_breach,
            "confidence": min(len(history) / 1440, 0.95)
        }


class IntelligentAlertManager:
    """æ™ºèƒ½å‘Šè­¦ç®¡ç†å™¨ä¸»ç±»"""

    def __init__(self):
        self.graph = ServiceDependencyGraph()
        self.aggregator = AlertAggregator(self.graph)
        self.suppressor = AlertSuppressor()
        self.predictor = AlertPredictor()

        self._alerts: Dict[str, Alert] = {}
        self._notification_handlers: List[Callable] = []
        self._running = False
        self._lock = Lock()

    def register_service(self, name: str, upstream_dependencies: List[str] = None):
        """æ³¨å†ŒæœåŠ¡"""
        self.graph.add_service(name)
        if upstream_dependencies:
            for dep in upstream_dependencies:
                self.graph.add_dependency(name, dep)
        logger.info(f"Service registered: {name}")

    def add_notification_handler(self, handler: Callable):
        """æ·»åŠ é€šçŸ¥å¤„ç†å™¨"""
        self._notification_handlers.append(handler)

    def process_alert(self, name: str, service: str, severity: AlertSeverity,
                     value: float, threshold: float, message: str,
                     labels: Dict = None) -> Optional[AlertGroup]:
        """å¤„ç†å‘Šè­¦"""
        alert = Alert(
            alert_id=str(uuid.uuid4())[:12],
            name=name,
            severity=severity,
            status=AlertStatus.FIRING,
            service=service,
            instance=labels.get("instance", "unknown") if labels else "unknown",
            message=message,
            value=value,
            threshold=threshold,
            starts_at=datetime.now(),
            labels=labels or {}
        )

        should_suppress, reason = self.suppressor.should_suppress(alert)
        if should_suppress:
            alert.status = AlertStatus.SUPPRESSED
            logger.info(f"Alert suppressed: {alert.alert_id}, reason: {reason}")
            return None

        with self._lock:
            self._alerts[alert.alert_id] = alert

        group = self.aggregator.process_alert(alert)

        if len(group.alerts) >= 3 and not group.root_cause_alert:
            root_cause = self.aggregator.analyze_root_cause(group.group_id)
            if root_cause:
                logger.info(f"Root cause identified: {root_cause.service} - {root_cause.message}")

        self._notify(alert, group)

        return group

    def _notify(self, alert: Alert, group: AlertGroup):
        """å‘é€é€šçŸ¥"""
        for handler in self._notification_handlers:
            try:
                handler(alert, group)
            except Exception as e:
                logger.error(f"Notification handler failed: {e}")

    def resolve_alert(self, alert_id: str):
        """è§£å†³å‘Šè­¦"""
        with self._lock:
            if alert_id in self._alerts:
                self._alerts[alert_id].status = AlertStatus.RESOLVED
                self._alerts[alert_id].ends_at = datetime.now()

    def get_dashboard_summary(self) -> Dict:
        """è·å–ä»ªè¡¨ç›˜æ‘˜è¦"""
        with self._lock:
            firing = [a for a in self._alerts.values() if a.status == AlertStatus.FIRING]
            critical = [a for a in firing if a.severity == AlertSeverity.CRITICAL]

        groups = self.aggregator.get_active_groups()
        with_root_cause = sum(1 for g in groups if g.root_cause_alert)

        return {
            "total_alerts": len(self._alerts),
            "firing_alerts": len(firing),
            "critical_alerts": len(critical),
            "active_groups": len(groups),
            "groups_with_root_cause": with_root_cause,
            "suppression_rate": self._calculate_suppression_rate(),
            "aggregator_stats": self.aggregator.get_stats()
        }

    def _calculate_suppression_rate(self) -> float:
        """è®¡ç®—æŠ‘åˆ¶ç‡"""
        with self._lock:
            total = len(self._alerts)
            suppressed = len([a for a in self._alerts.values() if a.status == AlertStatus.SUPPRESSED])
        return suppressed / total if total > 0 else 0.0


# ============ æ¼”ç¤º ============

def demo_alert_system():
    """æ¼”ç¤ºæ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ"""
    manager = IntelligentAlertManager()

    manager.register_service("api-gateway", ["order-service"])
    manager.register_service("order-service", ["payment-service", "inventory-service"])
    manager.register_service("payment-service", ["database"])
    manager.register_service("inventory-service", ["database"])
    manager.register_service("database", [])

    def print_notification(alert: Alert, group: AlertGroup):
        if group.root_cause_alert == alert:
            print(f"  [ROOT CAUSE] {alert.service}: {alert.message}")
        else:
            print(f"  {alert.service}: {alert.message}")

    manager.add_notification_handler(print_notification)

    manager.suppressor.add_suppression_rule(
        "test-environment",
        {"environment": "test"},
        3600
    )

    manager.suppressor.add_maintenance_window(
        "payment-service",
        datetime.now() - timedelta(minutes=30),
        datetime.now() + timedelta(minutes=30),
        "Scheduled maintenance"
    )

    print("="*60)
    print("æ¨¡æ‹Ÿæ•…éšœåœºæ™¯ï¼šæ•°æ®åº“æ•…éšœå¼•å‘è¿é”ååº”")
    print("="*60)

    time.sleep(0.1)
    manager.process_alert(
        name="DatabaseConnectionError",
        service="database",
        severity=AlertSeverity.CRITICAL,
        value=100,
        threshold=10,
        message="Database connection pool exhausted",
        labels={"datacenter": "dc1", "instance": "db-01"}
    )

    time.sleep(0.5)
    manager.process_alert(
        name="PaymentTimeout",
        service="payment-service",
        severity=AlertSeverity.CRITICAL,
        value=30,
        threshold=5,
        message="Payment processing timeout",
        labels={"datacenter": "dc1", "instance": "payment-01"}
    )

    time.sleep(0.3)
    manager.process_alert(
        name="InventoryQueryFailed",
        service="inventory-service",
        severity=AlertSeverity.HIGH,
        value=50,
        threshold=10,
        message="Inventory query failed",
        labels={"datacenter": "dc1", "instance": "inventory-01"}
    )

    time.sleep(0.2)
    manager.process_alert(
        name="OrderProcessingError",
        service="order-service",
        severity=AlertSeverity.HIGH,
        value=25,
        threshold=5,
        message="Order processing error rate high",
        labels={"datacenter": "dc1", "instance": "order-01"}
    )

    time.sleep(0.1)
    manager.process_alert(
        name="HighLatency",
        service="api-gateway",
        severity=AlertSeverity.MEDIUM,
        value=2000,
        threshold=500,
        message="API response latency high",
        labels={"datacenter": "dc1", "instance": "gateway-01"}
    )

    manager.process_alert(
        name="TestAlert",
        service="test-service",
        severity=AlertSeverity.CRITICAL,
        value=100,
        threshold=10,
        message="Test alert",
        labels={"environment": "test"}
    )

    print("\n" + "="*60)
    print("å‘Šè­¦å¤„ç†ç»“æœ:")
    summary = manager.get_dashboard_summary()
    print(f"  æ€»å‘Šè­¦æ•°: {summary['total_alerts']}")
    print(f"  æ´»è·ƒå‘Šè­¦: {summary['firing_alerts']}")
    print(f"  ä¸¥é‡å‘Šè­¦: {summary['critical_alerts']}")
    print(f"  æ´»è·ƒå‘Šè­¦ç»„: {summary['active_groups']}")
    print(f"  å·²è¯†åˆ«æ ¹å› : {summary['groups_with_root_cause']}")
    print(f"  å‘Šè­¦æŠ‘åˆ¶ç‡: {summary['suppression_rate']*100:.1f}%")

    print("\nå‘Šè­¦ç»„è¯¦æƒ…:")
    for group in manager.aggregator.get_active_groups():
        print(f"  ç»„ {group.group_id}:")
        print(f"    å‘Šè­¦æ•°é‡: {len(group.alerts)}")
        print(f"    ä¸¥é‡çº§åˆ«: {group.severity.value}")
        if group.root_cause_alert:
            print(f"    æ ¹å› : {group.root_cause_alert.service} - {group.root_cause_alert.message}")
        print(f"    å½±å“æœåŠ¡: {', '.join(set(a.service for a in group.alerts))}")


if __name__ == '__main__':
    demo_alert_system()
```

### 5.5 æ•ˆæœè¯„ä¼°

**æ€§èƒ½æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | æ”¹è¿›å‰ | æ”¹è¿›å | æå‡å¹…åº¦ |
|------|--------|--------|----------|
| æ—¥å‡å‘Šè­¦æ•°é‡ | 5000æ¡ | 800æ¡ | -84% |
| å‘Šè­¦å‡†ç¡®ç‡ | 30% | 92% | +62% |
| æ ¹å› å®šä½æ—¶é—´ | 45åˆ†é’Ÿ | 6åˆ†é’Ÿ | -87% |
| å‘Šè­¦å“åº”æ—¶é—´ | 15åˆ†é’Ÿ | 3åˆ†é’Ÿ | -80% |
| è¯¯æŠ¥ç‡ | 70% | 8% | -62% |
| é¢„æµ‹å‡†ç¡®ç‡ | - | 85% | - |

**ä¸šåŠ¡ä»·å€¼**ï¼š

1. **è¿ç»´æ•ˆç‡æå‡**ï¼š
   - è¿ç»´äººå‘˜æ¯æ—¥å¤„ç†å‘Šè­¦æ—¶é—´ä»4å°æ—¶é™è‡³30åˆ†é’Ÿ
   - å‘Šè­¦ç–²åŠ³æ˜¾è‘—é™ä½ï¼Œå…³é”®å‘Šè­¦ä¸è¢«æ·¹æ²¡
   - å€¼ç­äººå‘˜ä»æ¯ç­3äººå‡è‡³1äººï¼Œå¹´èŠ‚çœäººåŠ›æˆæœ¬600ä¸‡å…ƒ

2. **æ•…éšœå¤„ç†åŠ é€Ÿ**ï¼š
   - MTTRä»å¹³å‡45åˆ†é’Ÿé™è‡³8åˆ†é’Ÿ
   - æ ¹å› è‡ªåŠ¨è¯†åˆ«å‡†ç¡®ç‡è¾¾85%ï¼Œå¤§å¹…å‡å°‘æ’æŸ¥æ—¶é—´
   - æ•…éšœå½±å“èŒƒå›´è¯„ä¼°è‡ªåŠ¨åŒ–ï¼Œæ”¯æŒå¿«é€Ÿå†³ç­–

3. **ä¸»åŠ¨é¢„é˜²**ï¼š
   - 30%çš„æ½œåœ¨æ•…éšœè¢«æå‰é¢„æµ‹å¹¶å¤„ç†
   - é€šè¿‡è¶‹åŠ¿åˆ†æé¿å…å¤šæ¬¡å®¹é‡ä¸è¶³å¯¼è‡´çš„æ•…éšœ
   - é¢„æµ‹æ€§ç»´æŠ¤å‡å°‘ç´§æ€¥æ‰©å®¹æ¬¡æ•°60%

4. **SLAæå‡**ï¼š
   - ç³»ç»Ÿå¯ç”¨æ€§ä»99.9%æå‡è‡³99.99%
   - å®¢æˆ·æ»¡æ„åº¦æå‡15ä¸ªç™¾åˆ†ç‚¹
   - å› æ•…éšœå¯¼è‡´çš„èµ”å¿å‡å°‘80%

**ç»éªŒæ•™è®­**ï¼š

1. **ä¾èµ–å›¾è°±ç»´æŠ¤**ï¼šæœåŠ¡ä¾èµ–å›¾è°±éœ€è¦è‡ªåŠ¨å‘ç°å’Œæ›´æ–°ï¼Œæ‰‹å·¥ç»´æŠ¤å®¹æ˜“è¿‡æ—¶
2. **é˜ˆå€¼åŠ¨æ€è°ƒæ•´**ï¼šé™æ€é˜ˆå€¼éš¾ä»¥é€‚åº”ä¸šåŠ¡å˜åŒ–ï¼ŒåŠ¨æ€é˜ˆå€¼éœ€è¦è¶³å¤Ÿå†å²æ•°æ®æ”¯æ’‘
3. **äººæœºååŒ**ï¼šè‡ªåŠ¨æ ¹å› åˆ†æä»…ä¾›å‚è€ƒï¼Œå…³é”®å†³ç­–ä»éœ€äººå·¥ç¡®è®¤
4. **æ¸è¿›å¼è°ƒä¼˜**ï¼šæŠ‘åˆ¶è§„åˆ™éœ€è¦é€æ­¥è°ƒæ•´ï¼Œè¿‡äºæ¿€è¿›å¯èƒ½å¯¼è‡´æ¼æŠ¥
5. **å¤šç»´åº¦å…³è”**ï¼šç»“åˆæŒ‡æ ‡ã€æ—¥å¿—ã€è¿½è¸ªå¤šç»´æ•°æ®è¿›è¡Œæ ¹å› åˆ†ææ•ˆæœæ›´ä½³


---

## 6. æ¡ˆä¾‹5ï¼šå¯è§‚æµ‹æ€§æ•°æ®å¯è§†åŒ–å¹³å°

### 6.1 ä¸šåŠ¡èƒŒæ™¯

**ä¼ä¸šèƒŒæ™¯**ï¼š
æŸè·¨å›½åˆ¶é€ ä¼ä¸šï¼ˆä»¥ä¸‹ç®€ç§°"Eå…¬å¸"ï¼‰ï¼Œåœ¨å…¨çƒæ‹¥æœ‰20ä¸ªå·¥å‚ã€500æ¡ç”Ÿäº§çº¿ï¼Œéƒ¨ç½²äº†è¶…è¿‡200ä¸‡ä¸ªä¼ æ„Ÿå™¨ã€‚éœ€è¦ç»Ÿä¸€çš„å¯è§†åŒ–å¹³å°ç›‘æ§ç”Ÿäº§çŠ¶æ€ã€è®¾å¤‡å¥åº·ã€è´¨é‡æ§åˆ¶ç­‰å…³é”®æŒ‡æ ‡ã€‚

**ä¸šåŠ¡ç—›ç‚¹**ï¼š

1. **æ•°æ®å­¤å²›**ï¼šå„å·¥å‚ä½¿ç”¨ä¸åŒçš„ç›‘æ§ç³»ç»Ÿï¼Œæ•°æ®æ— æ³•ç»Ÿä¸€æŸ¥çœ‹
2. **å¯è§†åŒ–èƒ½åŠ›ä¸è¶³**ï¼šç°æœ‰å·¥å…·ä»…æ”¯æŒç®€å•å›¾è¡¨ï¼Œæ— æ³•æ»¡è¶³å¤æ‚åˆ†æéœ€æ±‚
3. **å®æ—¶æ€§å·®**ï¼šç”Ÿäº§æ•°æ®å»¶è¿Ÿè¶…è¿‡5åˆ†é’Ÿï¼Œæ— æ³•å®ç°å®æ—¶ç”Ÿäº§è°ƒæ§
4. **ç§»åŠ¨ç«¯ç¼ºå¤±**ï¼šç®¡ç†äººå‘˜æ— æ³•é€šè¿‡æ‰‹æœºéšæ—¶æŸ¥çœ‹ç”Ÿäº§çŠ¶æ€
5. **å®šåˆ¶åŒ–å›°éš¾**ï¼šä¸åŒè§’è‰²ï¼ˆå‚é•¿ã€å·¥ç¨‹å¸ˆã€æ“ä½œå‘˜ï¼‰éœ€è¦ä¸åŒçš„è§†å›¾

**ä¸šåŠ¡ç›®æ ‡**ï¼š

- æ„å»ºç»Ÿä¸€çš„å…¨å±€å¯è§†åŒ–å¹³å°
- æ•°æ®å»¶è¿Ÿæ§åˆ¶åœ¨10ç§’ä»¥å†…
- æ”¯æŒåƒäººå¹¶å‘çš„å®æ—¶ä»ªè¡¨ç›˜
- æä¾›ç§»åŠ¨ç«¯åŸç”Ÿä½“éªŒ
- å®ç°æ‹–æ‹½å¼è‡ªå®šä¹‰ä»ªè¡¨ç›˜

### 6.2 æŠ€æœ¯æŒ‘æˆ˜

1. **æµ·é‡æ•°æ®å®æ—¶æ¸²æŸ“**ï¼š200ä¸‡ä¼ æ„Ÿå™¨ï¼Œæ¯ç§’æ•°ç™¾ä¸‡æ•°æ®ç‚¹éœ€è¦å®æ—¶å±•ç¤º
2. **è·¨åœ°åŸŸæ•°æ®èšåˆ**ï¼šå…¨çƒ20ä¸ªå·¥å‚æ•°æ®éœ€è¦ç»Ÿä¸€èšåˆå±•ç¤º
3. **å¤æ‚å›¾è¡¨æ”¯æŒ**ï¼šéœ€è¦æ”¯æŒçƒ­åŠ›å›¾ã€æ¡‘åŸºå›¾ã€æ‹“æ‰‘å›¾ç­‰é«˜çº§å¯è§†åŒ–
4. **ç§»åŠ¨ç«¯é€‚é…**ï¼šåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šæµç•…å±•ç¤ºå¤æ‚æ•°æ®
5. **æƒé™ä¸éš”ç¦»**ï¼šä¸åŒè§’è‰²çœ‹åˆ°ä¸åŒçš„æ•°æ®å’Œè§†å›¾

### 6.3 è§£å†³æ–¹æ¡ˆ

**æ¶æ„è®¾è®¡**ï¼š

- å‰ç«¯é‡‡ç”¨React + ECharts + WebGLå®ç°é«˜æ€§èƒ½å¯è§†åŒ–
- åç«¯ä½¿ç”¨WebSocketæ¨é€å®æ—¶æ•°æ®
- æ•°æ®é¢„èšåˆï¼šåŸå§‹æ•°æ®â†’ç§’çº§â†’åˆ†é’Ÿçº§â†’å°æ—¶çº§å¤šçº§èšåˆ
- CDNåŠ é€Ÿå…¨çƒè®¿é—®
- åŸºäºRBACçš„ç»†ç²’åº¦æƒé™æ§åˆ¶

### 6.4 å®Œæ•´ä»£ç å®ç°

**å¯è§‚æµ‹æ€§æ•°æ®å¯è§†åŒ–å¹³å°å®Œæ•´å®ç°ï¼ˆçº¦480è¡Œï¼‰**ï¼š

```python
#!/usr/bin/env python3
"""
å¯è§‚æµ‹æ€§æ•°æ®å¯è§†åŒ–å¹³å°
åŠŸèƒ½ï¼šå®æ—¶æ•°æ®æ¨é€ã€ä»ªè¡¨ç›˜ç®¡ç†ã€å›¾è¡¨æ¸²æŸ“ã€æƒé™æ§åˆ¶
"""

import json
import time
import random
import asyncio
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from enum import Enum
from collections import defaultdict
import threading


class ChartType(str, Enum):
    """å›¾è¡¨ç±»å‹"""
    LINE = "line"
    BAR = "bar"
    PIE = "pie"
    GAUGE = "gauge"
    HEATMAP = "heatmap"
    TOPOLOGY = "topology"
    SANKEY = "sankey"
    TABLE = "table"


class TimeRange(str, Enum):
    """æ—¶é—´èŒƒå›´"""
    LAST_5M = "5m"
    LAST_15M = "15m"
    LAST_1H = "1h"
    LAST_6H = "6h"
    LAST_24H = "24h"
    LAST_7D = "7d"
    CUSTOM = "custom"


@dataclass
class DataPoint:
    """æ•°æ®ç‚¹"""
    timestamp: datetime
    value: float
    labels: Dict[str, str] = field(default_factory=dict)

    def to_dict(self) -> Dict:
        return {
            "timestamp": int(self.timestamp.timestamp() * 1000),
            "value": self.value,
            "labels": self.labels
        }


@dataclass
class ChartConfig:
    """å›¾è¡¨é…ç½®"""
    chart_id: str
    title: str
    chart_type: ChartType
    data_source: str
    query: str
    time_range: TimeRange
    refresh_interval: int
    dimensions: List[str] = field(default_factory=list)
    metrics: List[str] = field(default_factory=list)
    filters: Dict[str, Any] = field(default_factory=dict)
    options: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Dashboard:
    """ä»ªè¡¨ç›˜"""
    dashboard_id: str
    name: str
    description: str
    owner: str
    charts: List[ChartConfig] = field(default_factory=list)
    layout: Dict[str, Any] = field(default_factory=dict)
    is_public: bool = False
    allowed_roles: List[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)


class DataSource:
    """æ•°æ®æºæŠ½è±¡"""

    def __init__(self, name: str, data_type: str):
        self.name = name
        self.data_type = data_type
        self._data_cache: Dict[str, List[DataPoint]] = defaultdict(list)
        self._lock = threading.RLock()

    def query(self, query: str, start: datetime, end: datetime,
              filters: Dict = None) -> List[DataPoint]:
        """æŸ¥è¯¢æ•°æ®"""
        cache_key = f"{query}:{start.isoformat()}:{end.isoformat()}"

        with self._lock:
            if cache_key in self._data_cache:
                return self._data_cache[cache_key]

        points = self._generate_mock_data(query, start, end, filters)

        with self._lock:
            self._data_cache[cache_key] = points

        return points

    def _generate_mock_data(self, query: str, start: datetime,
                           end: datetime, filters: Dict) -> List[DataPoint]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
        points = []
        current = start
        interval = timedelta(seconds=60)

        while current <= end:
            value = self._calculate_value(query, current)
            labels = filters or {}
            points.append(DataPoint(timestamp=current, value=value, labels=labels))
            current += interval

        return points

    def _calculate_value(self, query: str, timestamp: datetime) -> float:
        """è®¡ç®—æŒ‡æ ‡å€¼"""
        base = 50
        hour_factor = abs(12 - timestamp.hour) / 12
        noise = random.uniform(-10, 10)

        if "cpu" in query.lower():
            return min(100, max(0, base * hour_factor + 20 + noise))
        elif "memory" in query.lower():
            return min(100, max(0, base * 0.8 + 30 + noise))
        elif "temperature" in query.lower():
            return 25 + hour_factor * 15 + noise / 2
        else:
            return base + noise

    def get_realtime_value(self, query: str) -> DataPoint:
        """è·å–å®æ—¶å€¼"""
        return DataPoint(
            timestamp=datetime.now(),
            value=self._calculate_value(query, datetime.now()),
            labels={}
        )


class ChartRenderer:
    """å›¾è¡¨æ¸²æŸ“å™¨"""

    def __init__(self, data_source: DataSource):
        self.data_source = data_source

    def render(self, config: ChartConfig) -> Dict[str, Any]:
        """æ¸²æŸ“å›¾è¡¨"""
        start, end = self._parse_time_range(config.time_range)
        points = self.data_source.query(config.query, start, end, config.filters)

        if config.chart_type == ChartType.LINE:
            return self._render_line_chart(config, points)
        elif config.chart_type == ChartType.BAR:
            return self._render_bar_chart(config, points)
        elif config.chart_type == ChartType.PIE:
            return self._render_pie_chart(config, points)
        elif config.chart_type == ChartType.GAUGE:
            return self._render_gauge_chart(config, points)
        elif config.chart_type == ChartType.HEATMAP:
            return self._render_heatmap(config, points)
        elif config.chart_type == ChartType.TABLE:
            return self._render_table(config, points)
        else:
            return self._render_line_chart(config, points)

    def _parse_time_range(self, time_range: TimeRange) -> tuple:
        """è§£ææ—¶é—´èŒƒå›´"""
        end = datetime.now()
        deltas = {
            TimeRange.LAST_5M: timedelta(minutes=5),
            TimeRange.LAST_15M: timedelta(minutes=15),
            TimeRange.LAST_1H: timedelta(hours=1),
            TimeRange.LAST_6H: timedelta(hours=6),
            TimeRange.LAST_24H: timedelta(hours=24),
            TimeRange.LAST_7D: timedelta(days=7),
        }
        start = end - deltas.get(time_range, timedelta(hours=1))
        return start, end

    def _render_line_chart(self, config: ChartConfig, points: List[DataPoint]) -> Dict:
        """æ¸²æŸ“æŠ˜çº¿å›¾"""
        x_data = [p.timestamp.strftime("%H:%M") for p in points]
        y_data = [p.value for p in points]

        return {
            "type": "line",
            "title": {"text": config.title},
            "xAxis": {"type": "category", "data": x_data},
            "yAxis": {"type": "value"},
            "series": [{
                "name": config.query,
                "type": "line",
                "data": y_data,
                "smooth": True,
                "areaStyle": {"opacity": 0.3}
            }],
            "tooltip": {"trigger": "axis"}
        }

    def _render_bar_chart(self, config: ChartConfig, points: List[DataPoint]) -> Dict:
        """æ¸²æŸ“æŸ±çŠ¶å›¾"""
        hourly = defaultdict(list)
        for p in points:
            hour = p.timestamp.hour
            hourly[hour].append(p.value)

        x_data = [f"{h:02d}:00" for h in sorted(hourly.keys())]
        y_data = [sum(hourly[h]) / len(hourly[h]) for h in sorted(hourly.keys())]

        return {
            "type": "bar",
            "title": {"text": config.title},
            "xAxis": {"type": "category", "data": x_data},
            "yAxis": {"type": "value"},
            "series": [{"name": config.query, "type": "bar", "data": y_data}]
        }

    def _render_pie_chart(self, config: ChartConfig, points: List[DataPoint]) -> Dict:
        """æ¸²æŸ“é¥¼å›¾"""
        segments = {"Low": 0, "Medium": 0, "High": 0}
        for p in points:
            if p.value < 33:
                segments["Low"] += 1
            elif p.value < 66:
                segments["Medium"] += 1
            else:
                segments["High"] += 1

        data = [{"name": k, "value": v} for k, v in segments.items()]

        return {
            "type": "pie",
            "title": {"text": config.title},
            "series": [{
                "type": "pie",
                "radius": ["40%", "70%"],
                "data": data
            }]
        }

    def _render_gauge_chart(self, config: ChartConfig, points: List[DataPoint]) -> Dict:
        """æ¸²æŸ“ä»ªè¡¨ç›˜"""
        current_value = points[-1].value if points else 0

        return {
            "type": "gauge",
            "title": {"text": config.title},
            "series": [{
                "type": "gauge",
                "progress": {"show": True},
                "detail": {"valueAnimation": True, "formatter": "{value}%"},
                "data": [{"value": round(current_value, 1), "name": config.query}],
                "axisLine": {
                    "lineStyle": {
                        "color": [[0.3, "#67e8f9"], [0.7, "#facc15"], [1, "#f87171"]]
                    }
                }
            }]
        }

    def _render_heatmap(self, config: ChartConfig, points: List[DataPoint]) -> Dict:
        """æ¸²æŸ“çƒ­åŠ›å›¾"""
        data = []
        for day in range(7):
            for hour in range(24):
                value = random.uniform(0, 100)
                data.append([day, hour, round(value, 1)])

        days = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]

        return {
            "type": "heatmap",
            "title": {"text": config.title},
            "xAxis": {"type": "category", "data": list(range(24))},
            "yAxis": {"type": "category", "data": days},
            "visualMap": {"min": 0, "max": 100, "calculable": True},
            "series": [{"type": "heatmap", "data": data}]
        }

    def _render_table(self, config: ChartConfig, points: List[DataPoint]) -> Dict:
        """æ¸²æŸ“è¡¨æ ¼"""
        headers = ["Time", "Value", "Status"]
        rows = []
        for p in points[-10:]:
            status = "Normal" if p.value < 70 else "Warning" if p.value < 90 else "Critical"
            rows.append([p.timestamp.strftime("%Y-%m-%d %H:%M:%S"), round(p.value, 2), status])

        return {
            "type": "table",
            "title": {"text": config.title},
            "headers": headers,
            "rows": rows
        }


class RealtimeDataPusher:
    """å®æ—¶æ•°æ®æ¨é€å™¨"""

    def __init__(self, data_source: DataSource):
        self.data_source = data_source
        self._subscribers: Dict[str, List[Callable]] = defaultdict(list)
        self._running = False
        self._lock = threading.Lock()

    def subscribe(self, chart_id: str, callback: Callable):
        """è®¢é˜…å›¾è¡¨å®æ—¶æ•°æ®"""
        with self._lock:
            self._subscribers[chart_id].append(callback)

    def unsubscribe(self, chart_id: str, callback: Callable):
        """å–æ¶ˆè®¢é˜…"""
        with self._lock:
            if callback in self._subscribers[chart_id]:
                self._subscribers[chart_id].remove(callback)

    def start(self):
        """å¯åŠ¨æ¨é€æœåŠ¡"""
        self._running = True
        threading.Thread(target=self._push_loop, daemon=True).start()

    def stop(self):
        """åœæ­¢æ¨é€æœåŠ¡"""
        self._running = False

    def _push_loop(self):
        """æ¨é€å¾ªç¯"""
        while self._running:
            with self._lock:
                subscribers_copy = dict(self._subscribers)

            for chart_id, callbacks in subscribers_copy.items():
                point = self.data_source.get_realtime_value(f"metric_{chart_id}")
                data = {
                    "chart_id": chart_id,
                    "timestamp": int(point.timestamp.timestamp() * 1000),
                    "value": point.value
                }

                for callback in callbacks:
                    try:
                        callback(data)
                    except Exception as e:
                        print(f"Push error: {e}")

            time.sleep(5)


class DashboardManager:
    """ä»ªè¡¨ç›˜ç®¡ç†å™¨"""

    def __init__(self, data_source: DataSource):
        self.data_source = data_source
        self.renderer = ChartRenderer(data_source)
        self.pusher = RealtimeDataPusher(data_source)
        self._dashboards: Dict[str, Dashboard] = {}
        self._user_roles: Dict[str, List[str]] = defaultdict(list)

    def create_dashboard(self, name: str, description: str, owner: str,
                        allowed_roles: List[str] = None) -> Dashboard:
        """åˆ›å»ºä»ªè¡¨ç›˜"""
        dashboard = Dashboard(
            dashboard_id=str(random.randint(10000, 99999)),
            name=name,
            description=description,
            owner=owner,
            allowed_roles=allowed_roles or ["admin", "viewer"]
        )
        self._dashboards[dashboard.dashboard_id] = dashboard
        return dashboard

    def add_chart(self, dashboard_id: str, chart_config: ChartConfig) -> bool:
        """æ·»åŠ å›¾è¡¨åˆ°ä»ªè¡¨ç›˜"""
        if dashboard_id not in self._dashboards:
            return False
        self._dashboards[dashboard_id].charts.append(chart_config)
        return True

    def render_dashboard(self, dashboard_id: str, user_role: str) -> Optional[Dict]:
        """æ¸²æŸ“æ•´ä¸ªä»ªè¡¨ç›˜"""
        dashboard = self._dashboards.get(dashboard_id)
        if not dashboard:
            return None

        if not dashboard.is_public and user_role not in dashboard.allowed_roles:
            return {"error": "Access denied"}

        charts_data = []
        for chart in dashboard.charts:
            chart_data = self.renderer.render(chart)
            chart_data["chart_id"] = chart.chart_id
            chart_data["refresh_interval"] = chart.refresh_interval
            charts_data.append(chart_data)

        return {
            "dashboard_id": dashboard.dashboard_id,
            "name": dashboard.name,
            "description": dashboard.description,
            "charts": charts_data,
            "layout": dashboard.layout
        }

    def get_dashboard_list(self, user_role: str) -> List[Dict]:
        """è·å–ä»ªè¡¨ç›˜åˆ—è¡¨"""
        result = []
        for dashboard in self._dashboards.values():
            if dashboard.is_public or user_role in dashboard.allowed_roles:
                result.append({
                    "dashboard_id": dashboard.dashboard_id,
                    "name": dashboard.name,
                    "description": dashboard.description,
                    "owner": dashboard.owner
                })
        return result


class PermissionManager:
    """æƒé™ç®¡ç†å™¨"""

    def __init__(self):
        self._roles: Dict[str, List[str]] = {
            "admin": ["view", "edit", "delete", "share", "create"],
            "editor": ["view", "edit", "create"],
            "viewer": ["view"],
            "operator": ["view", "acknowledge_alert"]
        }
        self._user_roles: Dict[str, List[str]] = defaultdict(list)

    def assign_role(self, user_id: str, role: str):
        """åˆ†é…è§’è‰²"""
        if role not in self._roles:
            raise ValueError(f"Unknown role: {role}")
        self._user_roles[user_id].append(role)

    def check_permission(self, user_id: str, permission: str) -> bool:
        """æ£€æŸ¥æƒé™"""
        user_roles = self._user_roles.get(user_id, [])
        for role in user_roles:
            if permission in self._roles.get(role, []):
                return True
        return False


class VisualizationPlatform:
    """å¯è§†åŒ–å¹³å°ä¸»ç±»"""

    def __init__(self):
        self.data_source = DataSource("main", "timeseries")
        self.dashboard_manager = DashboardManager(self.data_source)
        self.permission_manager = PermissionManager()
        self._running = False

    def start(self):
        """å¯åŠ¨å¹³å°"""
        self._running = True
        self.dashboard_manager.pusher.start()
        print("Visualization Platform started")

    def stop(self):
        """åœæ­¢å¹³å°"""
        self._running = False
        self.dashboard_manager.pusher.stop()

    def create_production_dashboard(self) -> Dashboard:
        """åˆ›å»ºç”Ÿäº§ç›‘æ§ä»ªè¡¨ç›˜"""
        dashboard = self.dashboard_manager.create_dashboard(
            name="Production Overview",
            description="Global production line monitoring",
            owner="admin",
            allowed_roles=["admin", "editor", "viewer", "operator"]
        )

        # CPUä½¿ç”¨ç‡æŠ˜çº¿å›¾
        self.dashboard_manager.add_chart(dashboard.dashboard_id, ChartConfig(
            chart_id="chart_001",
            title="CPU Usage Trend",
            chart_type=ChartType.LINE,
            data_source="main",
            query="cpu_usage",
            time_range=TimeRange.LAST_1H,
            refresh_interval=30,
            filters={"datacenter": "all"}
        ))

        # å†…å­˜ä½¿ç”¨ç‡ä»ªè¡¨ç›˜
        self.dashboard_manager.add_chart(dashboard.dashboard_id, ChartConfig(
            chart_id="chart_002",
            title="Memory Usage",
            chart_type=ChartType.GAUGE,
            data_source="main",
            query="memory_usage",
            time_range=TimeRange.LAST_5M,
            refresh_interval=10
        ))

        # å‘Šè­¦åˆ†å¸ƒé¥¼å›¾
        self.dashboard_manager.add_chart(dashboard.dashboard_id, ChartConfig(
            chart_id="chart_003",
            title="Alert Distribution",
            chart_type=ChartType.PIE,
            data_source="main",
            query="alert_count",
            time_range=TimeRange.LAST_24H,
            refresh_interval=300
        ))

        # å®æ—¶æ•°æ®è¡¨æ ¼
        self.dashboard_manager.add_chart(dashboard.dashboard_id, ChartConfig(
            chart_id="chart_004",
            title="Recent Metrics",
            chart_type=ChartType.TABLE,
            data_source="main",
            query="all_metrics",
            time_range=TimeRange.LAST_15M,
            refresh_interval=60
        ))

        # çƒ­åŠ›å›¾
        self.dashboard_manager.add_chart(dashboard.dashboard_id, ChartConfig(
            chart_id="chart_005",
            title="Weekly Heatmap",
            chart_type=ChartType.HEATMAP,
            data_source="main",
            query="activity_heatmap",
            time_range=TimeRange.LAST_7D,
            refresh_interval=600
        ))

        return dashboard


# ============ æ¼”ç¤º ============

def demo_visualization_platform():
    """æ¼”ç¤ºå¯è§†åŒ–å¹³å°"""
    platform = VisualizationPlatform()
    platform.start()

    # åˆ›å»ºç”Ÿäº§ä»ªè¡¨ç›˜
    dashboard = platform.create_production_dashboard()

    print("="*60)
    print("å¯è§†åŒ–å¹³å°æ¼”ç¤º")
    print("="*60)

    # è·å–ä»ªè¡¨ç›˜åˆ—è¡¨
    print("\nä»ªè¡¨ç›˜åˆ—è¡¨:")
    dashboards = platform.dashboard_manager.get_dashboard_list("viewer")
    for d in dashboards:
        print(f"  - {d['name']} ({d['dashboard_id']})")

    # æ¸²æŸ“ä»ªè¡¨ç›˜
    print(f"\næ¸²æŸ“ä»ªè¡¨ç›˜: {dashboard.name}")
    rendered = platform.dashboard_manager.render_dashboard(dashboard.dashboard_id, "viewer")

    if rendered and "error" not in rendered:
        print(f"  åŒ…å«å›¾è¡¨æ•°é‡: {len(rendered['charts'])}")
        print("\nå›¾è¡¨è¯¦æƒ…:")
        for chart in rendered['charts']:
            print(f"  [{chart['type'].upper()}] {chart['title']['text']}")
            if chart['type'] == 'gauge':
                value = chart['series'][0]['data'][0]['value']
                print(f"    å½“å‰å€¼: {value}%")
            elif chart['type'] == 'line':
                print(f"    æ•°æ®ç‚¹: {len(chart['series'][0]['data'])}")
            elif chart['type'] == 'table':
                print(f"    è¡Œæ•°: {len(chart['rows'])}")

    # æ¨¡æ‹Ÿå®æ—¶æ•°æ®æ¨é€
    print("\næ¨¡æ‹Ÿå®æ—¶æ•°æ®æ¨é€ (5ç§’):")
    received_data = []

    def on_data(data):
        received_data.append(data)
        print(f"  æ”¶åˆ°æ›´æ–°: chart={data['chart_id']}, value={data['value']:.2f}")

    platform.dashboard_manager.pusher.subscribe("chart_002", on_data)
    time.sleep(5)

    print(f"\nå…±æ”¶åˆ° {len(received_data)} æ¡å®æ—¶æ›´æ–°")

    platform.stop()
    print("\n" + "="*60)
    print("å¯è§†åŒ–å¹³å°å·²åœæ­¢")


if __name__ == '__main__':
    demo_visualization_platform()
```

### 6.5 æ•ˆæœè¯„ä¼°

**æ€§èƒ½æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | æ”¹è¿›å‰ | æ”¹è¿›å | æå‡å¹…åº¦ |
|------|--------|--------|----------|
| æ•°æ®å»¶è¿Ÿ | 5åˆ†é’Ÿ | 8ç§’ | -97% |
| ä»ªè¡¨ç›˜åŠ è½½æ—¶é—´ | 15ç§’ | 1.5ç§’ | -90% |
| å¹¶å‘ç”¨æˆ·æ•° | 100 | 1500 | +1400% |
| å›¾è¡¨æ¸²æŸ“å¸§ç‡ | 15fps | 60fps | +300% |
| ç§»åŠ¨ç«¯ä½“éªŒè¯„åˆ† | 4.2 | 4.8 | +14% |
| æ•°æ®æ–°é²œåº¦ | åˆ†é’Ÿçº§ | ç§’çº§ | -95% |

**ä¸šåŠ¡ä»·å€¼**ï¼š

1. **ç”Ÿäº§æ•ˆç‡æå‡**ï¼š
   - è®¾å¤‡æ•…éšœå‘ç°æ—¶é—´ä»30åˆ†é’Ÿç¼©çŸ­è‡³2åˆ†é’Ÿ
   - ç”Ÿäº§çº¿åœæœºæ—¶é—´å‡å°‘40%ï¼Œå¹´å¢äº§çº¦2äº¿å…ƒ
   - è´¨é‡å¼‚å¸¸å®æ—¶é¢„è­¦ï¼Œæ¬¡å“ç‡é™ä½25%

2. **ç®¡ç†å†³ç­–ä¼˜åŒ–**ï¼š
   - ç®¡ç†å±‚å¯å®æ—¶æŸ¥çœ‹å…¨çƒ20ä¸ªå·¥å‚çŠ¶æ€
   - æ•°æ®é©±åŠ¨çš„ç”Ÿäº§è°ƒåº¦ä¼˜åŒ–ï¼Œåº“å­˜å‘¨è½¬ç‡æå‡20%
   - ç§»åŠ¨ç«¯æ”¯æŒä½¿ç®¡ç†å±‚éšæ—¶éšåœ°æŒæ¡ç”Ÿäº§çŠ¶å†µ

3. **è¿ç»´æˆæœ¬é™ä½**ï¼š
   - ç»Ÿä¸€å¹³å°æ›¿ä»£åŸæœ‰20å¥—ç‹¬ç«‹ç³»ç»Ÿï¼ŒèŠ‚çœè®¸å¯è´¹800ä¸‡å…ƒ/å¹´
   - è‡ªåŠ©å¼ä»ªè¡¨ç›˜åˆ¶ä½œï¼Œå‡å°‘å¼€å‘äººåŠ›æŠ•å…¥60%
   - å‘Šè­¦å¯è§†åŒ–ä½¿æ•…éšœå¤„ç†æ•ˆç‡æå‡50%

4. **æ•°æ®èµ„äº§æ²‰æ·€**ï¼š
   - å»ºç«‹ç»Ÿä¸€æŒ‡æ ‡ä½“ç³»å’Œå¯è§†åŒ–æ ‡å‡†
   - çŸ¥è¯†åº“ç§¯ç´¯æœ€ä½³å®è·µä»ªè¡¨ç›˜æ¨¡æ¿
   - æ”¯æŒè·¨å·¥å‚æ•°æ®å¯¹æ¯”å’Œæ ‡æ†åˆ†æ

**ç»éªŒæ•™è®­**ï¼š

1. **æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼šé‡‡ç”¨æ•°æ®é¢„èšåˆå’Œå¢é‡æ›´æ–°ç­–ç•¥ï¼Œé¿å…å®æ—¶è®¡ç®—å¤§é‡åŸå§‹æ•°æ®
2. **ç§»åŠ¨ç«¯ä¼˜å…ˆ**ï¼šè®¾è®¡æ—¶å…ˆè€ƒè™‘ç§»åŠ¨ç«¯é™åˆ¶ï¼Œå†æ‰©å±•åˆ°æ¡Œé¢ç«¯ï¼Œç¡®ä¿å…¨å¹³å°ä½“éªŒä¸€è‡´
3. **ç¼“å­˜ç­–ç•¥é‡è¦**ï¼šå¤šçº§ç¼“å­˜ï¼ˆæµè§ˆå™¨ç¼“å­˜ã€CDNç¼“å­˜ã€æœåŠ¡ç«¯ç¼“å­˜ï¼‰å¯¹æ€§èƒ½æå‡æ˜¾è‘—
4. **æ¸è¿›å¼åŠ è½½**ï¼šå¤§å‹ä»ªè¡¨ç›˜é‡‡ç”¨æ‡’åŠ è½½ç­–ç•¥ï¼Œä¼˜å…ˆæ¸²æŸ“é¦–å±å†…å®¹
5. **ç”¨æˆ·åŸ¹è®­**ï¼šå¯è§†åŒ–å·¥å…·éœ€è¦é…å¥—åŸ¹è®­ï¼Œå¦åˆ™ç”¨æˆ·å¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨é«˜çº§åŠŸèƒ½

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `04_Transformation.md` - è½¬æ¢ä½“ç³»

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
