# å¯è§‚æµ‹æ€§Schemaè½¬æ¢ä½“ç³»

## ğŸ“‘ ç›®å½•

- [å¯è§‚æµ‹æ€§Schemaè½¬æ¢ä½“ç³»](#å¯è§‚æµ‹æ€§schemaè½¬æ¢ä½“ç³»)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. è½¬æ¢ä½“ç³»æ¦‚è¿°](#1-è½¬æ¢ä½“ç³»æ¦‚è¿°)
    - [1.1 è½¬æ¢ç›®æ ‡](#11-è½¬æ¢ç›®æ ‡)
  - [2. åè®®è½¬æ¢](#2-åè®®è½¬æ¢)
    - [2.1 OTLPåˆ°Prometheusè½¬æ¢](#21-otlpåˆ°prometheusè½¬æ¢)
    - [2.2 OTLPåˆ°Jaegerè½¬æ¢](#22-otlpåˆ°jaegerè½¬æ¢)
    - [2.3 Prometheusåˆ°OTLPè½¬æ¢](#23-prometheusåˆ°otlpè½¬æ¢)
  - [3. æ•°æ®æ ¼å¼è½¬æ¢](#3-æ•°æ®æ ¼å¼è½¬æ¢)
  - [4. è½¬æ¢å·¥å…·](#4-è½¬æ¢å·¥å…·)
  - [5. è½¬æ¢éªŒè¯](#5-è½¬æ¢éªŒè¯)
  - [6. å¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨ä¸åˆ†æ](#6-å¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨ä¸åˆ†æ)
    - [6.1 PostgreSQLå¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨](#61-postgresqlå¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨)
    - [6.2 å¯è§‚æµ‹æ€§æ•°æ®åˆ†ææŸ¥è¯¢](#62-å¯è§‚æµ‹æ€§æ•°æ®åˆ†ææŸ¥è¯¢)

---

## 1. è½¬æ¢ä½“ç³»æ¦‚è¿°

å¯è§‚æµ‹æ€§Schemaè½¬æ¢ä½“ç³»æ”¯æŒOTLPã€Prometheusã€Jaegerç­‰åè®®ä¹‹é—´çš„è½¬æ¢ã€‚

### 1.1 è½¬æ¢ç›®æ ‡

1. **åè®®è½¬æ¢**ï¼šOTLP â†” Prometheus, OTLP â†” Jaeger
2. **æ•°æ®æ ¼å¼è½¬æ¢**ï¼šgRPC â†” HTTP/JSON
3. **æŒ‡æ ‡æ ¼å¼è½¬æ¢**ï¼šOTLP Metric â†” Prometheus Metric

---

## 2. åè®®è½¬æ¢

### 2.1 OTLPåˆ°Prometheusè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- OTLP Metric â†’ Prometheus Metric
- OTLP DataPoint â†’ Prometheus Sample
- OTLP Resource Attributes â†’ Prometheus Labels

### 2.2 OTLPåˆ°Jaegerè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- OTLP Span â†’ Jaeger Span
- OTLP Trace â†’ Jaeger Trace
- OTLP Resource â†’ Jaeger Process

### 2.3 Prometheusåˆ°OTLPè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- Prometheus Metric â†’ OTLP Metric
- Prometheus Labels â†’ OTLP Attributes
- Prometheus Sample â†’ OTLP DataPoint

---

## 3. æ•°æ®æ ¼å¼è½¬æ¢

æ”¯æŒgRPCã€HTTP/JSONã€Protobufç­‰æ ¼å¼ä¹‹é—´çš„è½¬æ¢ã€‚

---

## 4. è½¬æ¢å·¥å…·

- **OpenTelemetry Collector**ï¼šOTLPæ”¶é›†å™¨
- **Prometheus Exporter**ï¼šPrometheuså¯¼å‡ºå™¨
- **Jaeger Exporter**ï¼šJaegerå¯¼å‡ºå™¨

---

## 5. è½¬æ¢éªŒè¯

éªŒè¯è½¬æ¢çš„è¯­ä¹‰ç­‰ä»·æ€§ã€æ•°æ®å®Œæ•´æ€§å’Œæ€§èƒ½ã€‚

---

## 6. å¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨ä¸åˆ†æ

### 6.1 PostgreSQLå¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨

**å¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨æ–¹æ¡ˆ**ï¼š

```python
import psycopg2
import json
from typing import Dict, List, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass

@dataclass
class MetricDataPoint:
    """æŒ‡æ ‡æ•°æ®ç‚¹"""
    metric_name: str
    value: float
    timestamp: datetime
    labels: Dict[str, str]
    resource: Dict[str, str]

@dataclass
class LogRecord:
    """æ—¥å¿—è®°å½•"""
    severity: str
    message: str
    timestamp: datetime
    attributes: Dict[str, str]
    trace_id: Optional[str]
    span_id: Optional[str]

@dataclass
class Span:
    """è¿½è¸ªSpan"""
    trace_id: str
    span_id: str
    parent_span_id: Optional[str]
    name: str
    start_time: datetime
    end_time: datetime
    attributes: Dict[str, str]

class ObservabilityStorage:
    """å¯è§‚æµ‹æ€§æ•°æ®å­˜å‚¨ç³»ç»Ÿ"""

    def __init__(self, connection_string: str):
        self.conn = psycopg2.connect(connection_string)
        self.cur = self.conn.cursor()
        self._create_tables()

    def _create_tables(self):
        """åˆ›å»ºå¯è§‚æµ‹æ€§æ•°æ®è¡¨"""
        # æŒ‡æ ‡æ•°æ®è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS metrics (
                id BIGSERIAL PRIMARY KEY,
                metric_name VARCHAR(500) NOT NULL,
                value FLOAT NOT NULL,
                timestamp TIMESTAMP NOT NULL,
                labels JSONB,
                resource JSONB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # æ—¥å¿—è®°å½•è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS logs (
                id BIGSERIAL PRIMARY KEY,
                severity VARCHAR(50) NOT NULL,
                message TEXT NOT NULL,
                timestamp TIMESTAMP NOT NULL,
                attributes JSONB,
                trace_id VARCHAR(32),
                span_id VARCHAR(16),
                resource JSONB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # è¿½è¸ªSpanè¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS spans (
                id BIGSERIAL PRIMARY KEY,
                trace_id VARCHAR(32) NOT NULL,
                span_id VARCHAR(16) NOT NULL,
                parent_span_id VARCHAR(16),
                name VARCHAR(500) NOT NULL,
                start_time TIMESTAMP NOT NULL,
                end_time TIMESTAMP NOT NULL,
                duration_ms BIGINT GENERATED ALWAYS AS (
                    EXTRACT(EPOCH FROM (end_time - start_time)) * 1000
                ) STORED,
                attributes JSONB,
                resource JSONB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # èµ„æºå®šä¹‰è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS resources (
                id SERIAL PRIMARY KEY,
                service_name VARCHAR(200) NOT NULL,
                service_version VARCHAR(100),
                attributes JSONB NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # ç»Ÿè®¡ä¿¡æ¯è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS observability_statistics (
                id SERIAL PRIMARY KEY,
                metric_name VARCHAR(500),
                statistic_type VARCHAR(50) NOT NULL,
                time_window TIMESTAMP NOT NULL,
                statistics JSONB NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(metric_name, statistic_type, time_window)
            )
        """)

        # åˆ›å»ºç´¢å¼•
        self.cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_metrics_name_time
            ON metrics(metric_name, timestamp DESC)
        """)
        self.cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_logs_severity_time
            ON logs(severity, timestamp DESC)
        """)
        self.cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_spans_trace_id
            ON spans(trace_id, start_time)
        """)
        self.cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_spans_trace_span
            ON spans(trace_id, span_id)
        """)

        self.conn.commit()

    def store_metric(self, data_point: MetricDataPoint):
        """å­˜å‚¨æŒ‡æ ‡æ•°æ®ç‚¹"""
        self.cur.execute("""
            INSERT INTO metrics
            (metric_name, value, timestamp, labels, resource)
            VALUES (%s, %s, %s, %s::jsonb, %s::jsonb)
        """, (data_point.metric_name, data_point.value,
              data_point.timestamp, json.dumps(data_point.labels or {}),
              json.dumps(data_point.resource or {})))
        self.conn.commit()

    def store_log(self, log: LogRecord):
        """å­˜å‚¨æ—¥å¿—è®°å½•"""
        self.cur.execute("""
            INSERT INTO logs
            (severity, message, timestamp, attributes, trace_id, span_id, resource)
            VALUES (%s, %s, %s, %s::jsonb, %s, %s, %s::jsonb)
        """, (log.severity, log.message, log.timestamp,
              json.dumps(log.attributes or {}), log.trace_id, log.span_id,
              json.dumps({})))
        self.conn.commit()

    def store_span(self, span: Span):
        """å­˜å‚¨è¿½è¸ªSpan"""
        self.cur.execute("""
            INSERT INTO spans
            (trace_id, span_id, parent_span_id, name, start_time, end_time, attributes, resource)
            VALUES (%s, %s, %s, %s, %s, %s, %s::jsonb, %s::jsonb)
        """, (span.trace_id, span.span_id, span.parent_span_id,
              span.name, span.start_time, span.end_time,
              json.dumps(span.attributes or {}), json.dumps({})))
        self.conn.commit()

    def query_trace(self, trace_id: str):
        """æŸ¥è¯¢å®Œæ•´è¿½è¸ª"""
        self.cur.execute("""
            SELECT trace_id, span_id, parent_span_id, name,
                   start_time, end_time, duration_ms, attributes
            FROM spans
            WHERE trace_id = %s
            ORDER BY start_time
        """, (trace_id,))
        return self.cur.fetchall()

    def calculate_metric_statistics(self, metric_name: str, time_window: datetime):
        """è®¡ç®—æŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯"""
        self.cur.execute("""
            SELECT
                COUNT(*) as count,
                AVG(value) as avg_value,
                MIN(value) as min_value,
                MAX(value) as max_value,
                STDDEV(value) as stddev_value
            FROM metrics
            WHERE metric_name = %s AND timestamp >= %s
        """, (metric_name, time_window))

        stats = dict(zip([desc[0] for desc in self.cur.description],
                         self.cur.fetchone()))

        # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        self.cur.execute("""
            INSERT INTO observability_statistics
            (metric_name, statistic_type, time_window, statistics)
            VALUES (%s, %s, %s, %s::jsonb)
            ON CONFLICT (metric_name, statistic_type, time_window)
            DO UPDATE SET statistics = EXCLUDED.statistics
        """, (metric_name, "aggregate", time_window, json.dumps(stats)))
        self.conn.commit()

        return stats
```

### 6.2 å¯è§‚æµ‹æ€§æ•°æ®åˆ†ææŸ¥è¯¢

**æŸ¥è¯¢ç¤ºä¾‹**ï¼š

```python
# æŸ¥è¯¢å®Œæ•´è¿½è¸ª
spans = storage.query_trace("abc123def456")

# è®¡ç®—æŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯
stats = storage.calculate_metric_statistics(
    metric_name="http_request_duration",
    time_window=datetime.now() - timedelta(hours=1)
)
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `05_Case_Studies.md` - å®è·µæ¡ˆä¾‹

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
