# æ•°æ®åº“Schemaå®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [æ•°æ®åº“Schemaå®è·µæ¡ˆä¾‹](#æ•°æ®åº“schemaå®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¡ˆä¾‹æ¦‚è¿°](#1-æ¡ˆä¾‹æ¦‚è¿°)
  - [2. æ¡ˆä¾‹1ï¼šSQLiteåˆ°PostgreSQLè¿ç§»](#2-æ¡ˆä¾‹1sqliteåˆ°postgresqlè¿ç§»)
    - [2.1 ä¸šåŠ¡èƒŒæ™¯](#21-ä¸šåŠ¡èƒŒæ™¯)
    - [2.2 æŠ€æœ¯æŒ‘æˆ˜](#22-æŠ€æœ¯æŒ‘æˆ˜)
    - [2.3 å®Œæ•´ä»£ç å®ç°](#23-å®Œæ•´ä»£ç å®ç°)
    - [2.4 æ•ˆæœè¯„ä¼°](#24-æ•ˆæœè¯„ä¼°)
  - [3. æ¡ˆä¾‹2ï¼šSchemaç‰ˆæœ¬ç®¡ç†](#3-æ¡ˆä¾‹2schemaç‰ˆæœ¬ç®¡ç†)
    - [3.1 ä¸šåŠ¡èƒŒæ™¯](#31-ä¸šåŠ¡èƒŒæ™¯)
    - [3.2 æŠ€æœ¯æŒ‘æˆ˜](#32-æŠ€æœ¯æŒ‘æˆ˜)
    - [3.3 å®Œæ•´ä»£ç å®ç°](#33-å®Œæ•´ä»£ç å®ç°)
    - [3.4 æ•ˆæœè¯„ä¼°](#34-æ•ˆæœè¯„ä¼°)
  - [4. æ¡ˆä¾‹3ï¼šæ•°æ®åº“Schemaè‡ªåŠ¨ç”Ÿæˆ](#4-æ¡ˆä¾‹3æ•°æ®åº“schemaè‡ªåŠ¨ç”Ÿæˆ)
    - [4.1 ä¸šåŠ¡èƒŒæ™¯](#41-ä¸šåŠ¡èƒŒæ™¯)
    - [4.2 æŠ€æœ¯æŒ‘æˆ˜](#42-æŠ€æœ¯æŒ‘æˆ˜)
    - [4.3 å®Œæ•´ä»£ç å®ç°](#43-å®Œæ•´ä»£ç å®ç°)
    - [4.4 æ•ˆæœè¯„ä¼°](#44-æ•ˆæœè¯„ä¼°)

---

## 1. æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›æ•°æ®åº“Schemaåœ¨å®é™…åº”ç”¨ä¸­çš„å®è·µæ¡ˆä¾‹ï¼Œæ¶µç›–æ•°æ®åº“è¿ç§»ã€ç‰ˆæœ¬ç®¡ç†å’Œè‡ªåŠ¨ç”Ÿæˆä¸‰å¤§æ ¸å¿ƒåœºæ™¯ã€‚æ¯ä¸ªæ¡ˆä¾‹åŒ…å«è¯¦ç»†çš„ä¸šåŠ¡èƒŒæ™¯ã€æŠ€æœ¯æŒ‘æˆ˜åˆ†æã€å®Œæ•´çš„Pythonä»£ç å®ç°ä»¥åŠé‡åŒ–çš„æ•ˆæœè¯„ä¼°ã€‚

---

## 2. æ¡ˆä¾‹1ï¼šSQLiteåˆ°PostgreSQLè¿ç§»

### 2.1 ä¸šåŠ¡èƒŒæ™¯

**ä¼ä¸šèƒŒæ™¯**ï¼š
- **å…¬å¸åç§°**ï¼šäº‘æ™ºç§‘æŠ€ï¼ˆCloudMind Techï¼‰
- **è¡Œä¸šé¢†åŸŸ**ï¼šSaaSä¼ä¸šç®¡ç†è½¯ä»¶
- **å…¬å¸è§„æ¨¡**ï¼š500+å‘˜å·¥ï¼ŒæœåŠ¡10ä¸‡+ä¼ä¸šå®¢æˆ·
- **åŸæœ‰ç³»ç»Ÿ**ï¼šåŸºäºSQLiteçš„ç§»åŠ¨ç«¯ç¦»çº¿æ•°æ®å­˜å‚¨æ–¹æ¡ˆ

**ä¸šåŠ¡ç—›ç‚¹**ï¼š
1. **æ€§èƒ½ç“¶é¢ˆ**ï¼šSQLiteåœ¨å¹¶å‘è®¿é—®è¶…è¿‡100ç”¨æˆ·æ—¶å‡ºç°ä¸¥é‡æ€§èƒ½ä¸‹é™
2. **æ•°æ®å­¤å²›**ï¼šå„åœ°åˆ†å…¬å¸æ•°æ®åˆ†æ•£åœ¨æœ¬åœ°SQLiteæ–‡ä»¶ä¸­ï¼Œæ— æ³•å®æ—¶åŒæ­¥
3. **æ‰©å±•å—é™**ï¼šå•æ–‡ä»¶å­˜å‚¨é™åˆ¶ï¼ˆæœ€å¤§140TBç†è®ºå€¼ï¼Œå®é™…æ€§èƒ½åœ¨10GBåæ€¥å‰§ä¸‹é™ï¼‰
4. **åˆ†æå›°éš¾**ï¼šæ— æ³•è¿›è¡Œå¤æ‚çš„è·¨è¡¨åˆ†æå’Œå®æ—¶æŠ¥è¡¨ç”Ÿæˆ
5. **å¤‡ä»½æ¢å¤**ï¼šç¼ºä¹è‡ªåŠ¨åŒ–çš„å¤‡ä»½æœºåˆ¶ï¼Œæ•°æ®ä¸¢å¤±é£é™©é«˜

**ä¸šåŠ¡ç›®æ ‡**ï¼š
1. å°†æ ¸å¿ƒæ•°æ®è¿ç§»è‡³PostgreSQLï¼Œæ”¯æŒ1000+å¹¶å‘ç”¨æˆ·
2. å®ç°æ•°æ®çš„å®æ—¶é›†ä¸­ç®¡ç†å’Œåˆ†æ
3. å»ºç«‹è‡ªåŠ¨åŒ–çš„å¤‡ä»½å’Œç¾éš¾æ¢å¤æœºåˆ¶
4. è¿ç§»è¿‡ç¨‹é›¶åœæœºï¼Œæ•°æ®é›¶ä¸¢å¤±
5. è¿ç§»åæŸ¥è¯¢æ€§èƒ½æå‡50%ä»¥ä¸Š

### 2.2 æŠ€æœ¯æŒ‘æˆ˜

| æŒ‘æˆ˜ç‚¹ | æè¿° | å½±å“çº§åˆ« |
|--------|------|----------|
| æ•°æ®ç±»å‹æ˜ å°„ | SQLiteåŠ¨æ€ç±»å‹ä¸PostgreSQLä¸¥æ ¼ç±»å‹çš„è½¬æ¢ | é«˜ |
| å¹¶å‘è¿ç§» | åœ¨çº¿ç³»ç»Ÿéœ€è¦ä¸åœæœºè¿ç§»ï¼Œæ•°æ®æŒç»­å˜æ›´ | é«˜ |
| æ•°æ®ä¸€è‡´æ€§éªŒè¯ | è·¨æ•°æ®åº“çš„æ•°æ®ä¸€è‡´æ€§æ ¡éªŒæœºåˆ¶ | é«˜ |
| å¤–é”®çº¦æŸé‡å»º | SQLiteå¤–é”®æ”¯æŒæœ‰é™ï¼Œéœ€è¦é‡æ–°è®¾è®¡çº¦æŸ | ä¸­ |
| æ€§èƒ½ä¼˜åŒ– | è¿ç§»åæŸ¥è¯¢è®¡åˆ’å’Œç´¢å¼•ä¼˜åŒ– | ä¸­ |

### 2.3 å®Œæ•´ä»£ç å®ç°

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SQLite to PostgreSQL Migration Tool
ä¼ä¸šçº§æ•°æ®åº“è¿ç§»è§£å†³æ–¹æ¡ˆ
"""

import sqlite3
import psycopg2
import psycopg2.extras
from typing import Dict, List, Tuple, Optional, Callable
from dataclasses import dataclass
from enum import Enum
import logging
import hashlib
import json
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MigrationStatus(Enum):
    """è¿ç§»çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    VERIFIED = "verified"


@dataclass
class MigrationMetrics:
    """è¿ç§»æŒ‡æ ‡æ•°æ®ç±»"""
    table_name: str
    row_count: int
    migration_time: float
    status: MigrationStatus
    checksum: str
    error_message: Optional[str] = None


class SchemaTranslator:
    """Schemaè½¬æ¢å™¨ï¼šSQLiteåˆ°PostgreSQL"""
    
    # æ•°æ®ç±»å‹æ˜ å°„è¡¨
    TYPE_MAPPING = {
        'INTEGER': 'INTEGER',
        'REAL': 'DOUBLE PRECISION',
        'TEXT': 'VARCHAR(255)',
        'BLOB': 'BYTEA',
        'NUMERIC': 'DECIMAL(20, 10)',
        'BOOLEAN': 'BOOLEAN',
        'DATETIME': 'TIMESTAMP',
        'DATE': 'DATE',
        'TIME': 'TIME'
    }
    
    # çº¦æŸæ˜ å°„
    CONSTRAINT_MAPPING = {
        'PRIMARY KEY': 'PRIMARY KEY',
        'UNIQUE': 'UNIQUE',
        'NOT NULL': 'NOT NULL',
        'AUTOINCREMENT': 'SERIAL'
    }
    
    def __init__(self):
        self.translation_log: List[Dict] = []
    
    def translate_type(self, sqlite_type: str, constraints: List[str]) -> str:
        """è½¬æ¢SQLiteæ•°æ®ç±»å‹åˆ°PostgreSQL"""
        sqlite_upper = sqlite_type.upper()
        
        # å¤„ç†è‡ªå¢å­—æ®µ
        if 'PRIMARY KEY' in constraints and sqlite_upper == 'INTEGER':
            if any('AUTOINCREMENT' in c.upper() for c in constraints):
                return 'SERIAL PRIMARY KEY'
            return 'SERIAL PRIMARY KEY'
        
        # æ—¶é—´æˆ³ç‰¹æ®Šå¤„ç†
        if 'created_at' in str(constraints).lower() or 'updated_at' in str(constraints).lower():
            if sqlite_upper == 'INTEGER':
                return 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'
        
        return self.TYPE_MAPPING.get(sqlite_upper, 'VARCHAR(255)')
    
    def translate_table_schema(self, sqlite_schema: Dict) -> str:
        """è½¬æ¢è¡¨Schema"""
        table_name = sqlite_schema['name']
        columns = sqlite_schema['columns']
        
        pg_columns = []
        primary_keys = []
        unique_constraints = []
        foreign_keys = []
        
        for col in columns:
            col_name = col['name']
            col_type = self.translate_type(col['type'], col.get('constraints', []))
            constraints = []
            
            # å¤„ç†çº¦æŸ
            for constraint in col.get('constraints', []):
                constraint_upper = constraint.upper()
                if 'PRIMARY KEY' in constraint_upper:
                    if 'SERIAL' not in col_type:
                        primary_keys.append(col_name)
                elif 'UNIQUE' in constraint_upper:
                    unique_constraints.append(col_name)
                elif 'NOT NULL' in constraint_upper:
                    constraints.append('NOT NULL')
                elif 'DEFAULT' in constraint_upper:
                    default_val = constraint.split('DEFAULT')[1].strip()
                    constraints.append(f'DEFAULT {default_val}')
            
            # å¤„ç†å¤–é”®
            if col.get('foreign_key'):
                fk = col['foreign_key']
                foreign_keys.append(
                    f"FOREIGN KEY ({col_name}) REFERENCES {fk['table']}({fk['column']})"
                )
            
            col_def = f"    {col_name} {col_type}"
            if constraints:
                col_def += ' ' + ' '.join(constraints)
            pg_columns.append(col_def)
        
        # ç»„è£…çº¦æŸ
        if primary_keys:
            pg_columns.append(f"    PRIMARY KEY ({', '.join(primary_keys)})")
        
        for uk in unique_constraints:
            pg_columns.append(f"    UNIQUE ({uk})")
        
        pg_columns.extend([f"    {fk}" for fk in foreign_keys])
        
        ddl = f"CREATE TABLE {table_name} (\n"
        ddl += ',\n'.join(pg_columns)
        ddl += "\n);"
        
        self.translation_log.append({
            'table': table_name,
            'sqlite_columns': len(columns),
            'pg_ddl': ddl
        })
        
        return ddl


class DataMigrator:
    """æ•°æ®è¿ç§»å™¨"""
    
    def __init__(self, sqlite_path: str, pg_config: Dict):
        self.sqlite_path = sqlite_path
        self.pg_config = pg_config
        self.translator = SchemaTranslator()
        self.metrics: List[MigrationMetrics] = []
        self.batch_size = 1000
    
    def get_sqlite_tables(self) -> List[Dict]:
        """è·å–SQLiteæ‰€æœ‰è¡¨ç»“æ„"""
        conn = sqlite3.connect(self.sqlite_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'")
        tables = cursor.fetchall()
        
        result = []
        for (table_name,) in tables:
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = cursor.fetchall()
            
            cursor.execute(f"PRAGMA foreign_key_list({table_name})")
            foreign_keys = cursor.fetchall()
            fk_map = {fk[3]: {'table': fk[2], 'column': fk[4]} for fk in foreign_keys}
            
            col_info = []
            for col in columns:
                col_data = {
                    'name': col[1],
                    'type': col[2],
                    'constraints': []
                }
                if col[3]:  # notnull
                    col_data['constraints'].append('NOT NULL')
                if col[4] is not None:  # default
                    col_data['constraints'].append(f'DEFAULT {col[4]}')
                if col[5]:  # pk
                    col_data['constraints'].append('PRIMARY KEY')
                    if col[2].upper() == 'INTEGER':
                        col_data['constraints'].append('AUTOINCREMENT')
                
                if col[1] in fk_map:
                    col_data['foreign_key'] = fk_map[col[1]]
                
                col_info.append(col_data)
            
            result.append({
                'name': table_name,
                'columns': col_info
            })
        
        conn.close()
        return result
    
    def create_postgres_schema(self, tables: List[Dict]) -> None:
        """åœ¨PostgreSQLä¸­åˆ›å»ºSchema"""
        conn = psycopg2.connect(**self.pg_config)
        cursor = conn.cursor()
        
        for table in tables:
            ddl = self.translator.translate_table_schema(table)
            try:
                cursor.execute(f"DROP TABLE IF EXISTS {table['name']} CASCADE")
                cursor.execute(ddl)
                logger.info(f"Created table: {table['name']}")
            except Exception as e:
                logger.error(f"Failed to create table {table['name']}: {e}")
                raise
        
        conn.commit()
        conn.close()
    
    def migrate_table(self, table_name: str) -> MigrationMetrics:
        """è¿ç§»å•ä¸ªè¡¨çš„æ•°æ®"""
        start_time = time.time()
        
        try:
            # è¿æ¥SQLite
            sqlite_conn = sqlite3.connect(self.sqlite_path)
            sqlite_cursor = sqlite_conn.cursor()
            
            # è¿æ¥PostgreSQL
            pg_conn = psycopg2.connect(**self.pg_config)
            pg_cursor = pg_conn.cursor()
            
            # è·å–åˆ—å
            sqlite_cursor.execute(f"PRAGMA table_info({table_name})")
            columns = [col[1] for col in sqlite_cursor.fetchall()]
            column_str = ', '.join(columns)
            
            # è·å–æ•°æ®è¡Œæ•°
            sqlite_cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            row_count = sqlite_cursor.fetchone()[0]
            
            # æ‰¹é‡è¿ç§»æ•°æ®
            sqlite_cursor.execute(f"SELECT * FROM {table_name}")
            
            batch = []
            checksum_data = []
            
            for row in sqlite_cursor:
                batch.append(row)
                checksum_data.append(str(row))
                
                if len(batch) >= self.batch_size:
                    self._insert_batch(pg_cursor, table_name, column_str, columns, batch)
                    batch = []
            
            if batch:
                self._insert_batch(pg_cursor, table_name, column_str, columns, batch)
            
            # è®¡ç®—æ ¡éªŒå’Œ
            checksum = hashlib.md5(
                ''.join(sorted(checksum_data)).encode()
            ).hexdigest()
            
            pg_conn.commit()
            sqlite_conn.close()
            pg_conn.close()
            
            migration_time = time.time() - start_time
            
            metrics = MigrationMetrics(
                table_name=table_name,
                row_count=row_count,
                migration_time=migration_time,
                status=MigrationStatus.COMPLETED,
                checksum=checksum
            )
            
            logger.info(f"Migrated {table_name}: {row_count} rows in {migration_time:.2f}s")
            return metrics
            
        except Exception as e:
            migration_time = time.time() - start_time
            return MigrationMetrics(
                table_name=table_name,
                row_count=0,
                migration_time=migration_time,
                status=MigrationStatus.FAILED,
                checksum='',
                error_message=str(e)
            )
    
    def _insert_batch(self, cursor, table_name: str, column_str: str, 
                      columns: List[str], batch: List[Tuple]) -> None:
        """æ‰¹é‡æ’å…¥æ•°æ®"""
        placeholders = ', '.join(['%s'] * len(columns))
        query = f"INSERT INTO {table_name} ({column_str}) VALUES ({placeholders})"
        
        psycopg2.extras.execute_batch(cursor, query, batch)
    
    def verify_migration(self, table_name: str, expected_checksum: str) -> bool:
        """éªŒè¯è¿ç§»æ•°æ®ä¸€è‡´æ€§"""
        conn = psycopg2.connect(**self.pg_config)
        cursor = conn.cursor()
        
        cursor.execute(f"SELECT * FROM {table_name}")
        rows = cursor.fetchall()
        
        checksum_data = [str(row) for row in rows]
        actual_checksum = hashlib.md5(
            ''.join(sorted(checksum_data)).encode()
        ).hexdigest()
        
        conn.close()
        
        return actual_checksum == expected_checksum
    
    def run_migration(self) -> Dict:
        """æ‰§è¡Œå®Œæ•´è¿ç§»æµç¨‹"""
        logger.info("Starting migration process...")
        
        # 1. è·å–SQLiteè¡¨ç»“æ„
        tables = self.get_sqlite_tables()
        logger.info(f"Found {len(tables)} tables to migrate")
        
        # 2. åˆ›å»ºPostgreSQL Schema
        self.create_postgres_schema(tables)
        
        # 3. å¹¶è¡Œè¿ç§»æ•°æ®
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(self.migrate_table, table['name']): table['name']
                for table in tables
            }
            
            for future in as_completed(futures):
                table_name = futures[future]
                try:
                    metrics = future.result()
                    self.metrics.append(metrics)
                except Exception as e:
                    logger.error(f"Migration failed for {table_name}: {e}")
        
        # 4. éªŒè¯æ•°æ®ä¸€è‡´æ€§
        verified_count = 0
        for metrics in self.metrics:
            if metrics.status == MigrationStatus.COMPLETED:
                is_valid = self.verify_migration(metrics.table_name, metrics.checksum)
                if is_valid:
                    metrics.status = MigrationStatus.VERIFIED
                    verified_count += 1
        
        # ç”ŸæˆæŠ¥å‘Š
        total_rows = sum(m.row_count for m in self.metrics)
        total_time = sum(m.migration_time for m in self.metrics)
        failed_tables = [m for m in self.metrics if m.status == MigrationStatus.FAILED]
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_tables': len(tables),
            'total_rows': total_rows,
            'total_time': total_time,
            'verified_tables': verified_count,
            'failed_tables': len(failed_tables),
            'tables_per_second': total_rows / total_time if total_time > 0 else 0,
            'metrics': self.metrics
        }
        
        return report


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # é…ç½®
    SQLITE_DB = 'source.db'
    PG_CONFIG = {
        'host': 'localhost',
        'port': 5432,
        'database': 'target_db',
        'user': 'postgres',
        'password': 'password'
    }
    
    # æ‰§è¡Œè¿ç§»
    migrator = DataMigrator(SQLITE_DB, PG_CONFIG)
    report = migrator.run_migration()
    
    # è¾“å‡ºæŠ¥å‘Š
    print("\n" + "="*60)
    print("MIGRATION REPORT")
    print("="*60)
    print(f"Total Tables: {report['total_tables']}")
    print(f"Total Rows: {report['total_rows']:,}")
    print(f"Total Time: {report['total_time']:.2f}s")
    print(f"Verified Tables: {report['verified_tables']}")
    print(f"Failed Tables: {report['failed_tables']}")
    print(f"Throughput: {report['tables_per_second']:,.0f} rows/s")
    print("="*60)
```

### 2.4 æ•ˆæœè¯„ä¼°

**æ€§èƒ½æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | è¿ç§»å‰(SQLite) | è¿ç§»å(PostgreSQL) | æå‡å¹…åº¦ |
|------|----------------|-------------------|----------|
| å¹¶å‘ç”¨æˆ·æ•° | 50 | 1000+ | 2000% |
| å¹³å‡æŸ¥è¯¢å“åº”æ—¶é—´ | 450ms | 85ms | 81% â†“ |
| æ•°æ®å†™å…¥TPS | 120 | 850 | 608% â†‘ |
| å¤æ‚æŠ¥è¡¨ç”Ÿæˆæ—¶é—´ | 15åˆ†é’Ÿ | 45ç§’ | 95% â†“ |
| å¤‡ä»½æ—¶é—´ | æ‰‹åŠ¨/ä¸å®šæœŸ | è‡ªåŠ¨/15åˆ†é’Ÿ | è‡ªåŠ¨åŒ– |

**ä¸šåŠ¡ä»·å€¼**ï¼š

| ç»´åº¦ | ä»·å€¼æè¿° | é‡åŒ–æ•°æ® |
|------|----------|----------|
| **è¿ç»´æ•ˆç‡** | è‡ªåŠ¨åŒ–è¿ç»´å‡å°‘äººå·¥å¹²é¢„ | è¿ç»´å·¥æ—¶å‡å°‘70% |
| **ç³»ç»Ÿå¯ç”¨æ€§** | ä»99.5%æå‡è‡³99.95% | å¹´åœæœºæ—¶é—´ä»43å°æ—¶é™è‡³4å°æ—¶ |
| **æ•°æ®åˆ†æ** | å®æ—¶åˆ†æèƒ½åŠ›æ”¯æŒä¸šåŠ¡å†³ç­– | æŠ¥è¡¨ç”Ÿæˆæ•ˆç‡æå‡95% |
| **æ‰©å±•æ€§** | æ”¯æŒä¸šåŠ¡å¿«é€Ÿå¢é•¿ | ç”¨æˆ·æ‰¿è½½èƒ½åŠ›æå‡20å€ |
| **æˆæœ¬èŠ‚çº¦** | å‡å°‘ç¡¬ä»¶å’ŒäººåŠ›æˆæœ¬ | å¹´åº¦ITæˆæœ¬é™ä½35% |

**ç»éªŒæ•™è®­**ï¼š

1. **ç±»å‹æ˜ å°„è¦è°¨æ…**ï¼šSQLiteçš„åŠ¨æ€ç±»å‹å¯¼è‡´éƒ¨åˆ†æ•°æ®éœ€è¦ç‰¹æ®Šæ¸…æ´—ï¼Œå»ºè®®è¿ç§»å‰è¿›è¡Œæ•°æ®è´¨é‡åˆ†æ
2. **åˆ†æ‰¹æ¬¡è¿ç§»é™ä½é£é™©**ï¼šå¤§è¡¨åˆ†æ‰¹è¿ç§»å¯å‡å°‘å•æ¬¡å¤±è´¥çš„å½±å“èŒƒå›´ï¼Œå»ºè®®å•æ‰¹æ¬¡ä¸è¶…è¿‡100ä¸‡è¡Œ
3. **æ ¡éªŒå’ŒéªŒè¯å¿…ä¸å¯å°‘**ï¼šMD5æ ¡éªŒå‘ç°äº†0.3%çš„æ•°æ®å·®å¼‚ï¼Œä¸»è¦æºäºæ—¶åŒºå¤„ç†é—®é¢˜
4. **ç´¢å¼•é‡å»ºç­–ç•¥**ï¼šè¿ç§»åéœ€è¦é‡æ–°åˆ†ææŸ¥è¯¢æ¨¡å¼å»ºç«‹åˆé€‚çš„ç´¢å¼•ï¼Œè€Œéç®€å•å¤åˆ¶åŸç´¢å¼•
5. **å›æ»šè®¡åˆ’å¿…é¡»å‡†å¤‡**ï¼šè¿ç§»è¿‡ç¨‹ä¸­é‡åˆ°ç½‘ç»œä¸­æ–­2æ¬¡ï¼Œå›æ»šæœºåˆ¶ç¡®ä¿äº†ä¸šåŠ¡è¿ç»­æ€§

**ROIåˆ†æ**ï¼š
- é¡¹ç›®æ€»æŠ•èµ„ï¼š45ä¸‡å…ƒï¼ˆå¼€å‘30ä¸‡+ç¡¬ä»¶15ä¸‡ï¼‰
- å¹´åº¦èŠ‚çº¦ï¼š78ä¸‡å…ƒï¼ˆäººåŠ›52ä¸‡+ç¡¬ä»¶26ä¸‡ï¼‰
- æŠ•èµ„å›æ”¶æœŸï¼š7ä¸ªæœˆ
- 3å¹´å‡€ç°å€¼ï¼ˆNPVï¼‰ï¼š189ä¸‡å…ƒ

---

## 3. æ¡ˆä¾‹2ï¼šSchemaç‰ˆæœ¬ç®¡ç†

### 3.1 ä¸šåŠ¡èƒŒæ™¯

**ä¼ä¸šèƒŒæ™¯**ï¼š
- **å…¬å¸åç§°**ï¼šé‡‘èæ•°æ®æœåŠ¡æœ‰é™å…¬å¸ï¼ˆFinData Corpï¼‰
- **è¡Œä¸šé¢†åŸŸ**ï¼šé‡‘èç§‘æŠ€/æ•°æ®æœåŠ¡
- **å…¬å¸è§„æ¨¡**ï¼š200+å¼€å‘äººå‘˜ï¼Œç®¡ç†50+æ•°æ®åº“å®ä¾‹
- **ä¸šåŠ¡ç‰¹ç‚¹**ï¼šé«˜åº¦ç›‘ç®¡è¡Œä¸šï¼ŒSchemaå˜æ›´éœ€è¦å®¡è®¡è¿½è¸ª

**ä¸šåŠ¡ç—›ç‚¹**ï¼š
1. **å˜æ›´æ··ä¹±**ï¼šå¤šä¸ªå›¢é˜ŸåŒæ—¶ä¿®æ”¹Schemaï¼Œç»å¸¸å‡ºç°å†²çªå’Œè¦†ç›–
2. **å›æ»šå›°éš¾**ï¼šç”Ÿäº§ç¯å¢ƒSchemaå˜æ›´å¤±è´¥åï¼Œå›æ»šéœ€è¦æ•°å°æ—¶ç”šè‡³æ•°å¤©
3. **ç¯å¢ƒä¸ä¸€è‡´**ï¼šå¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ç¯å¢ƒçš„Schemaç‰ˆæœ¬ä¸åŒæ­¥
4. **å®¡è®¡ç¼ºå¤±**ï¼šæ— æ³•æ»¡è¶³é‡‘èç›‘ç®¡å¯¹Schemaå˜æ›´çš„å®¡è®¡è¦æ±‚
5. **åä½œä½æ•ˆ**ï¼šDBAå’Œå¼€å‘å›¢é˜Ÿé€šè¿‡é‚®ä»¶æ²Ÿé€šå˜æ›´ï¼Œæ•ˆç‡ä½ä¸‹ä¸”å®¹æ˜“å‡ºé”™

**ä¸šåŠ¡ç›®æ ‡**ï¼š
1. å»ºç«‹ç»Ÿä¸€çš„Schemaç‰ˆæœ¬ç®¡ç†æœºåˆ¶
2. å®ç°Schemaå˜æ›´çš„å¯è¿½æº¯å’Œå¯å›æ»š
3. è‡ªåŠ¨åŒ–å¤šç¯å¢ƒSchemaåŒæ­¥
4. æ»¡è¶³é‡‘èç›‘ç®¡åˆè§„è¦æ±‚ï¼ˆSOXã€PCI-DSSï¼‰
5. å°†Schemaå˜æ›´æ—¶é—´ä»å¹³å‡2å¤©ç¼©çŸ­åˆ°2å°æ—¶

### 3.2 æŠ€æœ¯æŒ‘æˆ˜

| æŒ‘æˆ˜ç‚¹ | æè¿° | å½±å“çº§åˆ« |
|--------|------|----------|
| ç‰ˆæœ¬å†²çªè§£å†³ | å¤šåˆ†æ”¯å¹¶è¡Œå¼€å‘æ—¶çš„Schemaåˆå¹¶ | é«˜ |
| æ•°æ®è¿ç§»è„šæœ¬ | ç»“æ„å˜æ›´ä¼´éšçš„æ•°æ®è½¬æ¢ | é«˜ |
| é›¶åœæœºéƒ¨ç½² | åœ¨çº¿ç³»ç»Ÿçš„çƒ­æ›´æ–°ç­–ç•¥ | é«˜ |
| å›æ»šæœºåˆ¶ | å¤±è´¥åçš„å¿«é€Ÿæ¢å¤èƒ½åŠ› | é«˜ |
| å¤šæ•°æ®åº“æ”¯æŒ | MySQLã€PostgreSQLã€Oracleç»Ÿä¸€ç®¡ç† | ä¸­ |

### 3.3 å®Œæ•´ä»£ç å®ç°

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Schema Version Management System
ä¼ä¸šçº§æ•°æ®åº“Schemaç‰ˆæœ¬ç®¡ç†è§£å†³æ–¹æ¡ˆ
"""

import os
import re
import hashlib
import json
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Callable
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import logging
import sqlite3
from contextlib import contextmanager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MigrationType(Enum):
    """è¿ç§»ç±»å‹"""
    SCHEMA = "schema"      # ç»“æ„å˜æ›´
    DATA = "data"          # æ•°æ®å˜æ›´
    INDEX = "index"        # ç´¢å¼•å˜æ›´
    SEED = "seed"          # ç§å­æ•°æ®


class MigrationStatus(Enum):
    """è¿ç§»çŠ¶æ€"""
    PENDING = "pending"
    APPLIED = "applied"
    FAILED = "failed"
    ROLLED_BACK = "rolled_back"


@dataclass
class Migration:
    """è¿ç§»è®°å½•æ•°æ®ç±»"""
    version: str
    name: str
    type: MigrationType
    author: str
    created_at: datetime
    checksum: str
    sql_up: str
    sql_down: str
    status: MigrationStatus = MigrationStatus.PENDING
    applied_at: Optional[datetime] = None
    execution_time_ms: Optional[int] = None
    error_message: Optional[str] = None
    
    def to_dict(self) -> Dict:
        return {
            'version': self.version,
            'name': self.name,
            'type': self.type.value,
            'author': self.author,
            'created_at': self.created_at.isoformat(),
            'checksum': self.checksum,
            'status': self.status.value,
            'applied_at': self.applied_at.isoformat() if self.applied_at else None,
            'execution_time_ms': self.execution_time_ms,
            'error_message': self.error_message
        }


class SchemaVersionManager:
    """Schemaç‰ˆæœ¬ç®¡ç†å™¨"""
    
    def __init__(self, db_connection_string: str, migrations_dir: str = 'migrations'):
        self.db_connection_string = db_connection_string
        self.migrations_dir = Path(migrations_dir)
        self.migrations_dir.mkdir(exist_ok=True)
        self._init_schema_table()
    
    def _init_schema_table(self) -> None:
        """åˆå§‹åŒ–ç‰ˆæœ¬æ§åˆ¶è¡¨"""
        sql = """
        CREATE TABLE IF NOT EXISTS schema_migrations (
            version VARCHAR(20) PRIMARY KEY,
            name VARCHAR(255) NOT NULL,
            type VARCHAR(20) NOT NULL,
            author VARCHAR(100) NOT NULL,
            created_at TIMESTAMP NOT NULL,
            checksum VARCHAR(64) NOT NULL,
            sql_up TEXT NOT NULL,
            sql_down TEXT NOT NULL,
            status VARCHAR(20) DEFAULT 'pending',
            applied_at TIMESTAMP,
            execution_time_ms INTEGER,
            error_message TEXT
        );
        
        CREATE INDEX IF NOT EXISTS idx_schema_migrations_status 
        ON schema_migrations(status);
        
        CREATE INDEX IF NOT EXISTS idx_schema_migrations_applied_at 
        ON schema_migrations(applied_at);
        """
        self._execute_sql(sql)
    
    def _execute_sql(self, sql: str) -> None:
        """æ‰§è¡ŒSQLè¯­å¥"""
        conn = sqlite3.connect(self.db_connection_string)
        cursor = conn.cursor()
        try:
            cursor.executescript(sql)
            conn.commit()
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            conn.close()
    
    def create_migration(self, name: str, mig_type: MigrationType, 
                         author: str, sql_up: str, sql_down: str) -> Migration:
        """åˆ›å»ºæ–°çš„è¿ç§»æ–‡ä»¶"""
        # ç”Ÿæˆç‰ˆæœ¬å·ï¼šYYYYMMDD_HHMMSS
        version = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # è®¡ç®—æ ¡éªŒå’Œ
        content = f"{sql_up}{sql_down}"
        checksum = hashlib.sha256(content.encode()).hexdigest()
        
        migration = Migration(
            version=version,
            name=name,
            type=mig_type,
            author=author,
            created_at=datetime.now(),
            checksum=checksum,
            sql_up=sql_up,
            sql_down=sql_down,
            status=MigrationStatus.PENDING
        )
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        self._save_migration_file(migration)
        
        # è®°å½•åˆ°æ•°æ®åº“
        self._record_migration(migration)
        
        logger.info(f"Created migration: {version}_{name}")
        return migration
    
    def _save_migration_file(self, migration: Migration) -> None:
        """ä¿å­˜è¿ç§»æ–‡ä»¶"""
        filename = f"{migration.version}_{migration.name}.sql"
        filepath = self.migrations_dir / filename
        
        content = f"""-- Migration: {migration.name}
-- Version: {migration.version}
-- Type: {migration.type.value}
-- Author: {migration.author}
-- Created: {migration.created_at.isoformat()}
-- Checksum: {migration.checksum}

-- UP (Apply changes)
{migration.sql_up}

-- DOWN (Rollback changes)
{migration.sql_down}
"""
        filepath.write_text(content, encoding='utf-8')
    
    def _record_migration(self, migration: Migration) -> None:
        """è®°å½•è¿ç§»åˆ°æ•°æ®åº“"""
        sql = """
        INSERT OR REPLACE INTO schema_migrations 
        (version, name, type, author, created_at, checksum, sql_up, sql_down, status)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """
        conn = sqlite3.connect(self.db_connection_string)
        cursor = conn.cursor()
        cursor.execute(sql, (
            migration.version, migration.name, migration.type.value,
            migration.author, migration.created_at, migration.checksum,
            migration.sql_up, migration.sql_down, migration.status.value
        ))
        conn.commit()
        conn.close()
    
    def get_pending_migrations(self) -> List[Migration]:
        """è·å–å¾…æ‰§è¡Œçš„è¿ç§»"""
        sql = """
        SELECT * FROM schema_migrations 
        WHERE status = 'pending' 
        ORDER BY version ASC
        """
        return self._query_migrations(sql)
    
    def get_applied_migrations(self) -> List[Migration]:
        """è·å–å·²åº”ç”¨çš„è¿ç§»"""
        sql = """
        SELECT * FROM schema_migrations 
        WHERE status = 'applied' 
        ORDER BY applied_at DESC
        """
        return self._query_migrations(sql)
    
    def _query_migrations(self, sql: str) -> List[Migration]:
        """æŸ¥è¯¢è¿ç§»è®°å½•"""
        conn = sqlite3.connect(self.db_connection_string)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute(sql)
        rows = cursor.fetchall()
        conn.close()
        
        migrations = []
        for row in rows:
            migrations.append(Migration(
                version=row['version'],
                name=row['name'],
                type=MigrationType(row['type']),
                author=row['author'],
                created_at=datetime.fromisoformat(row['created_at']),
                checksum=row['checksum'],
                sql_up=row['sql_up'],
                sql_down=row['sql_down'],
                status=MigrationStatus(row['status']),
                applied_at=datetime.fromisoformat(row['applied_at']) if row['applied_at'] else None,
                execution_time_ms=row['execution_time_ms'],
                error_message=row['error_message']
            ))
        return migrations
    
    def apply_migration(self, version: str, dry_run: bool = False) -> Tuple[bool, str]:
        """åº”ç”¨æŒ‡å®šç‰ˆæœ¬çš„è¿ç§»"""
        # è·å–è¿ç§»è¯¦æƒ…
        conn = sqlite3.connect(self.db_connection_string)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM schema_migrations WHERE version = ?", (version,))
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            return False, f"Migration {version} not found"
        
        if row['status'] == 'applied':
            return True, f"Migration {version} already applied"
        
        sql_up = row['sql_up']
        
        if dry_run:
            logger.info(f"[DRY RUN] Would execute:\n{sql_up}")
            return True, "Dry run completed"
        
        start_time = datetime.now()
        try:
            self._execute_sql(sql_up)
            
            execution_time = int((datetime.now() - start_time).total_seconds() * 1000)
            
            # æ›´æ–°çŠ¶æ€
            conn = sqlite3.connect(self.db_connection_string)
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE schema_migrations 
                SET status = 'applied', applied_at = ?, execution_time_ms = ?
                WHERE version = ?
            """, (datetime.now().isoformat(), execution_time, version))
            conn.commit()
            conn.close()
            
            logger.info(f"Applied migration {version} in {execution_time}ms")
            return True, f"Applied successfully in {execution_time}ms"
            
        except Exception as e:
            error_msg = str(e)
            conn = sqlite3.connect(self.db_connection_string)
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE schema_migrations 
                SET status = 'failed', error_message = ?
                WHERE version = ?
            """, (error_msg, version))
            conn.commit()
            conn.close()
            
            logger.error(f"Failed to apply migration {version}: {error_msg}")
            return False, error_msg
    
    def rollback_migration(self, version: str, dry_run: bool = False) -> Tuple[bool, str]:
        """å›æ»šæŒ‡å®šç‰ˆæœ¬çš„è¿ç§»"""
        conn = sqlite3.connect(self.db_connection_string)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM schema_migrations WHERE version = ?", (version,))
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            return False, f"Migration {version} not found"
        
        if row['status'] != 'applied':
            return False, f"Migration {version} is not in applied status"
        
        sql_down = row['sql_down']
        
        if dry_run:
            logger.info(f"[DRY RUN] Would execute:\n{sql_down}")
            return True, "Dry run completed"
        
        try:
            self._execute_sql(sql_down)
            
            conn = sqlite3.connect(self.db_connection_string)
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE schema_migrations 
                SET status = 'rolled_back', applied_at = NULL
                WHERE version = ?
            """, (version,))
            conn.commit()
            conn.close()
            
            logger.info(f"Rolled back migration {version}")
            return True, "Rolled back successfully"
            
        except Exception as e:
            logger.error(f"Failed to rollback migration {version}: {e}")
            return False, str(e)
    
    def migrate_up(self, target_version: Optional[str] = None, 
                   dry_run: bool = False) -> Dict:
        """æ‰§è¡Œæ‰€æœ‰å¾…å¤„ç†çš„è¿ç§»"""
        pending = self.get_pending_migrations()
        
        if target_version:
            pending = [m for m in pending if m.version <= target_version]
        
        results = {
            'total': len(pending),
            'successful': 0,
            'failed': 0,
            'migrations': []
        }
        
        for migration in pending:
            success, message = self.apply_migration(migration.version, dry_run)
            results['migrations'].append({
                'version': migration.version,
                'name': migration.name,
                'success': success,
                'message': message
            })
            
            if success:
                results['successful'] += 1
            else:
                results['failed'] += 1
                if not dry_run:
                    break  # å¤±è´¥ååœæ­¢
        
        return results
    
    def migrate_down(self, steps: int = 1, dry_run: bool = False) -> Dict:
        """å›æ»šæœ€è¿‘çš„Nä¸ªè¿ç§»"""
        applied = self.get_applied_migrations()
        to_rollback = applied[:steps]
        
        results = {
            'total': len(to_rollback),
            'successful': 0,
            'failed': 0,
            'migrations': []
        }
        
        for migration in to_rollback:
            success, message = self.rollback_migration(migration.version, dry_run)
            results['migrations'].append({
                'version': migration.version,
                'name': migration.name,
                'success': success,
                'message': message
            })
            
            if success:
                results['successful'] += 1
            else:
                results['failed'] += 1
        
        return results
    
    def verify_checksums(self) -> List[Dict]:
        """éªŒè¯æ‰€æœ‰å·²åº”ç”¨è¿ç§»çš„æ ¡éªŒå’Œ"""
        applied = self.get_applied_migrations()
        mismatches = []
        
        for migration in applied:
            filepath = self.migrations_dir / f"{migration.version}_{migration.name}.sql"
            if not filepath.exists():
                mismatches.append({
                    'version': migration.version,
                    'issue': 'File not found'
                })
                continue
            
            content = filepath.read_text(encoding='utf-8')
            # æå–SQLéƒ¨åˆ†é‡æ–°è®¡ç®—æ ¡éªŒå’Œ
            match = re.search(r'-- UP.*?(-- DOWN|$)', content, re.DOTALL)
            if match:
                sql_up = match.group(0)
                match_down = re.search(r'-- DOWN.*', content, re.DOTALL)
                sql_down = match_down.group(0) if match_down else ''
                current_checksum = hashlib.sha256(
                    f"{sql_up}{sql_down}".encode()
                ).hexdigest()
                
                if current_checksum != migration.checksum:
                    mismatches.append({
                        'version': migration.version,
                        'issue': 'Checksum mismatch',
                        'expected': migration.checksum,
                        'actual': current_checksum
                    })
        
        return mismatches
    
    def generate_report(self) -> Dict:
        """ç”Ÿæˆè¿ç§»æŠ¥å‘Š"""
        all_migrations = self.get_applied_migrations() + self.get_pending_migrations()
        
        return {
            'summary': {
                'total_migrations': len(all_migrations),
                'applied': len(self.get_applied_migrations()),
                'pending': len(self.get_pending_migrations()),
                'failed': len([m for m in all_migrations if m.status == MigrationStatus.FAILED]),
                'rolled_back': len([m for m in all_migrations if m.status == MigrationStatus.ROLLED_BACK])
            },
            'checksum_status': 'VALID' if not self.verify_checksums() else 'INVALID',
            'migrations': [m.to_dict() for m in all_migrations]
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = SchemaVersionManager('schema_version.db', 'migrations')
    
    # åˆ›å»ºç¤ºä¾‹è¿ç§»ï¼šæ·»åŠ ç”¨æˆ·è¡¨
    migration1 = manager.create_migration(
        name='create_users_table',
        mig_type=MigrationType.SCHEMA,
        author='dev_team',
        sql_up="""
            CREATE TABLE users (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                username VARCHAR(50) NOT NULL UNIQUE,
                email VARCHAR(100) NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            CREATE INDEX idx_users_email ON users(email);
        """,
        sql_down="""
            DROP INDEX IF EXISTS idx_users_email;
            DROP TABLE IF EXISTS users;
        """
    )
    
    # åˆ›å»ºç¤ºä¾‹è¿ç§»ï¼šæ·»åŠ è®¢å•è¡¨
    migration2 = manager.create_migration(
        name='create_orders_table',
        mig_type=MigrationType.SCHEMA,
        author='dev_team',
        sql_up="""
            CREATE TABLE orders (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER NOT NULL,
                total_amount DECIMAL(10, 2) NOT NULL,
                status VARCHAR(20) DEFAULT 'pending',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (user_id) REFERENCES users(id)
            );
            CREATE INDEX idx_orders_user ON orders(user_id);
        """,
        sql_down="""
            DROP INDEX IF EXISTS idx_orders_user;
            DROP TABLE IF EXISTS orders;
        """
    )
    
    # æ‰§è¡Œè¿ç§»
    print("\n=== Applying migrations ===")
    result = manager.migrate_up(dry_run=False)
    print(f"Total: {result['total']}, Successful: {result['successful']}, Failed: {result['failed']}")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n=== Migration Report ===")
    report = manager.generate_report()
    print(json.dumps(report['summary'], indent=2))
```

### 3.4 æ•ˆæœè¯„ä¼°

**æ€§èƒ½æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | å®æ–½å‰ | å®æ–½å | æå‡å¹…åº¦ |
|------|--------|--------|----------|
| Schemaå˜æ›´éƒ¨ç½²æ—¶é—´ | 2å¤© | 45åˆ†é’Ÿ | 96% â†“ |
| å˜æ›´å¤±è´¥ç‡ | 15% | 2% | 87% â†“ |
| å›æ»šæ—¶é—´ | 4-8å°æ—¶ | 5åˆ†é’Ÿ | 98% â†“ |
| ç¯å¢ƒä¸ä¸€è‡´é—®é¢˜ | æ¯æœˆ5-10æ¬¡ | 0æ¬¡ | 100% â†“ |
| å®¡è®¡å‡†å¤‡æ—¶é—´ | 3å¤© | å³æ—¶ç”Ÿæˆ | 100% â†“ |

**ä¸šåŠ¡ä»·å€¼**ï¼š

| ç»´åº¦ | ä»·å€¼æè¿° | é‡åŒ–æ•°æ® |
|------|----------|----------|
| **åˆè§„æ€§** | æ»¡è¶³SOXã€PCI-DSSå®¡è®¡è¦æ±‚ | å®¡è®¡é€šè¿‡100% |
| **åä½œæ•ˆç‡** | å¼€å‘-DBAåä½œæµç¨‹æ ‡å‡†åŒ– | æ²Ÿé€šæˆæœ¬é™ä½60% |
| **é£é™©æ§åˆ¶** | å˜æ›´é£é™©å¯è§†åŒ–å’Œå¯è¿½æº¯ | ç”Ÿäº§äº‹æ•…å‡å°‘80% |
| **éƒ¨ç½²é¢‘ç‡** | æ”¯æŒæŒç»­äº¤ä»˜å’Œå¿«é€Ÿè¿­ä»£ | éƒ¨ç½²é¢‘ç‡æå‡5å€ |
| **çŸ¥è¯†ç®¡ç†** | Schemaå˜æ›´å†å²å®Œæ•´ä¿ç•™ | å›¢é˜ŸçŸ¥è¯†æ²‰æ·€ |

**ç»éªŒæ•™è®­**ï¼š

1. **è¿ç§»ç²’åº¦æ§åˆ¶**ï¼šåˆæœŸå°†å¤šä¸ªå˜æ›´åˆå¹¶åœ¨ä¸€ä¸ªè¿ç§»ä¸­ï¼Œå¯¼è‡´å›æ»šå›°éš¾ã€‚å»ºè®®æ¯ä¸ªè¿ç§»åªåŒ…å«ä¸€ä¸ªç‹¬ç«‹å˜æ›´
2. **æ•°æ®è¿ç§»ç­–ç•¥**ï¼šç»“æ„å˜æ›´ä¼´éšçš„æ•°æ®è½¬æ¢éœ€è¦é¢å¤–æµ‹è¯•ï¼Œå»ºè®®åœ¨æµ‹è¯•ç¯å¢ƒä½¿ç”¨ç”Ÿäº§æ•°æ®é‡çš„1%è¿›è¡ŒéªŒè¯
3. **å›¢é˜Ÿåä½œè§„èŒƒ**ï¼šå¿…é¡»å»ºç«‹åˆ†æ”¯ç®¡ç†ç­–ç•¥ï¼Œé¿å…å¤šäººåŒæ—¶ä¿®æ”¹åŒä¸€è¡¨çš„å†²çª
4. **ç›‘æ§å‘Šè­¦**ï¼šéœ€è¦ç›‘æ§é•¿æ—¶é—´è¿è¡Œçš„è¿ç§»ï¼Œè®¾ç½®è¶…æ—¶å‘Šè­¦ï¼ˆå»ºè®®DDLæ“ä½œè¶…è¿‡5åˆ†é’Ÿå‘Šè­¦ï¼‰
5. **æƒé™æ§åˆ¶**ï¼šç”Ÿäº§ç¯å¢ƒå˜æ›´éœ€è¦åŒäººå¤æ ¸æœºåˆ¶ï¼Œç³»ç»Ÿå±‚é¢å®ç°å®¡æ‰¹æµç¨‹é›†æˆ

**åˆè§„æ€§æå‡**ï¼š
- å®¡è®¡æ—¥å¿—å®Œæ•´åº¦ï¼šä»60%æå‡è‡³100%
- å˜æ›´å¯è¿½æº¯æ€§ï¼š100%å¯è¿½æº¯åˆ°å…·ä½“å¼€å‘äººå‘˜å’Œå®¡æ‰¹è®°å½•
- ç›‘ç®¡æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼šä»3å¤©ç¼©çŸ­è‡³å®æ—¶ç”Ÿæˆ

---

## 4. æ¡ˆä¾‹3ï¼šæ•°æ®åº“Schemaè‡ªåŠ¨ç”Ÿæˆ

### 4.1 ä¸šåŠ¡èƒŒæ™¯

**ä¼ä¸šèƒŒæ™¯**ï¼š
- **å…¬å¸åç§°**ï¼šæ™ºæ…§ç‰©æµç§‘æŠ€æœ‰é™å…¬å¸ï¼ˆSmartLogisticsï¼‰
- **è¡Œä¸šé¢†åŸŸï¼šç‰©æµ/ä¾›åº”é“¾ç®¡ç†
- **å…¬å¸è§„æ¨¡**ï¼šæœåŠ¡300+ç‰©æµä¼ä¸šï¼Œæ—¥å¤„ç†è®¢å•500ä¸‡+
- **æŠ€æœ¯æ¶æ„**ï¼šå¾®æœåŠ¡æ¶æ„ï¼Œ50+ç‹¬ç«‹æœåŠ¡

**ä¸šåŠ¡ç—›ç‚¹**ï¼š
1. **é‡å¤åŠ³åŠ¨**ï¼šæ¯ä¸ªå¾®æœåŠ¡éƒ½éœ€è¦æ‰‹åŠ¨ç¼–å†™DDLå’ŒORMæ¨¡å‹ï¼Œå¼€å‘æ•ˆç‡ä½
2. **ä¸ä¸€è‡´æ€§**ï¼šä¸åŒæœåŠ¡çš„æ•°æ®åº“è®¾è®¡è§„èŒƒä¸ç»Ÿä¸€ï¼Œå­—æ®µå‘½åæ··ä¹±
3. **æ–‡æ¡£æ»å**ï¼šæ•°æ®åº“æ–‡æ¡£ç»å¸¸ä¸å®é™…Schemaä¸ä¸€è‡´ï¼Œç»´æŠ¤å›°éš¾
4. **è·¨æœåŠ¡è”è¡¨**ï¼šç¼ºä¹ç»Ÿä¸€çš„Schemaè§†å›¾ï¼Œè·¨æœåŠ¡æŸ¥è¯¢éœ€è¦å¤§é‡æ²Ÿé€šæˆæœ¬
5. **æ–°äººä¸Šæ‰‹**ï¼šæ–°å‘˜å·¥éœ€è¦å¤§é‡æ—¶é—´ç†Ÿæ‚‰å„æœåŠ¡çš„æ•°æ®åº“ç»“æ„

**ä¸šåŠ¡ç›®æ ‡**ï¼š
1. ä»ç»Ÿä¸€çš„Schemaå®šä¹‰è‡ªåŠ¨ç”Ÿæˆå¤šæ•°æ®åº“çš„DDL
2. è‡ªåŠ¨ç”Ÿæˆå„è¯­è¨€çš„ORMä»£ç ï¼ˆPython/SQLAlchemyã€Java/JPAã€Go/GORMï¼‰
3. è‡ªåŠ¨ç”ŸæˆAPIæ–‡æ¡£å’Œæ•°æ®å­—å…¸
4. å»ºç«‹ä¼ä¸šçº§æ•°æ®åº“è®¾è®¡è§„èŒƒå¹¶å¼ºåˆ¶æ‰§è¡Œ
5. å°†æ•°æ®åº“è®¾è®¡æ—¶é—´ä»3å¤©ç¼©çŸ­åˆ°30åˆ†é’Ÿ

### 4.2 æŠ€æœ¯æŒ‘æˆ˜

| æŒ‘æˆ˜ç‚¹ | æè¿° | å½±å“çº§åˆ« |
|--------|------|----------|
| å¤šæ•°æ®åº“å…¼å®¹ | åŒæ—¶æ”¯æŒMySQLã€PostgreSQLã€Oracleçš„DDLç”Ÿæˆ | é«˜ |
| ORMä»£ç ç”Ÿæˆ | ç”Ÿæˆç¬¦åˆå„è¯­è¨€ä¹ æƒ¯çš„ORMä»£ç  | é«˜ |
| å¤æ‚å…³ç³»æ˜ å°„ | å¤šå¯¹å¤šå…³ç³»ã€ç»§æ‰¿ã€å¤åˆä¸»é”®çš„å¤„ç† | é«˜ |
| è§„èŒƒå¼ºåˆ¶æ‰§è¡Œ | è‡ªåŠ¨æ£€æŸ¥å‘½åè§„èŒƒå’Œæœ€ä½³å®è·µ | ä¸­ |
| å¢é‡æ›´æ–° | å·²æœ‰æ•°æ®åº“çš„Schemaå˜æ›´è„šæœ¬ç”Ÿæˆ | ä¸­ |

### 4.3 å®Œæ•´ä»£ç å®ç°

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Database Schema Auto-Generator
ä¼ä¸šçº§æ•°æ®åº“Schemaè‡ªåŠ¨ç”Ÿæˆè§£å†³æ–¹æ¡ˆ
"""

import json
import yaml
from typing import Dict, List, Optional, Union, Any
from dataclasses import dataclass, field, asdict
from enum import Enum
from pathlib import Path
import re
from datetime import datetime


class DataType(Enum):
    """é€šç”¨æ•°æ®ç±»å‹"""
    STRING = "string"
    INTEGER = "integer"
    BIGINT = "bigint"
    DECIMAL = "decimal"
    BOOLEAN = "boolean"
    DATETIME = "datetime"
    DATE = "date"
    TEXT = "text"
    JSON = "json"
    BLOB = "blob"


class RelationType(Enum):
    """å…³ç³»ç±»å‹"""
    ONE_TO_ONE = "one_to_one"
    ONE_TO_MANY = "one_to_many"
    MANY_TO_MANY = "many_to_many"


@dataclass
class Column:
    """åˆ—å®šä¹‰"""
    name: str
    type: DataType
    length: Optional[int] = None
    precision: Optional[int] = None
    scale: Optional[int] = None
    nullable: bool = True
    default: Optional[Any] = None
    primary_key: bool = False
    auto_increment: bool = False
    unique: bool = False
    index: bool = False
    comment: str = ""
    
    def to_dict(self) -> Dict:
        return {
            'name': self.name,
            'type': self.type.value,
            'length': self.length,
            'precision': self.precision,
            'scale': self.scale,
            'nullable': self.nullable,
            'default': self.default,
            'primary_key': self.primary_key,
            'auto_increment': self.auto_increment,
            'unique': self.unique,
            'index': self.index,
            'comment': self.comment
        }


@dataclass
class ForeignKey:
    """å¤–é”®å®šä¹‰"""
    name: str
    column: str
    ref_table: str
    ref_column: str
    on_delete: str = "RESTRICT"
    on_update: str = "CASCADE"


@dataclass
class Relation:
    """å…³ç³»å®šä¹‰"""
    name: str
    type: RelationType
    target_table: str
    source_column: str
    target_column: str
    join_table: Optional[str] = None  # ç”¨äºå¤šå¯¹å¤š


@dataclass
class Table:
    """è¡¨å®šä¹‰"""
    name: str
    comment: str = ""
    columns: List[Column] = field(default_factory=list)
    foreign_keys: List[ForeignKey] = field(default_factory=list)
    relations: List[Relation] = field(default_factory=list)
    indexes: List[Dict] = field(default_factory=list)
    
    def get_primary_key(self) -> Optional[Column]:
        for col in self.columns:
            if col.primary_key:
                return col
        return None
    
    def to_dict(self) -> Dict:
        return {
            'name': self.name,
            'comment': self.comment,
            'columns': [c.to_dict() for c in self.columns],
            'foreign_keys': [asdict(fk) for fk in self.foreign_keys],
            'relations': [asdict(r) for r in self.relations],
            'indexes': self.indexes
        }


@dataclass
class Schema:
    """Schemaå®šä¹‰"""
    name: str
    version: str
    description: str
    tables: List[Table] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        return {
            'name': self.name,
            'version': self.version,
            'description': self.description,
            'tables': [t.to_dict() for t in self.tables]
        }


class SchemaValidator:
    """SchemaéªŒè¯å™¨"""
    
    # å‘½åè§„èŒƒ
    NAMING_RULES = {
        'table': r'^[a-z][a-z0-9_]*$',
        'column': r'^[a-z][a-z0-9_]*$',
        'index': r'^idx_[a-z][a-z0-9_]*$',
        'foreign_key': r'^fk_[a-z][a-z0-9_]*$'
    }
    
    # ä¿ç•™å­—åˆ—è¡¨
    RESERVED_WORDS = {'select', 'from', 'where', 'order', 'group', 'table', 'column'}
    
    def __init__(self):
        self.errors: List[str] = []
        self.warnings: List[str] = []
    
    def validate(self, schema: Schema) -> bool:
        """éªŒè¯æ•´ä¸ªSchema"""
        self.errors = []
        self.warnings = []
        
        table_names = set()
        for table in schema.tables:
            self._validate_table(table, table_names)
            table_names.add(table.name)
        
        return len(self.errors) == 0
    
    def _validate_table(self, table: Table, existing_names: set) -> None:
        """éªŒè¯è¡¨å®šä¹‰"""
        # æ£€æŸ¥å‘½åè§„èŒƒ
        if not re.match(self.NAMING_RULES['table'], table.name):
            self.errors.append(f"Table '{table.name}': name violates naming convention")
        
        if table.name in self.RESERVED_WORDS:
            self.errors.append(f"Table '{table.name}': name is a reserved word")
        
        if table.name in existing_names:
            self.errors.append(f"Table '{table.name}': duplicate table name")
        
        # æ£€æŸ¥ä¸»é”®
        pk_count = sum(1 for col in table.columns if col.primary_key)
        if pk_count == 0:
            self.warnings.append(f"Table '{table.name}': missing primary key")
        elif pk_count > 1:
            self.errors.append(f"Table '{table.name}': multiple primary keys not allowed")
        
        # éªŒè¯åˆ—
        column_names = set()
        for col in table.columns:
            self._validate_column(col, table.name, column_names)
            column_names.add(col.name)
    
    def _validate_column(self, col: Column, table_name: str, existing_names: set) -> None:
        """éªŒè¯åˆ—å®šä¹‰"""
        if not re.match(self.NAMING_RULES['column'], col.name):
            self.errors.append(f"Column '{table_name}.{col.name}': name violates naming convention")
        
        if col.name in self.RESERVED_WORDS:
            self.errors.append(f"Column '{table_name}.{col.name}': name is a reserved word")
        
        if col.name in existing_names:
            self.errors.append(f"Column '{table_name}.{col.name}': duplicate column name")
        
        if col.primary_key and col.nullable:
            self.errors.append(f"Column '{table_name}.{col.name}': primary key cannot be nullable")
        
        if col.auto_increment and not col.primary_key:
            self.errors.append(f"Column '{table_name}.{col.name}': auto_increment must be primary key")


class DDLGenerator:
    """DDLç”Ÿæˆå™¨"""
    
    # æ•°æ®åº“ç±»å‹æ˜ å°„
    TYPE_MAPPINGS = {
        'mysql': {
            DataType.STRING: 'VARCHAR',
            DataType.INTEGER: 'INT',
            DataType.BIGINT: 'BIGINT',
            DataType.DECIMAL: 'DECIMAL',
            DataType.BOOLEAN: 'TINYINT(1)',
            DataType.DATETIME: 'DATETIME',
            DataType.DATE: 'DATE',
            DataType.TEXT: 'TEXT',
            DataType.JSON: 'JSON',
            DataType.BLOB: 'BLOB'
        },
        'postgresql': {
            DataType.STRING: 'VARCHAR',
            DataType.INTEGER: 'INTEGER',
            DataType.BIGINT: 'BIGINT',
            DataType.DECIMAL: 'DECIMAL',
            DataType.BOOLEAN: 'BOOLEAN',
            DataType.DATETIME: 'TIMESTAMP',
            DataType.DATE: 'DATE',
            DataType.TEXT: 'TEXT',
            DataType.JSON: 'JSONB',
            DataType.BLOB: 'BYTEA'
        },
        'oracle': {
            DataType.STRING: 'VARCHAR2',
            DataType.INTEGER: 'NUMBER',
            DataType.BIGINT: 'NUMBER',
            DataType.DECIMAL: 'NUMBER',
            DataType.BOOLEAN: 'NUMBER(1)',
            DataType.DATETIME: 'TIMESTAMP',
            DataType.DATE: 'DATE',
            DataType.TEXT: 'CLOB',
            DataType.JSON: 'CLOB',
            DataType.BLOB: 'BLOB'
        }
    }
    
    def __init__(self, db_type: str = 'postgresql'):
        self.db_type = db_type.lower()
        self.type_mapping = self.TYPE_MAPPINGS.get(self.db_type, self.TYPE_MAPPINGS['postgresql'])
    
    def generate_column_sql(self, col: Column) -> str:
        """ç”Ÿæˆåˆ—SQL"""
        parts = [f"    {col.name}"]
        
        # ç±»å‹
        db_type = self.type_mapping[col.type]
        if col.length and col.type in (DataType.STRING,):
            parts.append(f"{db_type}({col.length})")
        elif col.precision is not None and col.type == DataType.DECIMAL:
            scale = col.scale or 0
            parts.append(f"{db_type}({col.precision},{scale})")
        else:
            parts.append(db_type)
        
        # è‡ªå¢
        if col.auto_increment:
            if self.db_type == 'mysql':
                parts.append("AUTO_INCREMENT")
            elif self.db_type == 'postgresql':
                parts[1] = "SERIAL"
            elif self.db_type == 'oracle':
                pass  # Oracleä½¿ç”¨sequence
        
        # å¯ç©ºæ€§
        if not col.nullable:
            parts.append("NOT NULL")
        
        # é»˜è®¤å€¼
        if col.default is not None:
            if isinstance(col.default, str):
                parts.append(f"DEFAULT '{col.default}'")
            else:
                parts.append(f"DEFAULT {col.default}")
        
        # æ³¨é‡Š
        if col.comment:
            if self.db_type == 'postgresql':
                pass  # å•ç‹¬å¤„ç†
            elif self.db_type == 'mysql':
                parts.append(f"COMMENT '{col.comment}'")
        
        return ' '.join(parts)
    
    def generate_table_sql(self, table: Table) -> str:
        """ç”Ÿæˆè¡¨SQL"""
        lines = [f"CREATE TABLE {table.name} ("]
        
        # åˆ—å®šä¹‰
        column_defs = []
        primary_keys = []
        
        for col in table.columns:
            column_defs.append(self.generate_column_sql(col))
            if col.primary_key:
                primary_keys.append(col.name)
        
        # ä¸»é”®çº¦æŸ
        if primary_keys:
            column_defs.append(f"    PRIMARY KEY ({', '.join(primary_keys)})")
        
        # å¤–é”®çº¦æŸ
        for fk in table.foreign_keys:
            fk_sql = f"    CONSTRAINT {fk.name} FOREIGN KEY ({fk.column}) "
            fk_sql += f"REFERENCES {fk.ref_table}({fk.ref_column}) "
            fk_sql += f"ON DELETE {fk.on_delete} ON UPDATE {fk.on_update}"
            column_defs.append(fk_sql)
        
        lines.append(',\n'.join(column_defs))
        lines.append(");")
        
        # ç´¢å¼•
        for idx in table.indexes:
            idx_name = idx['name']
            idx_cols = ', '.join(idx['columns'])
            unique = 'UNIQUE ' if idx.get('unique') else ''
            lines.append(f"CREATE {unique}INDEX {idx_name} ON {table.name} ({idx_cols});")
        
        # æ³¨é‡Šï¼ˆPostgreSQLï¼‰
        if self.db_type == 'postgresql':
            if table.comment:
                lines.append(f"COMMENT ON TABLE {table.name} IS '{table.comment}';")
            for col in table.columns:
                if col.comment:
                    lines.append(f"COMMENT ON COLUMN {table.name}.{col.name} IS '{col.comment}';")
        
        return '\n'.join(lines)
    
    def generate_schema_sql(self, schema: Schema) -> str:
        """ç”Ÿæˆå®Œæ•´Schema SQL"""
        statements = [
            f"-- Schema: {schema.name}",
            f"-- Version: {schema.version}",
            f"-- Generated at: {datetime.now().isoformat()}",
            f"-- Database: {self.db_type}",
            ""
        ]
        
        # æŒ‰ä¾èµ–é¡ºåºæ’åºè¡¨ï¼ˆç®€å•å®ç°ï¼šå…ˆæ²¡æœ‰å¤–é”®çš„è¡¨ï¼‰
        sorted_tables = self._sort_tables_by_dependency(schema.tables)
        
        for table in sorted_tables:
            statements.append(self.generate_table_sql(table))
            statements.append("")
        
        return '\n'.join(statements)
    
    def _sort_tables_by_dependency(self, tables: List[Table]) -> List[Table]:
        """æŒ‰ä¾èµ–å…³ç³»æ’åºè¡¨"""
        table_map = {t.name: t for t in tables}
        sorted_tables = []
        visited = set()
        
        def visit(table: Table):
            if table.name in visited:
                return
            visited.add(table.name)
            
            # å…ˆè®¿é—®ä¾èµ–çš„è¡¨
            for fk in table.foreign_keys:
                if fk.ref_table in table_map:
                    visit(table_map[fk.ref_table])
            
            sorted_tables.append(table)
        
        for table in tables:
            visit(table)
        
        return sorted_tables


class ORMGenerator:
    """ORMä»£ç ç”Ÿæˆå™¨"""
    
    def __init__(self, language: str = 'python'):
        self.language = language.lower()
    
    def generate(self, schema: Schema) -> Dict[str, str]:
        """ç”ŸæˆORMä»£ç """
        if self.language == 'python':
            return self._generate_python_sqlalchemy(schema)
        elif self.language == 'java':
            return self._generate_java_jpa(schema)
        elif self.language == 'go':
            return self._generate_go_gorm(schema)
        else:
            raise ValueError(f"Unsupported language: {self.language}")
    
    def _generate_python_sqlalchemy(self, schema: Schema) -> Dict[str, str]:
        """ç”ŸæˆPython SQLAlchemyä»£ç """
        files = {}
        
        for table in schema.tables:
            lines = [
                "from sqlalchemy import Column, Integer, String, Decimal, Boolean, DateTime, Text, ForeignKey",
                "from sqlalchemy.orm import relationship",
                "from sqlalchemy.ext.declarative import declarative_base",
                "",
                "Base = declarative_base()",
                "",
                f"class {self._to_class_name(table.name)}(Base):",
                f'    """{table.comment}"""',
                f"    __tablename__ = '{table.name}'",
                ""
            ]
            
            for col in table.columns:
                sa_type = self._map_to_sqlalchemy_type(col)
                kwargs = []
                
                if col.primary_key:
                    kwargs.append("primary_key=True")
                if not col.nullable:
                    kwargs.append("nullable=False")
                if col.unique:
                    kwargs.append("unique=True")
                if col.default is not None:
                    if isinstance(col.default, str):
                        kwargs.append(f"default='{col.default}'")
                    else:
                        kwargs.append(f"default={col.default}")
                if col.comment:
                    kwargs.append(f"comment='{col.comment}'")
                
                kw_str = f", {', '.join(kwargs)}" if kwargs else ""
                lines.append(f"    {col.name} = Column({sa_type}{kw_str})")
            
            # å…³ç³»
            for rel in table.relations:
                target_class = self._to_class_name(rel.target_table)
                if rel.type == RelationType.ONE_TO_MANY:
                    lines.append(f"    {rel.name} = relationship('{target_class}', back_populates='{table.name}')")
                elif rel.type == RelationType.MANY_TO_ONE:
                    lines.append(f"    {rel.name} = relationship('{target_class}', back_populates='{table.name}')")
            
            files[f"{table.name}.py"] = '\n'.join(lines)
        
        return files
    
    def _map_to_sqlalchemy_type(self, col: Column) -> str:
        """æ˜ å°„åˆ°SQLAlchemyç±»å‹"""
        mapping = {
            DataType.STRING: f"String({col.length or 255})",
            DataType.INTEGER: "Integer",
            DataType.BIGINT: "BigInteger",
            DataType.DECIMAL: f"Decimal({col.precision or 10}, {col.scale or 2})",
            DataType.BOOLEAN: "Boolean",
            DataType.DATETIME: "DateTime",
            DataType.DATE: "Date",
            DataType.TEXT: "Text",
            DataType.JSON: "JSON",
            DataType.BLOB: "LargeBinary"
        }
        return mapping.get(col.type, "String(255)")
    
    def _to_class_name(self, table_name: str) -> str:
        """è½¬æ¢è¡¨åä¸ºç±»å"""
        return ''.join(word.capitalize() for word in table_name.split('_'))
    
    def _generate_java_jpa(self, schema: Schema) -> Dict[str, str]:
        """ç”ŸæˆJava JPAä»£ç """
        files = {}
        
        for table in schema.tables:
            class_name = self._to_class_name(table.name)
            lines = [
                "import javax.persistence.*;",
                "import java.math.BigDecimal;",
                "import java.time.LocalDateTime;",
                "import java.util.List;",
                "",
                f"@Entity",
                f"@Table(name = \"{table.name}\")",
                f"public class {class_name} {{",
                ""
            ]
            
            for col in table.columns:
                if col.primary_key:
                    lines.append("    @Id")
                    if col.auto_increment:
                        lines.append("    @GeneratedValue(strategy = GenerationType.IDENTITY)")
                
                col_def = f"    @Column(name = \"{col.name}\""
                if not col.nullable:
                    col_def += ", nullable = false"
                if col.unique:
                    col_def += ", unique = true"
                if col.length:
                    col_def += f", length = {col.length}"
                col_def += ")"
                lines.append(col_def)
                
                java_type = self._map_to_java_type(col)
                lines.append(f"    private {java_type} {self._to_camel_case(col.name)};")
                lines.append("")
            
            lines.append("}")
            files[f"{class_name}.java"] = '\n'.join(lines)
        
        return files
    
    def _map_to_java_type(self, col: Column) -> str:
        """æ˜ å°„åˆ°Javaç±»å‹"""
        mapping = {
            DataType.STRING: "String",
            DataType.INTEGER: "Integer",
            DataType.BIGINT: "Long",
            DataType.DECIMAL: "BigDecimal",
            DataType.BOOLEAN: "Boolean",
            DataType.DATETIME: "LocalDateTime",
            DataType.DATE: "LocalDate",
            DataType.TEXT: "String",
            DataType.JSON: "String",
            DataType.BLOB: "byte[]"
        }
        return mapping.get(col.type, "String")
    
    def _to_camel_case(self, snake_case: str) -> str:
        """è½¬æ¢è›‡å½¢å‘½åä¸ºé©¼å³°å‘½å"""
        parts = snake_case.split('_')
        return parts[0] + ''.join(word.capitalize() for word in parts[1:])
    
    def _generate_go_gorm(self, schema: Schema) -> Dict[str, str]:
        """ç”ŸæˆGo GORMä»£ç """
        files = {}
        
        for table in schema.tables:
            class_name = self._to_class_name(table.name)
            lines = [
                "package models",
                "",
                "import (",
                '    "time"',
                '    "gorm.io/gorm"',
                ")",
                "",
                f"// {class_name} {table.comment}",
                f"type {class_name} struct {{",
                "    gorm.Model"
            ]
            
            for col in table.columns:
                if col.name in ('id', 'created_at', 'updated_at', 'deleted_at'):
                    continue
                
                go_type = self._map_to_go_type(col)
                json_tag = f'json:"{col.name}"'
                gorm_tags = [f"column:{col.name}"]
                
                if col.primary_key:
                    gorm_tags.insert(0, "primaryKey")
                if col.auto_increment:
                    gorm_tags.append("autoIncrement")
                if not col.nullable:
                    gorm_tags.append("not null")
                
                tag = f' gorm:"{";".join(gorm_tags)}" {json_tag}'
                field_name = self._to_class_name(col.name)
                lines.append(f"    {field_name} {go_type}`{tag}`")
            
            lines.append("}")
            lines.append("")
            lines.append(f"// TableName æŒ‡å®šè¡¨å")
            lines.append(f"func ({class_name}) TableName() string {{")
            lines.append(f'    return "{table.name}"')
            lines.append("}")
            
            files[f"{table.name}.go"] = '\n'.join(lines)
        
        return files
    
    def _map_to_go_type(self, col: Column) -> str:
        """æ˜ å°„åˆ°Goç±»å‹"""
        mapping = {
            DataType.STRING: "string",
            DataType.INTEGER: "int",
            DataType.BIGINT: "int64",
            DataType.DECIMAL: "float64",
            DataType.BOOLEAN: "bool",
            DataType.DATETIME: "time.Time",
            DataType.DATE: "time.Time",
            DataType.TEXT: "string",
            DataType.JSON: "string",
            DataType.BLOB: "[]byte"
        }
        go_type = mapping.get(col.type, "string")
        if col.nullable and go_type != "string":
            return f"*{go_type}"
        return go_type


class SchemaGenerator:
    """Schemaç”Ÿæˆå™¨ä¸»ç±»"""
    
    def __init__(self, output_dir: str = 'generated'):
        self.output_dir = Path(output_dir)
        self.validator = SchemaValidator()
    
    def generate_from_yaml(self, yaml_path: str) -> Dict[str, Any]:
        """ä»YAMLæ–‡ä»¶ç”Ÿæˆæ‰€æœ‰ä»£ç """
        with open(yaml_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        schema = self._parse_schema(data)
        
        # éªŒè¯
        if not self.validator.validate(schema):
            return {
                'success': False,
                'errors': self.validator.errors,
                'warnings': self.validator.warnings
            }
        
        results = {
            'success': True,
            'warnings': self.validator.warnings,
            'generated_files': []
        }
        
        # ç”ŸæˆDDL
        for db_type in ['postgresql', 'mysql', 'oracle']:
            ddl_gen = DDLGenerator(db_type)
            sql = ddl_gen.generate_schema_sql(schema)
            
            output_path = self.output_dir / 'ddl' / f"{schema.name}_{db_type}.sql"
            output_path.parent.mkdir(parents=True, exist_ok=True)
            output_path.write_text(sql, encoding='utf-8')
            results['generated_files'].append(str(output_path))
        
        # ç”ŸæˆORMä»£ç 
        for lang in ['python', 'java', 'go']:
            orm_gen = ORMGenerator(lang)
            files = orm_gen.generate(schema)
            
            lang_dir = self.output_dir / 'orm' / lang
            lang_dir.mkdir(parents=True, exist_ok=True)
            
            for filename, content in files.items():
                output_path = lang_dir / filename
                output_path.write_text(content, encoding='utf-8')
                results['generated_files'].append(str(output_path))
        
        # ç”Ÿæˆæ•°æ®å­—å…¸
        dict_path = self.output_dir / f"{schema.name}_data_dictionary.md"
        dict_content = self._generate_data_dictionary(schema)
        dict_path.write_text(dict_content, encoding='utf-8')
        results['generated_files'].append(str(dict_path))
        
        return results
    
    def _parse_schema(self, data: Dict) -> Schema:
        """è§£æSchemaå®šä¹‰"""
        schema = Schema(
            name=data['name'],
            version=data['version'],
            description=data.get('description', '')
        )
        
        for table_data in data.get('tables', []):
            table = Table(
                name=table_data['name'],
                comment=table_data.get('comment', '')
            )
            
            for col_data in table_data.get('columns', []):
                col = Column(
                    name=col_data['name'],
                    type=DataType(col_data['type']),
                    length=col_data.get('length'),
                    precision=col_data.get('precision'),
                    scale=col_data.get('scale'),
                    nullable=col_data.get('nullable', True),
                    default=col_data.get('default'),
                    primary_key=col_data.get('primary_key', False),
                    auto_increment=col_data.get('auto_increment', False),
                    unique=col_data.get('unique', False),
                    index=col_data.get('index', False),
                    comment=col_data.get('comment', '')
                )
                table.columns.append(col)
            
            for fk_data in table_data.get('foreign_keys', []):
                fk = ForeignKey(**fk_data)
                table.foreign_keys.append(fk)
            
            for idx_data in table_data.get('indexes', []):
                table.indexes.append(idx_data)
            
            schema.tables.append(table)
        
        return schema
    
    def _generate_data_dictionary(self, schema: Schema) -> str:
        """ç”Ÿæˆæ•°æ®å­—å…¸"""
        lines = [
            f"# {schema.name} æ•°æ®å­—å…¸",
            "",
            f"ç‰ˆæœ¬: {schema.version}",
            f"æè¿°: {schema.description}",
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}",
            "",
            "## è¡¨æ¸…å•",
            "",
            "| è¡¨å | è¯´æ˜ | å­—æ®µæ•° |",
            "|------|------|--------|",
        ]
        
        for table in schema.tables:
            lines.append(f"| {table.name} | {table.comment} | {len(table.columns)} |")
        
        lines.append("")
        
        for table in schema.tables:
            lines.extend([
                f"## {table.name}",
                "",
                f"**è¯´æ˜**: {table.comment}",
                "",
                "| å­—æ®µå | ç±»å‹ | å¯ç©º | é»˜è®¤å€¼ | è¯´æ˜ |",
                "|--------|------|------|--------|------|"
            ])
            
            for col in table.columns:
                type_str = col.type.value
                if col.length:
                    type_str += f"({col.length})"
                elif col.precision:
                    type_str += f"({col.precision},{col.scale or 0})"
                
                nullable = "æ˜¯" if col.nullable else "å¦"
                default = str(col.default) if col.default else "-"
                
                lines.append(f"| {col.name} | {type_str} | {nullable} | {default} | {col.comment} |")
            
            lines.append("")
        
        return '\n'.join(lines)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åˆ›å»ºç¤ºä¾‹Schema YAML
    sample_schema = """
name: logistics_db
version: 1.0.0
description: ç‰©æµç®¡ç†ç³»ç»Ÿæ•°æ®åº“Schema

tables:
  - name: customers
    comment: å®¢æˆ·ä¿¡æ¯è¡¨
    columns:
      - name: id
        type: bigint
        primary_key: true
        auto_increment: true
        comment: ä¸»é”®ID
      - name: name
        type: string
        length: 100
        nullable: false
        comment: å®¢æˆ·åç§°
      - name: email
        type: string
        length: 100
        nullable: false
        unique: true
        comment: é‚®ç®±
      - name: phone
        type: string
        length: 20
        comment: ç”µè¯
      - name: created_at
        type: datetime
        default: CURRENT_TIMESTAMP
        comment: åˆ›å»ºæ—¶é—´
    indexes:
      - name: idx_customers_name
        columns: [name]
  
  - name: orders
    comment: è®¢å•è¡¨
    columns:
      - name: id
        type: bigint
        primary_key: true
        auto_increment: true
        comment: è®¢å•ID
      - name: customer_id
        type: bigint
        nullable: false
        comment: å®¢æˆ·ID
      - name: order_no
        type: string
        length: 50
        nullable: false
        unique: true
        comment: è®¢å•ç¼–å·
      - name: total_amount
        type: decimal
        precision: 12
        scale: 2
        nullable: false
        comment: è®¢å•æ€»é‡‘é¢
      - name: status
        type: string
        length: 20
        default: pending
        comment: è®¢å•çŠ¶æ€
      - name: created_at
        type: datetime
        default: CURRENT_TIMESTAMP
        comment: åˆ›å»ºæ—¶é—´
    foreign_keys:
      - name: fk_orders_customer
        column: customer_id
        ref_table: customers
        ref_column: id
        on_delete: RESTRICT
        on_update: CASCADE
    indexes:
      - name: idx_orders_customer
        columns: [customer_id]
      - name: idx_orders_status
        columns: [status]
"""
    
    # ä¿å­˜YAMLæ–‡ä»¶
    yaml_path = Path('sample_schema.yaml')
    yaml_path.write_text(sample_schema, encoding='utf-8')
    
    # ç”Ÿæˆä»£ç 
    generator = SchemaGenerator(output_dir='generated')
    result = generator.generate_from_yaml(str(yaml_path))
    
    if result['success']:
        print("âœ… Schemaç”ŸæˆæˆåŠŸ!")
        print(f"ç”Ÿæˆæ–‡ä»¶æ•°: {len(result['generated_files'])}")
        for f in result['generated_files']:
            print(f"  - {f}")
        if result['warnings']:
            print(f"\nâš ï¸ è­¦å‘Š ({len(result['warnings'])}):")
            for w in result['warnings']:
                print(f"  - {w}")
    else:
        print("âŒ SchemaéªŒè¯å¤±è´¥!")
        print("é”™è¯¯:")
        for e in result['errors']:
            print(f"  - {e}")
```

### 4.4 æ•ˆæœè¯„ä¼°

**æ€§èƒ½æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | å®æ–½å‰ | å®æ–½å | æå‡å¹…åº¦ |
|------|--------|--------|----------|
| æ•°æ®åº“è®¾è®¡æ—¶é—´ | 3å¤©/è¡¨ | 30åˆ†é’Ÿ/è¡¨ | 93% â†“ |
| ORMä»£ç ç¼–å†™æ—¶é—´ | 4å°æ—¶/è¡¨ | è‡ªåŠ¨ç”Ÿæˆ | 100% â†“ |
| æ–‡æ¡£æ›´æ–°æ—¶é—´ | 2å°æ—¶/å˜æ›´ | è‡ªåŠ¨åŒæ­¥ | 100% â†“ |
| è·¨æœåŠ¡SchemaæŸ¥è¯¢æ—¶é—´ | 30åˆ†é’Ÿ | å³æ—¶æŸ¥çœ‹ | 100% â†“ |
| è§„èŒƒè¿è§„ç‡ | 25% | 3% | 88% â†“ |

**ä¸šåŠ¡ä»·å€¼**ï¼š

| ç»´åº¦ | ä»·å€¼æè¿° | é‡åŒ–æ•°æ® |
|------|----------|----------|
| **å¼€å‘æ•ˆç‡** | å‡å°‘é‡å¤ç¼–ç å·¥ä½œ | å¼€å‘äººå‘˜æ•ˆç‡æå‡40% |
| **ä»£ç è´¨é‡** | ç»Ÿä¸€è§„èŒƒå‡å°‘Bug | æ•°æ®åº“ç›¸å…³Bugå‡å°‘60% |
| **åä½œæ•ˆç‡** | é™ä½è·¨å›¢é˜Ÿæ²Ÿé€šæˆæœ¬ | è®¾è®¡è¯„å®¡æ—¶é—´å‡å°‘70% |
| **çŸ¥è¯†æ²‰æ·€** | SchemaçŸ¥è¯†åº“ç§¯ç´¯ | æ–°äººä¸Šæ‰‹æ—¶é—´ç¼©çŸ­80% |
| **å¤šè¯­è¨€æ”¯æŒ** | ç»Ÿä¸€Schemaæ”¯æŒå¾®æœåŠ¡å¼‚æ„ | æ”¯æŒ3ç§è¯­è¨€ä»£ç ç”Ÿæˆ |

**ç»éªŒæ•™è®­**ï¼š

1. **è§„èŒƒè®¾è®¡è¦å¾ªåºæ¸è¿›**ï¼šåˆæœŸè¿‡äºä¸¥æ ¼çš„è§„èŒƒå¯¼è‡´æ¨è¡Œå›°éš¾ï¼Œå»ºè®®åˆ†é˜¶æ®µå®æ–½ï¼ˆåŸºç¡€è§„èŒƒâ†’è¿›é˜¶è§„èŒƒâ†’æœ€ä½³å®è·µï¼‰
2. **ä¿ç•™äººå·¥è°ƒæ•´ç©ºé—´**ï¼šè‡ªåŠ¨ç”Ÿæˆçš„ä»£ç æœ‰æ—¶éœ€è¦å¾®è°ƒï¼Œåº”æ”¯æŒè‡ªå®šä¹‰æ¨¡æ¿å’Œåç½®å¤„ç†
3. **ç‰ˆæœ¬å…¼å®¹æ€§**ï¼šä¸åŒæ•°æ®åº“ç‰ˆæœ¬çš„è¯­æ³•å·®å¼‚éœ€è¦ä¸“é—¨çš„é€‚é…å±‚
4. **æ€§èƒ½è€ƒé‡**ï¼šè‡ªåŠ¨ç”Ÿæˆçš„ç´¢å¼•å»ºè®®éœ€è¦ç»“åˆå®é™…æŸ¥è¯¢æ¨¡å¼ä¼˜åŒ–
5. **å›¢é˜Ÿåä½œ**ï¼šéœ€è¦é…å¥—çš„Code Reviewæµç¨‹ï¼Œç¡®ä¿ç”Ÿæˆçš„Schemaç»è¿‡æŠ€æœ¯è´Ÿè´£äººå®¡æ ¸

**æ•ˆç‡æå‡é‡åŒ–**ï¼š
- å¼€å‘äººå‘˜äººå‡æ—¥æœ‰æ•ˆç¼–ç æ—¶é—´ï¼šä»5.2å°æ—¶æå‡è‡³6.8å°æ—¶ï¼ˆ+31%ï¼‰
- æ•°æ®åº“è®¾è®¡è¯„å®¡é€šè¿‡ç‡ï¼šä»70%æå‡è‡³95%ï¼ˆ+36%ï¼‰
- å¾®æœåŠ¡ä¸Šçº¿å‘¨æœŸï¼šä»2å‘¨ç¼©çŸ­è‡³5å¤©ï¼ˆ-64%ï¼‰
- è·¨å›¢é˜Ÿæ•°æ®æ¥å£å¯¹æ¥æ—¶é—´ï¼šä»å¹³å‡3å¤©ç¼©çŸ­è‡³4å°æ—¶ï¼ˆ-94%ï¼‰

**æŠ•èµ„å›æŠ¥**ï¼š
- é¡¹ç›®æ€»æŠ•èµ„ï¼š60ä¸‡å…ƒï¼ˆå¼€å‘45ä¸‡+åŸ¹è®­15ä¸‡ï¼‰
- å¹´åº¦èŠ‚çº¦ï¼š150ä¸‡å…ƒï¼ˆäººåŠ›90ä¸‡+æ—¶é—´æˆæœ¬60ä¸‡ï¼‰
- æŠ•èµ„å›æ”¶æœŸï¼š4.8ä¸ªæœˆ
- 3å¹´ROIï¼š650%

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `04_Transformation.md` - è½¬æ¢ä½“ç³»

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21  
**æœ€åæ›´æ–°**ï¼š2025-02-15
