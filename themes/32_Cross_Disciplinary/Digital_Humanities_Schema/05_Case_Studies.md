# æ•°å­—äººæ–‡Schemaå®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [æ•°å­—äººæ–‡Schemaå®è·µæ¡ˆä¾‹](#æ•°å­—äººæ–‡schemaå®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¡ˆä¾‹æ¦‚è¿°](#1-æ¡ˆä¾‹æ¦‚è¿°)
  - [2. æ¡ˆä¾‹1ï¼šå¤ç±æ•°å­—åŒ–ä¸æ™ºèƒ½åˆ†æå¹³å°](#2-æ¡ˆä¾‹1å¤ç±æ•°å­—åŒ–ä¸æ™ºèƒ½åˆ†æå¹³å°)
    - [2.1 æœºæ„èƒŒæ™¯](#21-æœºæ„èƒŒæ™¯)
    - [2.2 ä¸šåŠ¡ç—›ç‚¹](#22-ä¸šåŠ¡ç—›ç‚¹)
    - [2.3 ä¸šåŠ¡ç›®æ ‡](#23-ä¸šåŠ¡ç›®æ ‡)
    - [2.4 æŠ€æœ¯æŒ‘æˆ˜](#24-æŠ€æœ¯æŒ‘æˆ˜)
    - [2.5 å®Œæ•´ä»£ç å®ç°](#25-å®Œæ•´ä»£ç å®ç°)
    - [2.6 æ•ˆæœè¯„ä¼°ä¸ROI](#26-æ•ˆæœè¯„ä¼°ä¸roi)
  - [3. æ¡ˆä¾‹2ï¼šå†å²åœ°å›¾æ—¶ç©ºåˆ†æç³»ç»Ÿ](#3-æ¡ˆä¾‹2å†å²åœ°å›¾æ—¶ç©ºåˆ†æç³»ç»Ÿ)
    - [3.1 æœºæ„èƒŒæ™¯](#31-æœºæ„èƒŒæ™¯)
    - [3.2 æŠ€æœ¯æŒ‘æˆ˜](#32-æŠ€æœ¯æŒ‘æˆ˜)
    - [3.3 å®Œæ•´ä»£ç å®ç°](#33-å®Œæ•´ä»£ç å®ç°)
    - [3.4 æ•ˆæœè¯„ä¼°ä¸ROI](#34-æ•ˆæœè¯„ä¼°ä¸roi)
  - [4. æ¡ˆä¾‹3ï¼šæ–‡åŒ–é—äº§3Dæ•°å­—åŒ–](#4-æ¡ˆä¾‹3æ–‡åŒ–é—äº§3dæ•°å­—åŒ–)
  - [5. æ¡ˆä¾‹æ€»ç»“](#5-æ¡ˆä¾‹æ€»ç»“)

---

## 1. æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›**æ•°å­—äººæ–‡Schemaçš„å®é™…åº”ç”¨æ¡ˆä¾‹**ï¼Œæ¶µç›–å¤ç±æ•°å­—åŒ–ã€å†å²åœ°ç†ä¿¡æ¯ã€æ–‡åŒ–é—äº§ä¿æŠ¤ç­‰é¢†åŸŸã€‚æ•°å­—äººæ–‡ç»“åˆè®¡ç®—æ–¹æ³•å’Œäººæ–‡ç ”ç©¶ï¼Œä¸ºäººæ–‡ç§‘å­¦æä¾›æ–°çš„ç ”ç©¶å·¥å…·å’ŒèŒƒå¼ã€‚

**æ¡ˆä¾‹ç±»å‹**ï¼š

- å¤ç±æ•°å­—åŒ–ä¸æ™ºèƒ½åˆ†æ
- å†å²åœ°å›¾æ—¶ç©ºåˆ†æ
- æ–‡åŒ–é—äº§3Dæ•°å­—åŒ–

---

## 2. æ¡ˆä¾‹1ï¼šå¤ç±æ•°å­—åŒ–ä¸æ™ºèƒ½åˆ†æå¹³å°

### 2.1 æœºæ„èƒŒæ™¯

**æœºæ„èƒŒæ™¯**ï¼š
æŸçœçº§å›¾ä¹¦é¦†ï¼ˆä»¥ä¸‹ç®€ç§°"HeritageLibrary"ï¼‰æˆç«‹äº1905å¹´ï¼Œæ˜¯ä¸­å›½å†å²æœ€æ‚ ä¹…çš„å…¬å…±å›¾ä¹¦é¦†ä¹‹ä¸€ã€‚é¦†è—å¤ç±å–„æœ¬30ä½™ä¸‡å†Œï¼ŒåŒ…æ‹¬å®‹å…ƒåˆ»æœ¬ã€æ˜æ¸…æŠ„æœ¬ã€åœ°æ–¹å¿—ã€å®¶è°±ç­‰çè´µæ–‡çŒ®ã€‚å…¶ä¸­30%çš„è—å“ä¸ºå­¤æœ¬æˆ–ç½•è§ç‰ˆæœ¬ï¼Œå…·æœ‰æé«˜çš„å­¦æœ¯ç ”ç©¶ä»·å€¼ã€‚

éšç€å¤ç±ä¿æŠ¤æ„è¯†çš„å¢å¼ºå’Œæ•°å­—åŒ–æŠ€æœ¯çš„å‘å±•ï¼Œå›¾ä¹¦é¦†äº2018å¹´å¯åŠ¨"ä¸­åå¤ç±æ•°å­—å·¥ç¨‹"ï¼Œè®¡åˆ’ç”¨10å¹´æ—¶é—´å®Œæˆå…¨éƒ¨é¦†è—å¤ç±çš„æ•°å­—åŒ–ã€‚ç›®å‰å·²ç»å®Œæˆ15%çš„æ•°å­—åŒ–å·¥ä½œï¼Œä½†é¢ä¸´æ•ˆç‡ä½ã€è´¨é‡å‚å·®ä¸é½ã€æ·±åº¦åˆ©ç”¨ä¸è¶³ç­‰é—®é¢˜ã€‚

### 2.2 ä¸šåŠ¡ç—›ç‚¹

1. **æ•°å­—åŒ–æ•ˆç‡ä½ä¸‹**ï¼šä¼ ç»Ÿäººå·¥æ‰«æ+æ ¡å¯¹æ¨¡å¼ï¼Œæ¯äººæ¯å¤©ä»…èƒ½å¤„ç†50é¡µï¼ŒæŒ‰æ­¤é€Ÿåº¦å®Œæˆå…¨éƒ¨æ•°å­—åŒ–éœ€è¦100å¹´ã€‚

2. **æ–‡å­—è¯†åˆ«å‡†ç¡®ç‡ä½**ï¼šå¤ç±æ–‡å­—å¤šä¸ºæ‰‹å†™ä½“ã€ç¹ä½“å­—ã€å¼‚ä½“å­—ï¼ŒOCRè¯†åˆ«å‡†ç¡®ç‡ä»…65%ï¼ŒåæœŸæ ¡å¯¹å·¥ä½œé‡å·¨å¤§ã€‚

3. **çŸ¥è¯†æå–å›°éš¾**ï¼šå¤ç±ä¸­çš„å†å²äººç‰©ã€åœ°åã€äº‹ä»¶ç­‰çŸ¥è¯†æ— æ³•è‡ªåŠ¨æå–ï¼Œç ”ç©¶è€…éœ€è¦é€é¡µé˜…è¯»æŸ¥æ‰¾ã€‚

4. **ç‰ˆæœ¬æ¯”å¯¹è€—æ—¶**ï¼šä¸åŒç‰ˆæœ¬å¤ç±çš„æ¯”å¯¹éœ€è¦ä¸“å®¶é€å­—æ ¸å¯¹ï¼Œä¸€éƒ¨ä¹¦çš„ç‰ˆæœ¬æ¯”å¯¹å¾€å¾€éœ€è¦æ•°æœˆæ—¶é—´ã€‚

5. **å¼€æ”¾åˆ©ç”¨ä¸è¶³**ï¼šæ•°å­—åŒ–åçš„å¤ç±ä»¥å›¾åƒä¸ºä¸»ï¼Œç¼ºä¹å…¨æ–‡æ£€ç´¢å’ŒçŸ¥è¯†å›¾è°±æ”¯æŒï¼Œåˆ©ç”¨ç‡ä½ã€‚

### 2.3 ä¸šåŠ¡ç›®æ ‡

1. **æå‡æ•°å­—åŒ–æ•ˆç‡**ï¼šå¼•å…¥è‡ªåŠ¨åŒ–æŠ€æœ¯ï¼Œå°†æ•°å­—åŒ–æ•ˆç‡æå‡10å€ï¼Œåœ¨5å¹´å†…å®Œæˆå…¨éƒ¨æ•°å­—åŒ–ã€‚

2. **æé«˜OCRå‡†ç¡®ç‡**ï¼šé’ˆå¯¹å¤ç±ç‰¹ç‚¹ä¼˜åŒ–OCRï¼Œå°†è¯†åˆ«å‡†ç¡®ç‡æå‡è‡³95%ä»¥ä¸Šã€‚

3. **æ™ºèƒ½çŸ¥è¯†æå–**ï¼šè‡ªåŠ¨è¯†åˆ«å¤ç±ä¸­çš„äººåã€åœ°åã€æ—¶é—´ã€äº‹ä»¶ç­‰å®ä½“ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±ã€‚

4. **è‡ªåŠ¨åŒ–ç‰ˆæœ¬æ¯”å¯¹**ï¼šå®ç°å¤šç‰ˆæœ¬å¤ç±çš„è‡ªåŠ¨æ¯”å¯¹ï¼Œæ ‡æ³¨å·®å¼‚ï¼Œè¾…åŠ©ä¸“å®¶æ ¡å‹˜ã€‚

5. **æ™ºæ…§æœåŠ¡å¹³å°**ï¼šå»ºè®¾å¤ç±å…¨æ–‡æ£€ç´¢ã€å…³è”æ¨èã€å¯è§†åŒ–åˆ†æç­‰æœåŠ¡å¹³å°ã€‚

### 2.4 æŠ€æœ¯æŒ‘æˆ˜

1. **å¤ç±å›¾åƒè´¨é‡å·®å¼‚å¤§**ï¼šå¹´ä»£ä¹…è¿œå¯¼è‡´çº¸å¼ æ³›é»„ã€å¢¨è¿¹è¤ªè‰²ã€è™«è›€ç ´æŸç­‰ï¼Œå›¾åƒé¢„å¤„ç†å¤æ‚ã€‚

2. **å¤æ–‡å­—ä½“å¤šæ ·æ€§**ï¼šç¯†ã€éš¶ã€æ¥·ã€è¡Œã€è‰ç­‰å¤šç§å­—ä½“ï¼Œä»¥åŠå¤§é‡çš„å¼‚ä½“å­—ã€é¿è®³å­—ã€‚

3. **ç‰ˆå¼å¤æ‚**ï¼šåŒè¡Œå°å­—ã€çœ‰æ‰¹ã€å¤¹æ³¨ã€æœ±å¢¨å¥—å°ç­‰å¤æ‚ç‰ˆå¼ï¼Œç‰ˆé¢åˆ†æå›°éš¾ã€‚

4. **ä¸Šä¸‹æ–‡ä¾èµ–å¼º**ï¼šå¤ç±æ–‡å­—å¸¸éœ€ç»“åˆä¸Šä¸‹æ–‡ç†è§£ï¼Œæ–­å¥å’Œè¯­ä¹‰åˆ†ææŒ‘æˆ˜å¤§ã€‚

5. **ä¸“å®¶çŸ¥è¯†æ•°å­—åŒ–**ï¼šå°†æ ¡å‹˜å­¦ã€ç‰ˆæœ¬å­¦ç­‰ä¸“å®¶çŸ¥è¯†è½¬åŒ–ä¸ºå¯è®¡ç®—æ¨¡å‹ã€‚

### 2.5 å®Œæ•´ä»£ç å®ç°

```python
#!/usr/bin/env python3
"""
å¤ç±æ•°å­—åŒ–ä¸æ™ºèƒ½åˆ†æå¹³å°
HeritageLibrary å¤ç±æ™ºæ…§æœåŠ¡å¹³å°

åŠŸèƒ½æ¨¡å—ï¼š
1. å¤ç±å›¾åƒé¢„å¤„ç†ä¸å¢å¼º
2. å¤ç±ä¸“ç”¨OCRè¯†åˆ«
3. å‘½åå®ä½“è¯†åˆ«ï¼ˆäººåã€åœ°åã€å®˜èŒç­‰ï¼‰
4. æ–‡æœ¬æ ¡å‹˜ä¸ç‰ˆæœ¬æ¯”å¯¹
5. çŸ¥è¯†å›¾è°±æ„å»º
6. å…¨æ–‡æ£€ç´¢ä¸å¯è§†åŒ–

æŠ€æœ¯æ ˆï¼šPython + OpenCV + PyTorch + Elasticsearch + Neo4j

ä½œè€…ï¼šæ•°å­—äººæ–‡æŠ€æœ¯å›¢é˜Ÿ
ç‰ˆæœ¬ï¼š2.0
"""

import cv2
import numpy as np
import pytesseract
from PIL import Image
import re
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple, Any
from collections import defaultdict
import json
import logging
from difflib import SequenceMatcher

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class AncientTextPage:
    """å¤ç±é¡µé¢"""
    page_id: str
    book_id: str
    volume: int
    page_number: int
    
    # å›¾åƒæ•°æ®
    image_path: str
    image: Optional[np.ndarray] = None
    
    # è¯†åˆ«ç»“æœ
    raw_text: str = ""
    corrected_text: str = ""
    confidence: float = 0.0
    
    # ç‰ˆé¢ä¿¡æ¯
    text_regions: List[Dict] = field(default_factory=list)
    illustrations: List[Dict] = field(default_factory=list)
    annotations: List[Dict] = field(default_factory=list)
    
    # å®ä½“
    entities: List[Dict] = field(default_factory=list)


@dataclass
class NamedEntity:
    """å‘½åå®ä½“"""
    entity_id: str
    text: str
    entity_type: str  # PERSON, LOCATION, TIME, BOOK, OFFICE
    start_pos: int
    end_pos: int
    confidence: float = 0.0
    
    # æ ‡å‡†åŒ–åç§°
    canonical_name: Optional[str] = None
    
    # å®ä½“é“¾æ¥
    wikidata_id: Optional[str] = None
    cbdb_id: Optional[str] = None  # ä¸­å›½å†ä»£äººç‰©ä¼ è®°èµ„æ–™åº“ID


class AncientImagePreprocessor:
    """å¤ç±å›¾åƒé¢„å¤„ç†å™¨"""
    
    def __init__(self):
        # å¤ç±ä¸“ç”¨å¤„ç†å‚æ•°
        self.params = {
            'denoise_h': 10,
            'denoise_template_window': 7,
            'denoise_search_window': 21,
            'adaptive_block_size': 15,
            'adaptive_c': 10
        }
    
    def preprocess(self, image: np.ndarray) -> np.ndarray:
        """é¢„å¤„ç†å¤ç±å›¾åƒ"""
        # 1. ç°åº¦è½¬æ¢
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image.copy()
        
        # 2. å»å™ª
        denoised = cv2.fastNlMeansDenoising(
            gray, None,
            h=self.params['denoise_h'],
            templateWindowSize=self.params['denoise_template_window'],
            searchWindowSize=self.params['denoise_search_window']
        )
        
        # 3. å¯¹æ¯”åº¦å¢å¼º
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(denoised)
        
        # 4. è‡ªé€‚åº”äºŒå€¼åŒ–
        binary = cv2.adaptiveThreshold(
            enhanced, 255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY,
            self.params['adaptive_block_size'],
            self.params['adaptive_c']
        )
        
        # 5. å»è¾¹æ¡†å’Œæ±¡è¿¹
        cleaned = self._remove_borders(binary)
        
        return cleaned
    
    def _remove_borders(self, image: np.ndarray) -> np.ndarray:
        """å»é™¤è¾¹æ¡†"""
        # æ£€æµ‹å¹¶å»é™¤é»‘è‰²è¾¹æ¡†
        h, w = image.shape
        border_ratio = 0.05
        
        # åˆ›å»ºmaskå»é™¤è¾¹ç¼˜
        mask = np.ones_like(image) * 255
        margin_h = int(h * border_ratio)
        margin_w = int(w * border_ratio)
        mask[margin_h:-margin_h, margin_w:-margin_w] = image[margin_h:-margin_h, margin_w:-margin_w]
        
        return mask
    
    def detect_layout(self, image: np.ndarray) -> Dict:
        """æ£€æµ‹ç‰ˆé¢ç»“æ„"""
        # æ–‡æœ¬åŒºåŸŸæ£€æµ‹
        contours, _ = cv2.findContours(
            255 - image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
        )
        
        text_regions = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            area = w * h
            
            # è¿‡æ»¤å¤ªå°çš„åŒºåŸŸ
            if area < 1000:
                continue
            
            # åˆ¤æ–­åŒºåŸŸç±»å‹
            aspect_ratio = w / float(h)
            
            if aspect_ratio > 3:
                region_type = "header" if y < image.shape[0] * 0.1 else "line"
            elif aspect_ratio < 0.3:
                region_type = "vertical_text"
            else:
                region_type = "paragraph"
            
            text_regions.append({
                'bbox': (x, y, w, h),
                'type': region_type,
                'area': area
            })
        
        # æŒ‰é˜…è¯»é¡ºåºæ’åºï¼ˆä»ä¸Šåˆ°ä¸‹ï¼Œä»å³åˆ°å·¦ï¼‰
        text_regions.sort(key=lambda r: (r['bbox'][1], -r['bbox'][0]))
        
        return {'text_regions': text_regions}


class AncientTextOCR:
    """å¤ç±ä¸“ç”¨OCR"""
    
    # å¤ç±å¸¸ç”¨å¼‚ä½“å­—æ˜ å°„
    VARIANT_CHARS = {
        'è¡†': 'çœ¾', 'ç¾£': 'ç¾¤', 'å³¯': 'å³°', 'çˆ²': 'ç‚º',
        'ç›´': 'ç›´', 'çœŸ': 'çœŸ', 'å€¤': 'å€¼'
    }
    
    # ç¹ç®€è½¬æ¢è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
    TRAD_TO_SIMP = {
        'è¡†': 'ä¼—', 'å“¡': 'å‘˜', 'åœ‹': 'å›½', 'æ›¸': 'ä¹¦',
        'é•·': 'é•¿', 'é–€': 'é—¨', 'æ±': 'ä¸œ', 'è»Š': 'è½¦'
    }
    
    def __init__(self, use_gpu: bool = False):
        self.use_gpu = use_gpu
        # åˆå§‹åŒ–Tesseractï¼Œä½¿ç”¨ä¸­æ–‡å¤ç±è®­ç»ƒæ•°æ®
        self.custom_config = r'--oem 3 --psm 6 -l chi_sim+chi_tra'
    
    def recognize(self, image: np.ndarray, regions: List[Dict] = None) -> Tuple[str, float]:
        """è¯†åˆ«æ–‡å­—"""
        if regions:
            # åˆ†åŒºåŸŸè¯†åˆ«
            texts = []
            confidences = []
            
            for region in regions:
                x, y, w, h = region['bbox']
                roi = image[y:y+h, x:x+w]
                
                text, conf = self._recognize_region(roi)
                texts.append(text)
                confidences.append(conf)
            
            full_text = '\n'.join(texts)
            avg_confidence = np.mean(confidences) if confidences else 0
        else:
            # æ•´é¡µè¯†åˆ«
            full_text, avg_confidence = self._recognize_region(image)
        
        # åå¤„ç†
        full_text = self._post_process(full_text)
        
        return full_text, avg_confidence
    
    def _recognize_region(self, image: np.ndarray) -> Tuple[str, float]:
        """è¯†åˆ«å•ä¸ªåŒºåŸŸ"""
        # ä½¿ç”¨Tesseract
        pil_image = Image.fromarray(image)
        
        try:
            data = pytesseract.image_to_data(
                pil_image, config=self.custom_config, output_type=pytesseract.Output.DICT
            )
            
            texts = []
            confidences = []
            
            for i, text in enumerate(data['text']):
                conf = int(data['conf'][i])
                if conf > 30 and text.strip():
                    texts.append(text)
                    confidences.append(conf)
            
            return ' '.join(texts), np.mean(confidences) if confidences else 0
        except Exception as e:
            logger.error(f"OCR error: {e}")
            return "", 0
    
    def _post_process(self, text: str) -> str:
        """åå¤„ç†è¯†åˆ«ç»“æœ"""
        # 1. è§„èŒƒåŒ–å¼‚ä½“å­—
        for var, std in self.VARIANT_CHARS.items():
            text = text.replace(var, std)
        
        # 2. å»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', '', text)
        
        # 3. ä¿®å¤å¸¸è§OCRé”™è¯¯
        text = self._fix_common_errors(text)
        
        return text
    
    def _fix_common_errors(self, text: str) -> str:
        """ä¿®å¤å¸¸è§OCRé”™è¯¯"""
        # ç®€åŒ–çš„é”™è¯¯ä¿®å¤è§„åˆ™
        corrections = {
            'ï¼Œ': 'ï¼Œ',
            'ã€‚': 'ã€‚',
            'ã€': 'ã€',
        }
        
        for wrong, correct in corrections.items():
            text = text.replace(wrong, correct)
        
        return text


class AncientTextNER:
    """å¤ç±å‘½åå®ä½“è¯†åˆ«"""
    
    # äººåç‰¹å¾è¯
    PERSON_INDICATORS = ['å­—', 'å·', 'è°¥', 'å°', 'çˆµ', 'å®˜', 'æˆ', 'ä»»', 'æ‹œ']
    
    # åœ°åç‰¹å¾è¯
    LOCATION_INDICATORS = ['å·', 'åºœ', 'å¿', 'éƒ¡', 'åŸ', 'å±±', 'æ°´', 'æ²³', 'æ¹–']
    
    # æ—¶é—´ç‰¹å¾è¯
    TIME_PATTERNS = [
        r'(\d{1,4})å¹´',
        r'(å”|å®‹|å…ƒ|æ˜|æ¸…)(åˆ|ä¸­|æœ«)',
        r'(æ˜¥|å¤|ç§‹|å†¬)',
    ]
    
    def __init__(self):
        # åŠ è½½äººåè¯å…¸ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.person_names = set()
        self.location_names = set()
    
    def extract_entities(self, text: str) -> List[NamedEntity]:
        """æå–å‘½åå®ä½“"""
        entities = []
        
        # è¯†åˆ«äººå
        person_entities = self._extract_persons(text)
        entities.extend(person_entities)
        
        # è¯†åˆ«åœ°å
        location_entities = self._extract_locations(text)
        entities.extend(location_entities)
        
        # è¯†åˆ«æ—¶é—´
        time_entities = self._extract_times(text)
        entities.extend(time_entities)
        
        # å»é‡å’Œæ’åº
        entities = self._deduplicate_entities(entities)
        entities.sort(key=lambda e: e.start_pos)
        
        return entities
    
    def _extract_persons(self, text: str) -> List[NamedEntity]:
        """æå–äººå"""
        entities = []
        
        # åŸºäºè§„åˆ™çš„ç®€å•è¯†åˆ«ï¼ˆå®é™…åº”ä½¿ç”¨è®­ç»ƒå¥½çš„NERæ¨¡å‹ï¼‰
        # æ¨¡å¼ï¼šå§“æ° + åå­—ï¼ˆ2-3å­—ï¼‰
        surname_pattern = r'[ç‹æå¼ åˆ˜é™ˆæ¨é»„èµµå‘¨å´å¾å­™é©¬æœ±èƒ¡éƒ­ä½•æ—é«˜ç½—éƒ‘æ¢è°¢å®‹å”è®¸éŸ©å†¯é‚“æ›¹å½­æ›¾][\u4e00-\u9fff]{1,2}'
        
        for match in re.finditer(surname_pattern, text):
            entity = NamedEntity(
                entity_id=f"PER_{match.start()}",
                text=match.group(),
                entity_type="PERSON",
                start_pos=match.start(),
                end_pos=match.end(),
                confidence=0.7
            )
            entities.append(entity)
        
        return entities
    
    def _extract_locations(self, text: str) -> List[NamedEntity]:
        """æå–åœ°å"""
        entities = []
        
        # æ¨¡å¼ï¼šåœ°å + ç‰¹å¾è¯
        for indicator in self.LOCATION_INDICATORS:
            pattern = f'[\u4e00-\u9fff]{{1,3}}{indicator}'
            for match in re.finditer(pattern, text):
                entity = NamedEntity(
                    entity_id=f"LOC_{match.start()}",
                    text=match.group(),
                    entity_type="LOCATION",
                    start_pos=match.start(),
                    end_pos=match.end(),
                    confidence=0.6
                )
                entities.append(entity)
        
        return entities
    
    def _extract_times(self, text: str) -> List[NamedEntity]:
        """æå–æ—¶é—´"""
        entities = []
        
        for pattern in self.TIME_PATTERNS:
            for match in re.finditer(pattern, text):
                entity = NamedEntity(
                    entity_id=f"TIME_{match.start()}",
                    text=match.group(),
                    entity_type="TIME",
                    start_pos=match.start(),
                    end_pos=match.end(),
                    confidence=0.8
                )
                entities.append(entity)
        
        return entities
    
    def _deduplicate_entities(self, entities: List[NamedEntity]) -> List[NamedEntity]:
        """å»é‡å®ä½“"""
        seen = set()
        unique = []
        
        for entity in entities:
            key = (entity.start_pos, entity.end_pos)
            if key not in seen:
                seen.add(key)
                unique.append(entity)
        
        return unique


class TextCollation:
    """æ–‡æœ¬æ ¡å‹˜å·¥å…·"""
    
    def __init__(self):
        pass
    
    def compare_versions(self, version_a: str, version_b: str) -> Dict:
        """æ¯”è¾ƒä¸¤ä¸ªç‰ˆæœ¬"""
        # ä½¿ç”¨SequenceMatcher
        matcher = SequenceMatcher(None, version_a, version_b)
        
        differences = []
        
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag != 'equal':
                differences.append({
                    'type': tag,  # 'replace', 'delete', 'insert'
                    'version_a': version_a[i1:i2],
                    'version_b': version_b[j1:j2],
                    'position_a': (i1, i2),
                    'position_b': (j1, j2)
                })
        
        similarity = matcher.ratio()
        
        return {
            'similarity': similarity,
            'differences': differences,
            'total_chars_a': len(version_a),
            'total_chars_b': len(version_b),
            'diff_count': len(differences)
        }
    
    def generate_collation_report(self, base_text: str, variants: Dict[str, str]) -> Dict:
        """ç”Ÿæˆæ ¡å‹˜æŠ¥å‘Š"""
        report = {
            'base_text_length': len(base_text),
            'variant_count': len(variants),
            'comparisons': {}
        }
        
        for version_name, version_text in variants.items():
            comparison = self.compare_versions(base_text, version_text)
            report['comparisons'][version_name] = comparison
        
        return report


class AncientTextPlatform:
    """å¤ç±æ™ºèƒ½å¹³å°ä¸»ç±»"""
    
    def __init__(self):
        self.preprocessor = AncientImagePreprocessor()
        self.ocr = AncientTextOCR()
        self.ner = AncientTextNER()
        self.collation = TextCollation()
        
        # æ•°æ®å­˜å‚¨
        self.pages: Dict[str, AncientTextPage] = {}
        self.books: Dict[str, List[AncientTextPage]] = defaultdict(list)
    
    def process_page(self, page_id: str, image_path: str) -> AncientTextPage:
        """å¤„ç†å•é¡µ"""
        # åŠ è½½å›¾åƒ
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        
        if image is None:
            raise ValueError(f"Cannot load image: {image_path}")
        
        page = AncientTextPage(
            page_id=page_id,
            book_id=page_id.split('_')[0],
            volume=1,
            page_number=int(page_id.split('_')[-1]),
            image_path=image_path,
            image=image
        )
        
        # é¢„å¤„ç†
        processed_image = self.preprocessor.preprocess(image)
        
        # ç‰ˆé¢åˆ†æ
        layout = self.preprocessor.detect_layout(processed_image)
        page.text_regions = layout['text_regions']
        
        # OCRè¯†åˆ«
        text, confidence = self.ocr.recognize(
            processed_image, 
            page.text_regions
        )
        page.raw_text = text
        page.corrected_text = text
        page.confidence = confidence
        
        # å®ä½“è¯†åˆ«
        page.entities = [
            {
                'text': e.text,
                'type': e.entity_type,
                'confidence': e.confidence
            }
            for e in self.ner.extract_entities(text)
        ]
        
        # ä¿å­˜
        self.pages[page_id] = page
        self.books[page.book_id].append(page)
        
        logger.info(f"Processed page {page_id}: {len(text)} chars, "
                   f"{len(page.entities)} entities, confidence {confidence:.2f}")
        
        return page
    
    def search(self, query: str) -> List[Dict]:
        """å…¨æ–‡æ£€ç´¢"""
        results = []
        
        for page in self.pages.values():
            if query in page.corrected_text:
                # æ‰¾åˆ°ä¸Šä¸‹æ–‡
                pos = page.corrected_text.find(query)
                context_start = max(0, pos - 50)
                context_end = min(len(page.corrected_text), pos + len(query) + 50)
                context = page.corrected_text[context_start:context_end]
                
                results.append({
                    'page_id': page.page_id,
                    'book_id': page.book_id,
                    'context': context,
                    'page_number': page.page_number
                })
        
        return results
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_pages = len(self.pages)
        total_chars = sum(len(p.corrected_text) for p in self.pages.values())
        total_entities = sum(len(p.entities) for p in self.pages.values())
        
        entity_types = defaultdict(int)
        for page in self.pages.values():
            for entity in page.entities:
                entity_types[entity['type']] += 1
        
        return {
            'total_pages': total_pages,
            'total_books': len(self.books),
            'total_characters': total_chars,
            'total_entities': total_entities,
            'average_confidence': np.mean([p.confidence for p in self.pages.values()]),
            'entity_distribution': dict(entity_types)
        }


# ==================== æ¼”ç¤º ====================

def demo_platform():
    """æ¼”ç¤ºå¹³å°åŠŸèƒ½"""
    print("=" * 70)
    print("å¤ç±æ•°å­—åŒ–ä¸æ™ºèƒ½åˆ†æå¹³å°æ¼”ç¤º")
    print("=" * 70)
    
    platform = AncientTextPlatform()
    
    # æ¨¡æ‹Ÿå¤„ç†ï¼ˆå®é™…éœ€è¦çœŸå®å›¾åƒï¼‰
    print("\nå¹³å°ç»„ä»¶åˆå§‹åŒ–å®Œæˆï¼š")
    print("  - å›¾åƒé¢„å¤„ç†å™¨")
    print("  - å¤ç±ä¸“ç”¨OCR")
    print("  - å‘½åå®ä½“è¯†åˆ«")
    print("  - æ–‡æœ¬æ ¡å‹˜å·¥å…·")
    
    print("\nç³»ç»ŸåŠŸèƒ½ï¼š")
    print("  1. å¤ç±å›¾åƒæ™ºèƒ½é¢„å¤„ç†")
    print("     - å»å™ªã€å¢å¼ºã€äºŒå€¼åŒ–")
    print("     - ç‰ˆé¢ç»“æ„åˆ†æ")
    print("  2. é«˜ç²¾åº¦æ–‡å­—è¯†åˆ«")
    print("     - æ”¯æŒç¹ä½“ã€å¼‚ä½“å­—")
    print("     - ç½®ä¿¡åº¦è¯„ä¼°")
    print("  3. æ™ºèƒ½å®ä½“æå–")
    print("     - äººåã€åœ°åã€æ—¶é—´")
    print("     - å…³ç³»è¯†åˆ«")
    print("  4. ç‰ˆæœ¬æ¯”å¯¹æ ¡å‹˜")
    print("     - è‡ªåŠ¨å·®å¼‚æ ‡æ³¨")
    print("     - æ ¡å‹˜æŠ¥å‘Šç”Ÿæˆ")
    print("  5. çŸ¥è¯†å›¾è°±æ„å»º")
    print("     - å®ä½“å…³è”")
    print("     - å¯è§†åŒ–å±•ç¤º")
    
    # æ ¡å‹˜æ¼”ç¤º
    print("\n--- æ–‡æœ¬æ ¡å‹˜æ¼”ç¤º ---")
    version_a = "å­”å­æ›°ï¼šå­¦è€Œæ—¶ä¹ ä¹‹ï¼Œä¸äº¦è¯´ä¹ï¼Ÿ"
    version_b = "å­”å­äº‘ï¼šå­¦è€Œæ—¶ä¹ ä¹‹ï¼Œä¸äº¦æ‚¦ä¹ï¼Ÿ"
    
    result = platform.collation.compare_versions(version_a, version_b)
    print(f"ç‰ˆæœ¬A: {version_a}")
    print(f"ç‰ˆæœ¬B: {version_b}")
    print(f"ç›¸ä¼¼åº¦: {result['similarity']:.2%}")
    print(f"å·®å¼‚æ•°: {result['diff_count']}")
    
    for diff in result['differences']:
        print(f"  [{diff['type']}] '{diff['version_a']}' vs '{diff['version_b']}'")
    
    print("\n" + "=" * 70)
    print("æ¼”ç¤ºå®Œæˆ")
    print("=" * 70)


if __name__ == "__main__":
    demo_platform()
```

### 2.6 æ•ˆæœè¯„ä¼°ä¸ROI

| æŒ‡æ ‡ | ä¼ ç»Ÿæ–¹æ³• | æ™ºèƒ½å¹³å° | æå‡å¹…åº¦ |
|------|---------|---------|----------|
| æ•°å­—åŒ–æ•ˆç‡ | 50é¡µ/äººå¤© | 500é¡µ/äººå¤© | **900%æå‡** |
| OCRå‡†ç¡®ç‡ | 65% | 92% | **42%æå‡** |
| å®ä½“æ ‡æ³¨æ•ˆç‡ | æ‰‹å·¥ | è‡ªåŠ¨ | **100%è‡ªåŠ¨åŒ–** |
| æ£€ç´¢å“åº”æ—¶é—´ | åˆ†é’Ÿçº§ | æ¯«ç§’çº§ | **99%ç¼©çŸ­** |
| ç‰ˆæœ¬æ¯”å¯¹æ—¶é—´ | æ•°æœˆ | æ•°å°æ—¶ | **99%ç¼©çŸ­** |

**æŠ•èµ„å›æŠ¥ç‡ï¼ˆROIï¼‰**ï¼š

| é¡¹ç›® | æˆæœ¬/æ”¶ç›Šï¼ˆä¸‡å…ƒï¼‰ |
|------|-----------------|
| å¹³å°å¼€å‘ | -800 |
| è®¾å¤‡é‡‡è´­ | -200 |
| æ•ˆç‡æå‡æ”¶ç›Š | +1200/å¹´ |
| å¼€æ”¾æœåŠ¡æ”¶ç›Š | +300/å¹´ |
| **3å¹´ROI** | **312%** |

---

## 3. æ¡ˆä¾‹2ï¼šå†å²åœ°å›¾æ—¶ç©ºåˆ†æç³»ç»Ÿ

*ï¼ˆç®€åŒ–ç‰ˆï¼‰*

### 3.1 æœºæ„èƒŒæ™¯

æŸå†å²åœ°ç†ç ”ç©¶ä¸­å¿ƒéœ€è¦åˆ†æå†ä»£è¡Œæ”¿åŒºåˆ’å˜è¿å’Œäººå£è¿ç§»è§„å¾‹ã€‚

### 3.2 æŠ€æœ¯æŒ‘æˆ˜

1. **å¤ä»Šåœ°åå¯¹ç…§**ï¼šå†å²åœ°åä¸ç°ä»£åœ°åçš„æ˜ å°„
2. **è¾¹ç•ŒçŸ¢é‡åŒ–**ï¼šå†å²åœ°å›¾è¾¹ç•Œçš„æ•°å­—åŒ–æå–
3. **æ—¶ç©ºæ•°æ®å»ºæ¨¡**ï¼šæ”¯æŒå¤šæ—¶é—´å°ºåº¦çš„æ•°æ®æŸ¥è¯¢

### 3.3 å®Œæ•´ä»£ç å®ç°

```python
#!/usr/bin/env python3
"""
å†å²åœ°å›¾æ—¶ç©ºåˆ†æç³»ç»Ÿ
"""

import json
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import geopandas as gpd
from shapely.geometry import Point, Polygon


@dataclass
class HistoricalPlace:
    """å†å²åœ°ç‚¹"""
    place_id: str
    name: str
    modern_name: Optional[str]
    
    # æ—¶ç©ºèŒƒå›´
    start_year: int
    end_year: int
    
    # åœ°ç†ä½ç½®
    latitude: float
    longitude: float
    
    # è¡Œæ”¿åŒºåˆ’å˜è¿
    parent_regions: List[Dict] = None
    
    def is_active_at(self, year: int) -> bool:
        """æ£€æŸ¥æŸå¹´æ˜¯å¦æœ‰æ•ˆ"""
        return self.start_year <= year <= self.end_year


class HistoricalGIS:
    """å†å²åœ°ç†ä¿¡æ¯ç³»ç»Ÿ"""
    
    def __init__(self):
        self.places: Dict[str, HistoricalPlace] = {}
        self.regions: Dict[str, gpd.GeoDataFrame] = {}
        
    def add_place(self, place: HistoricalPlace):
        """æ·»åŠ åœ°ç‚¹"""
        self.places[place.place_id] = place
        
    def query_by_time(self, year: int) -> List[HistoricalPlace]:
        """æŒ‰æ—¶é—´æŸ¥è¯¢"""
        return [p for p in self.places.values() if p.is_active_at(year)]
    
    def query_by_location(self, lat: float, lon: float, 
                         radius_km: float = 10) -> List[HistoricalPlace]:
        """æŒ‰ä½ç½®æŸ¥è¯¢"""
        center = Point(lon, lat)
        
        results = []
        for place in self.places.values():
            point = Point(place.longitude, place.latitude)
            distance = center.distance(point) * 111  # ç²—ç•¥è½¬æ¢ä¸ºkm
            if distance <= radius_km:
                results.append((place, distance))
        
        results.sort(key=lambda x: x[1])
        return [r[0] for r in results]
    
    def trace_boundary_change(self, region_name: str, 
                             start_year: int, end_year: int) -> List[Dict]:
        """è¿½è¸ªè¾¹ç•Œå˜è¿"""
        changes = []
        
        # ç®€åŒ–ç‰ˆï¼šæŸ¥è¯¢è¯¥æ—¶é—´æ®µå†…çš„æ‰€æœ‰å˜åŒ–
        for year in range(start_year, end_year + 1):
            places = self.query_by_time(year)
            region_places = [p for p in places if p.name == region_name]
            
            if region_places:
                changes.append({
                    'year': year,
                    'place_count': len(region_places),
                    'places': [p.modern_name for p in region_places]
                })
        
        return changes


# æ¼”ç¤º
if __name__ == "__main__":
    print("å†å²åœ°å›¾æ—¶ç©ºåˆ†æç³»ç»Ÿæ¼”ç¤º")
    print("-" * 50)
    
    gis = HistoricalGIS()
    
    # æ·»åŠ å†å²åœ°ç‚¹
    places = [
        HistoricalPlace("P001", "é•¿å®‰", "è¥¿å®‰", 1000, 1400, 34.3, 108.9),
        HistoricalPlace("P002", "æ´›é˜³", "æ´›é˜³", 500, 1500, 34.6, 112.4),
        HistoricalPlace("P003", "å¼€å°", "å¼€å°", 960, 1300, 34.8, 114.3),
    ]
    
    for place in places:
        gis.add_place(place)
    
    # æ—¶é—´æŸ¥è¯¢
    print("\n1100å¹´å­˜åœ¨çš„åœ°ç‚¹:")
    active_places = gis.query_by_time(1100)
    for p in active_places:
        print(f"  - {p.name}ï¼ˆä»Š{p.modern_name}ï¼‰")
    
    # ä½ç½®æŸ¥è¯¢
    print("\nè¥¿å®‰é™„è¿‘çš„å†å²åœ°ç‚¹:")
    nearby = gis.query_by_location(34.3, 108.9, radius_km=200)
    for p in nearby:
        print(f"  - {p.name}")
```

### 3.4 æ•ˆæœè¯„ä¼°ä¸ROI

| åº”ç”¨ | æ•ˆæœ |
|------|------|
| åœ°åæ£€ç´¢ | ç§’çº§å“åº” |
| è¾¹ç•Œå˜è¿å¯è§†åŒ– | æ”¯æŒåŠ¨ç”»å±•ç¤º |
| å­¦æœ¯ç ”ç©¶ | å‘è¡¨è®ºæ–‡20+ç¯‡ |

---

## 4. æ¡ˆä¾‹3ï¼šæ–‡åŒ–é—äº§3Dæ•°å­—åŒ–

*ï¼ˆä¿ç•™åŸæœ‰å†…å®¹ç»“æ„ï¼‰*

## 5. æ¡ˆä¾‹æ€»ç»“

### 5.1 æ¡ˆä¾‹å¯¹æ¯”

| æ¡ˆä¾‹ | æ ¸å¿ƒæŠ€æœ¯ | åº”ç”¨ä»·å€¼ |
|------|---------|---------|
| **å¤ç±æ•°å­—åŒ–** | OCR+NER+çŸ¥è¯†å›¾è°± | æ–‡çŒ®ä¿æŠ¤ã€å­¦æœ¯ç ”ç©¶ |
| **å†å²GIS** | æ—¶ç©ºæ•°æ®+å¯è§†åŒ– | å†å²ç ”ç©¶ã€æ•™å­¦ |
| **3Dæ•°å­—åŒ–** | ä¸‰ç»´é‡å»º+VR | æ–‡ç‰©ä¿æŠ¤ã€å±•ç¤º |

### 5.2 æœ€ä½³å®è·µ

1. **ä¿æŠ¤ä¼˜å…ˆ**ï¼šæ•°å­—åŒ–è¿‡ç¨‹ä¸­ç¡®ä¿åŸä»¶å®‰å…¨
2. **æ ‡å‡†è§„èŒƒ**ï¼šéµå¾ªå›½é™…æ•°å­—äººæ–‡æ ‡å‡†
3. **å¼€æ”¾å…±äº«**ï¼šä¿ƒè¿›å­¦æœ¯äº¤æµå’Œå…¬ä¼—æ•™è‚²
4. **é•¿æœŸä¿å­˜**ï¼šå»ºç«‹å¯æŒç»­çš„æ•°å­—ä¿å­˜ç­–ç•¥
5. **è·¨å­¦ç§‘åˆä½œ**ï¼šç»“åˆäººæ–‡ä¸è®¡ç®—ä¸“ä¸šçŸ¥è¯†

---

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-02-15
**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv2.0
**ç»´æŠ¤è€…**ï¼šDSL Schemaç ”ç©¶å›¢é˜Ÿ
