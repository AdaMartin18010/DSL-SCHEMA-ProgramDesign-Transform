# Thread Schemaå®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [Thread Schemaå®è·µæ¡ˆä¾‹](#thread-schemaå®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¡ˆä¾‹æ¦‚è¿°](#1-æ¡ˆä¾‹æ¦‚è¿°)
  - [2. æ¡ˆä¾‹1ï¼šThread Meshç½‘ç»œæ„å»º](#2-æ¡ˆä¾‹1thread-meshç½‘ç»œæ„å»º)
    - [2.1 åœºæ™¯æè¿°](#21-åœºæ™¯æè¿°)
    - [2.2 Schemaå®šä¹‰](#22-schemaå®šä¹‰)
    - [2.3 å®ç°ä»£ç ](#23-å®ç°ä»£ç )
  - [3. æ¡ˆä¾‹2ï¼šThreadè·¯ç”±ç®¡ç†å’Œä¼˜åŒ–](#3-æ¡ˆä¾‹2threadè·¯ç”±ç®¡ç†å’Œä¼˜åŒ–)
    - [3.1 åœºæ™¯æè¿°](#31-åœºæ™¯æè¿°)
    - [3.2 Schemaå®šä¹‰](#32-schemaå®šä¹‰)
    - [3.3 å®ç°ä»£ç ](#33-å®ç°ä»£ç )
  - [4. æ¡ˆä¾‹3ï¼šThreadç½‘ç»œå®‰å…¨å’Œå¯†é’¥ç®¡ç†](#4-æ¡ˆä¾‹3threadç½‘ç»œå®‰å…¨å’Œå¯†é’¥ç®¡ç†)
    - [4.1 åœºæ™¯æè¿°](#41-åœºæ™¯æè¿°)
    - [4.2 Schemaå®šä¹‰](#42-schemaå®šä¹‰)
    - [4.3 å®ç°ä»£ç ](#43-å®ç°ä»£ç )
  - [5. æ¡ˆä¾‹4ï¼šThreadç½‘ç»œæ€§èƒ½ç›‘æ§](#5-æ¡ˆä¾‹4threadç½‘ç»œæ€§èƒ½ç›‘æ§)
    - [5.1 åœºæ™¯æè¿°](#51-åœºæ™¯æè¿°)
    - [5.2 å®ç°ä»£ç ](#52-å®ç°ä»£ç )
  - [6. æ¡ˆä¾‹5ï¼šThreadåˆ°Zigbeeç½‘ç»œè½¬æ¢](#6-æ¡ˆä¾‹5threadåˆ°zigbeeç½‘ç»œè½¬æ¢)
    - [6.1 åœºæ™¯æè¿°](#61-åœºæ™¯æè¿°)
    - [6.2 å®ç°ä»£ç ](#62-å®ç°ä»£ç )
  - [7. æ¡ˆä¾‹6ï¼šThreadæ•°æ®å­˜å‚¨å’Œåˆ†æç³»ç»Ÿ](#7-æ¡ˆä¾‹6threadæ•°æ®å­˜å‚¨å’Œåˆ†æç³»ç»Ÿ)
    - [7.1 åœºæ™¯æè¿°](#71-åœºæ™¯æè¿°)
    - [7.2 å®ç°ä»£ç ](#72-å®ç°ä»£ç )
    - [7.3 æ•°æ®åˆ†æç¤ºä¾‹](#73-æ•°æ®åˆ†æç¤ºä¾‹)
  - [8. æ¡ˆä¾‹7ï¼šThreadç½‘ç»œæ‰©å±•ï¼ˆæ·»åŠ æ–°èŠ‚ç‚¹ï¼‰](#8-æ¡ˆä¾‹7threadç½‘ç»œæ‰©å±•æ·»åŠ æ–°èŠ‚ç‚¹)
    - [8.1 åœºæ™¯æè¿°](#81-åœºæ™¯æè¿°)
    - [8.2 Schemaå®šä¹‰](#82-schemaå®šä¹‰)
    - [8.3 å®ç°ä»£ç ](#83-å®ç°ä»£ç )
  - [9. æ¡ˆä¾‹8ï¼šThreadç½‘ç»œæ•…éšœæ¢å¤](#9-æ¡ˆä¾‹8threadç½‘ç»œæ•…éšœæ¢å¤)
    - [9.1 åœºæ™¯æè¿°](#91-åœºæ™¯æè¿°)
    - [9.2 Schemaå®šä¹‰](#92-schemaå®šä¹‰)
    - [9.3 å®ç°ä»£ç ](#93-å®ç°ä»£ç )
  - [10. æ¡ˆä¾‹9ï¼šå¤§è§„æ¨¡Threadç½‘ç»œç®¡ç†](#10-æ¡ˆä¾‹9å¤§è§„æ¨¡threadç½‘ç»œç®¡ç†)
    - [10.1 åœºæ™¯æè¿°](#101-åœºæ™¯æè¿°)
    - [10.2 Schemaå®šä¹‰](#102-schemaå®šä¹‰)
    - [10.3 å®ç°ä»£ç ](#103-å®ç°ä»£ç )
  - [11. æ¡ˆä¾‹10ï¼šThreadç½‘ç»œæ€§èƒ½ä¼˜åŒ–](#11-æ¡ˆä¾‹10threadç½‘ç»œæ€§èƒ½ä¼˜åŒ–)
    - [11.1 åœºæ™¯æè¿°](#111-åœºæ™¯æè¿°)
    - [11.2 Schemaå®šä¹‰](#112-schemaå®šä¹‰)
    - [11.3 å®ç°ä»£ç ](#113-å®ç°ä»£ç )
  - [12. æ¡ˆä¾‹11ï¼šThreadç½‘ç»œå®‰å…¨åŠ å›º](#12-æ¡ˆä¾‹11threadç½‘ç»œå®‰å…¨åŠ å›º)
    - [12.1 åœºæ™¯æè¿°](#121-åœºæ™¯æè¿°)
    - [12.2 Schemaå®šä¹‰](#122-schemaå®šä¹‰)
    - [12.3 å®ç°ä»£ç ](#123-å®ç°ä»£ç )

---

## 1. æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›Thread Schemaåœ¨å®é™…åº”ç”¨ä¸­çš„å®è·µæ¡ˆä¾‹ã€‚

---

## 2. æ¡ˆä¾‹1ï¼šThread Meshç½‘ç»œæ„å»º

### 2.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
ç”¨æˆ·éœ€è¦æ„å»ºä¸€ä¸ªThread Meshç½‘ç»œï¼Œè¿æ¥å¤šä¸ªæ™ºèƒ½å®¶å±…è®¾å¤‡ï¼Œ
åŒ…æ‹¬æ™ºèƒ½ç¯ã€ä¼ æ„Ÿå™¨ã€é—¨é”ç­‰ï¼Œå®ç°è®¾å¤‡é—´çš„Meshé€šä¿¡ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦åˆ›å»ºThreadç½‘ç»œå¹¶é…ç½®ç½‘ç»œå‚æ•°
- éœ€è¦å°†å¤šä¸ªè®¾å¤‡åŠ å…¥ç½‘ç»œ
- éœ€è¦ç®¡ç†ç½‘ç»œæ‹“æ‰‘
- éœ€è¦ç›‘æ§ç½‘ç»œçŠ¶æ€

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadNetworkManageråˆ›å»ºå’Œç®¡ç†Threadç½‘ç»œï¼Œå®ç°è®¾å¤‡çš„
è‡ªåŠ¨å‘ç°å’ŒåŠ å…¥ç½‘ç»œåŠŸèƒ½ã€‚

### 2.2 Schemaå®šä¹‰

è¯¦è§ç¬¬2.2èŠ‚åŸå§‹å®šä¹‰ã€‚

### 2.3 å®ç°ä»£ç 

**å®Œæ•´çš„Thread Meshç½‘ç»œæ„å»ºå®ç°**ï¼š

```python
import asyncio
from thread_network_manager import ThreadNetworkManager
from thread_storage import ThreadStorage

# åˆå§‹åŒ–å­˜å‚¨å’Œç®¡ç†å™¨
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
network_manager = ThreadNetworkManager()

async def build_thread_mesh_network():
    """æ„å»ºThread Meshç½‘ç»œ"""
    try:
        # åˆ›å»ºThreadç½‘ç»œ
        network_name = "SmartHomeNet"
        pan_id = 0x1234
        channel = 15
        network_key = "00112233445566778899AABBCCDDEEFF"

        print(f"Creating Thread network: {network_name}")
        success = network_manager.create_network(
            network_name, pan_id, channel, network_key
        )

        if not success:
            print("Failed to create network")
            return

        # å­˜å‚¨ç½‘ç»œä¿¡æ¯
        network_data = {
            "network_name": network_name,
            "pan_id": pan_id,
            "extended_pan_id": "DEADBEEF00CAFE00",
            "channel": channel,
            "network_key": network_key,
            "partition_id": 1
        }
        storage.store_network(network_data)

        # èŠ‚ç‚¹åˆ—è¡¨
        nodes = [
            {"node_id": "000D6F0000123456", "type": "Router"},
            {"node_id": "000D6F0000654321", "type": "EndDevice"},
            {"node_id": "000D6F0000789012", "type": "EndDevice"}
        ]

        # å°†èŠ‚ç‚¹åŠ å…¥ç½‘ç»œ
        for node in nodes:
            print(f"Joining node {node['node_id']} to network...")
            success = network_manager.join_network(
                node["node_id"],
                network_name,
                network_key,
                pan_id,
                channel
            )

            if success:
                # è·å–èŠ‚ç‚¹ä¿¡æ¯
                node_info = network_manager.get_node_info(node["node_id"])
                if node_info:
                    # å­˜å‚¨èŠ‚ç‚¹ä¿¡æ¯
                    storage.store_node(node_info)
                    print(f"Node {node['node_id']} joined successfully")
                    print(f"  Type: {node_info['node_type']}")
                    print(f"  Mesh Local: {node_info['mesh_local_address']}")
            else:
                print(f"Failed to join node {node['node_id']}")

        # è·å–ç½‘ç»œæ‹“æ‰‘
        topology = network_manager.get_network_topology(network_name)
        print(f"\nNetwork Topology:")
        print(f"  Total nodes: {len(topology['nodes'])}")
        print(f"  Routers: {len(topology['routers'])}")
        print(f"  End Devices: {len(topology['end_devices'])}")

        # æŸ¥è¯¢ç½‘ç»œå¥åº·çŠ¶æ€
        health = storage.get_network_health_status(network_name)
        print(f"\nNetwork Health:")
        print(f"  Health Score: {health['health_score']:.1f}/100")
        print(f"  Avg Link Quality: {health['avg_link_quality']:.1f}")
        print(f"  Low Battery Nodes: {health['low_battery_count']}")

    except Exception as e:
        print(f"Error building network: {e}")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(build_thread_mesh_network())
```

---

## 3. æ¡ˆä¾‹2ï¼šThreadè·¯ç”±ç®¡ç†å’Œä¼˜åŒ–

### 3.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
ç³»ç»Ÿéœ€è¦ç®¡ç†Threadç½‘ç»œçš„è·¯ç”±è¡¨ï¼Œç›‘æ§è·¯ç”±æ€§èƒ½ï¼Œ
å¹¶åœ¨è·¯ç”±å¤±æ•ˆæ—¶è‡ªåŠ¨æ›´æ–°è·¯ç”±è¡¨ï¼Œç¡®ä¿ç½‘ç»œè¿é€šæ€§ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®šæœŸæ›´æ–°è·¯ç”±è¡¨
- éœ€è¦ç›‘æ§è·¯ç”±æ€§èƒ½
- éœ€è¦æ£€æµ‹è·¯ç”±å¤±æ•ˆ
- éœ€è¦ä¼˜åŒ–è·¯ç”±è·¯å¾„

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadRoutingManagerç®¡ç†è·¯ç”±è¡¨ï¼Œå®šæœŸæ›´æ–°è·¯ç”±ä¿¡æ¯
å¹¶ç›‘æ§è·¯ç”±æ€§èƒ½ã€‚

### 3.2 Schemaå®šä¹‰

è¯¦è§ç¬¬3.2èŠ‚åŸå§‹å®šä¹‰ã€‚

### 3.3 å®ç°ä»£ç 

**å®Œæ•´çš„è·¯ç”±ç®¡ç†å®ç°**ï¼š

```python
import asyncio
from thread_network_manager import ThreadNetworkManager
from thread_routing_manager import ThreadRoutingManager
from thread_storage import ThreadStorage
import time

# åˆå§‹åŒ–ç»„ä»¶
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
network_manager = ThreadNetworkManager()
routing_manager = ThreadRoutingManager(network_manager)

async def manage_thread_routing():
    """ç®¡ç†Threadè·¯ç”±"""
    try:
        network_name = "SmartHomeNet"

        # è·å–ç½‘ç»œä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
        topology = network_manager.get_network_topology(network_name)
        nodes = topology["nodes"]

        print(f"Managing routing for {len(nodes)} nodes")

        # æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹çš„è·¯ç”±è¡¨
        for node in nodes:
            node_id = node["node_id"]
            print(f"Updating routing table for node {node_id}...")

            # æ›´æ–°è·¯ç”±è¡¨
            routing_manager.update_routing_table(node_id)

            # è·å–è·¯ç”±è¡¨
            routes = routing_manager.get_node_routes(node_id)
            print(f"  Found {len(routes)} routes")

            # å­˜å‚¨è·¯ç”±è¡¨åˆ°æ•°æ®åº“
            storage.store_routing_table(node_id, routes)

            # æ˜¾ç¤ºè·¯ç”±ç»Ÿè®¡
            route_stats = storage.get_routing_statistics(node_id)
            print(f"  Route count: {route_stats['route_count']}")
            print(f"  Avg cost: {route_stats['avg_cost']:.2f}")
            print(f"  Max hops: {route_stats['max_cost']}")

        # è·å–ç½‘ç»œè·¯ç”±ç»Ÿè®¡
        network_stats = routing_manager.get_network_routing_statistics(network_name)
        print(f"\nNetwork Routing Statistics:")
        print(f"  Total nodes: {network_stats['total_nodes']}")
        print(f"  Total routes: {network_stats['total_routes']}")
        print(f"  Avg routes per node: {network_stats['avg_routes_per_node']:.2f}")
        print(f"  Avg cost: {network_stats['avg_cost']:.2f}")
        print(f"  Max hops: {network_stats['max_hops']}")

        # æµ‹è¯•è·¯ç”±æŸ¥æ‰¾
        if len(nodes) >= 2:
            source_node = nodes[0]["node_id"]
            dest_address = nodes[1].get("mesh_local_address")

            if dest_address:
                route = routing_manager.find_route(source_node, dest_address)
                if route:
                    print(f"\nRoute from {source_node} to {dest_address}:")
                    print(f"  Next hop: {route['next_hop']}")
                    print(f"  Cost: {route['cost']}")
                else:
                    print(f"\nNo route found from {source_node} to {dest_address}")

    except Exception as e:
        print(f"Error managing routing: {e}")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(manage_thread_routing())
```

---

## 4. æ¡ˆä¾‹3ï¼šThreadç½‘ç»œå®‰å…¨å’Œå¯†é’¥ç®¡ç†

### 4.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
ç³»ç»Ÿéœ€è¦ç®¡ç†Threadç½‘ç»œçš„å®‰å…¨å¯†é’¥ï¼Œå®šæœŸè½®æ¢ç½‘ç»œå¯†é’¥ï¼Œ
ç¡®ä¿ç½‘ç»œå®‰å…¨ï¼Œå¹¶ç›‘æ§å®‰å…¨äº‹ä»¶ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®šæœŸè½®æ¢ç½‘ç»œå¯†é’¥
- éœ€è¦åŒæ­¥å¯†é’¥åˆ°æ‰€æœ‰èŠ‚ç‚¹
- éœ€è¦ç›‘æ§å®‰å…¨äº‹ä»¶
- éœ€è¦å¤„ç†å¯†é’¥è½®æ¢å¤±è´¥çš„æƒ…å†µ

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadSecurityManagerç®¡ç†ç½‘ç»œå¯†é’¥ï¼Œå®ç°å¯†é’¥è½®æ¢
å’Œå®‰å…¨ç›‘æ§åŠŸèƒ½ã€‚

### 4.2 Schemaå®šä¹‰

è¯¦è§ç¬¬4.2èŠ‚åŸå§‹å®šä¹‰ã€‚

### 4.3 å®ç°ä»£ç 

**å®Œæ•´çš„å®‰å…¨ç®¡ç†å®ç°**ï¼š

```python
import asyncio
from thread_security_manager import ThreadSecurityManager
from thread_storage import ThreadStorage
from datetime import datetime, timedelta

# åˆå§‹åŒ–ç»„ä»¶
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
security_manager = ThreadSecurityManager(storage)

async def manage_thread_security():
    """ç®¡ç†Threadç½‘ç»œå®‰å…¨"""
    try:
        network_name = "SmartHomeNet"

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢å¯†é’¥
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“æŸ¥è¯¢ä¸Šæ¬¡è½®æ¢æ—¶é—´
        last_rotation_time = datetime.now() - timedelta(days=2)
        rotation_interval = timedelta(days=1)

        if datetime.now() - last_rotation_time > rotation_interval:
            print("Rotating network key...")
            success = security_manager.rotate_network_key(network_name)

            if success:
                print("Network key rotated successfully")
                # è¿™é‡Œéœ€è¦é€šçŸ¥æ‰€æœ‰èŠ‚ç‚¹æ›´æ–°å¯†é’¥
                # å®é™…å®ç°éœ€è¦è°ƒç”¨OpenThread APIæ›´æ–°æ‰€æœ‰èŠ‚ç‚¹
            else:
                print("Failed to rotate network key")
        else:
            print("Network key is still valid, no rotation needed")

        # æŸ¥è¯¢å®‰å…¨ç»Ÿè®¡
        # è¿™é‡Œå¯ä»¥æŸ¥è¯¢å®‰å…¨äº‹ä»¶ã€å¯†é’¥ä½¿ç”¨æƒ…å†µç­‰

    except Exception as e:
        print(f"Error managing security: {e}")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(manage_thread_security())
```

---

## 5. æ¡ˆä¾‹4ï¼šThreadç½‘ç»œæ€§èƒ½ç›‘æ§

### 5.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
ç³»ç»Ÿéœ€è¦ç›‘æ§Threadç½‘ç»œçš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å»¶è¿Ÿã€ä¸¢åŒ…ç‡ã€
ååé‡ç­‰ï¼ŒåŠæ—¶å‘ç°ç½‘ç»œé—®é¢˜å¹¶ä¼˜åŒ–ç½‘ç»œæ€§èƒ½ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®æ—¶æ”¶é›†æ€§èƒ½æ•°æ®
- éœ€è¦åˆ†ææ€§èƒ½è¶‹åŠ¿
- éœ€è¦è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
- éœ€è¦ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadStorageå­˜å‚¨æ€§èƒ½æ•°æ®ï¼Œå®šæœŸæ”¶é›†å’Œåˆ†æç½‘ç»œæ€§èƒ½ã€‚

### 5.2 å®ç°ä»£ç 

**å®Œæ•´çš„æ€§èƒ½ç›‘æ§å®ç°**ï¼š

```python
import asyncio
from thread_network_manager import ThreadNetworkManager
from thread_storage import ThreadStorage
import time

# åˆå§‹åŒ–ç»„ä»¶
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
network_manager = ThreadNetworkManager()

async def monitor_thread_performance():
    """ç›‘æ§Threadç½‘ç»œæ€§èƒ½"""
    try:
        network_name = "SmartHomeNet"

        # è·å–ç½‘ç»œä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
        topology = network_manager.get_network_topology(network_name)
        nodes = topology["nodes"]

        print(f"Monitoring performance for {len(nodes)} nodes")

        # æ”¶é›†æ€§èƒ½æ•°æ®
        for node in nodes:
            node_id = node["node_id"]

            # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®æ”¶é›†
            # å®é™…å®ç°éœ€è¦ä»OpenThreadè·å–çœŸå®æ•°æ®
            performance_data = {
                "latency_ms": 50 + (hash(node_id) % 50),  # æ¨¡æ‹Ÿå»¶è¿Ÿ
                "packet_loss_rate": (hash(node_id) % 5) / 100.0,  # æ¨¡æ‹Ÿä¸¢åŒ…ç‡
                "throughput_kbps": 100 + (hash(node_id) % 200)  # æ¨¡æ‹Ÿååé‡
            }

            # å­˜å‚¨æ€§èƒ½æ•°æ®
            storage.store_performance_data(node_id, performance_data)

            print(f"Node {node_id}:")
            print(f"  Latency: {performance_data['latency_ms']}ms")
            print(f"  Packet Loss: {performance_data['packet_loss_rate']*100:.2f}%")
            print(f"  Throughput: {performance_data['throughput_kbps']}kbps")

        # æŸ¥è¯¢ç½‘ç»œæ€§èƒ½ç»Ÿè®¡
        perf_stats = storage.get_network_performance_statistics(network_name, hours=24)
        print(f"\nNetwork Performance Statistics (24h):")
        print(f"  Avg Latency: {perf_stats['avg_latency']:.2f}ms")
        print(f"  Avg Packet Loss: {perf_stats['avg_packet_loss']*100:.2f}%")
        print(f"  Avg Throughput: {perf_stats['avg_throughput']:.2f}kbps")
        print(f"  Monitored Nodes: {perf_stats['monitored_nodes']}")

        # æŸ¥è¯¢èŠ‚ç‚¹æ€§èƒ½å†å²
        if nodes:
            node_id = nodes[0]["node_id"]
            history = storage.get_node_performance_history(node_id, hours=1)
            print(f"\nPerformance History for {node_id} (1h):")
            print(f"  Data points: {len(history)}")
            if history:
                print(f"  Latest latency: {history[0]['latency_ms']}ms")

    except Exception as e:
        print(f"Error monitoring performance: {e}")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(monitor_thread_performance())
```

---

## 6. æ¡ˆä¾‹5ï¼šThreadåˆ°Zigbeeç½‘ç»œè½¬æ¢

### 6.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
éœ€è¦å°†Threadç½‘ç»œè½¬æ¢ä¸ºZigbeeç½‘ç»œï¼Œä»¥ä¾¿ä¸ç°æœ‰çš„Zigbeeç³»ç»Ÿé›†æˆï¼Œ
å®ç°è·¨åè®®è®¾å¤‡é€šä¿¡ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦è½¬æ¢èŠ‚ç‚¹ç±»å‹
- éœ€è¦è½¬æ¢ç½‘ç»œåœ°å€
- éœ€è¦ä¿æŒç½‘ç»œæ‹“æ‰‘ä¸€è‡´æ€§
- éœ€è¦å¤„ç†åè®®å·®å¼‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadToZigbeeConverterå®ç°Threadåˆ°Zigbeeçš„è½¬æ¢ã€‚

### 6.2 å®ç°ä»£ç 

è¯¦è§ `04_Transformation.md` ç¬¬2.2ç« ã€‚

---

## 7. æ¡ˆä¾‹6ï¼šThreadæ•°æ®å­˜å‚¨å’Œåˆ†æç³»ç»Ÿ

### 7.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
ä½¿ç”¨PostgreSQLå­˜å‚¨Threadç½‘ç»œæ•°æ®ï¼Œæ”¯æŒç½‘ç»œæ‹“æ‰‘åˆ†æã€
æ€§èƒ½ç›‘æ§å’Œå¥åº·çŠ¶æ€è¯„ä¼°ã€‚

### 7.2 å®ç°ä»£ç 

è¯¦è§ `04_Transformation.md` ç¬¬6ç« ã€‚

### 7.3 æ•°æ®åˆ†æç¤ºä¾‹

**ç½‘ç»œå¥åº·çŠ¶æ€æŸ¥è¯¢**ï¼š

```python
from thread_storage import ThreadStorage

storage = ThreadStorage("postgresql://user:pass@localhost/thread")

# æŸ¥è¯¢ç½‘ç»œå¥åº·çŠ¶æ€
health = storage.get_network_health_status("SmartHomeNet")
print("Network Health Status:")
print(f"  Total Nodes: {health['total_nodes']}")
print(f"  Routers: {health['router_count']}")
print(f"  End Devices: {health['end_device_count']}")
print(f"  Avg Link Quality: {health['avg_link_quality']:.1f}")
print(f"  Avg RSSI: {health['avg_rssi']:.1f}dBm")
print(f"  Low Battery Nodes: {health['low_battery_count']}")
print(f"  Health Score: {health['health_score']:.1f}/100")

# æŸ¥è¯¢ç½‘ç»œæ‹“æ‰‘ç»Ÿè®¡
topology_stats = storage.get_network_topology_statistics("SmartHomeNet")
print("\nTopology Statistics:")
for stat in topology_stats:
    print(f"  {stat['node_type']}: {stat['count']} nodes")
    print(f"    Avg Link Quality: {stat['avg_link_quality']:.1f}")
    print(f"    Avg RSSI: {stat['avg_rssi']:.1f}dBm")
```

---

## 8. æ¡ˆä¾‹7ï¼šThreadç½‘ç»œæ‰©å±•ï¼ˆæ·»åŠ æ–°èŠ‚ç‚¹ï¼‰

### 8.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
ç”¨æˆ·éœ€è¦åœ¨ç°æœ‰çš„Threadç½‘ç»œä¸­åŠ¨æ€æ·»åŠ æ–°çš„æ™ºèƒ½è®¾å¤‡ï¼ŒåŒ…æ‹¬æ–°çš„Routerå’ŒEnd Deviceï¼Œ
ç¡®ä¿æ–°èŠ‚ç‚¹èƒ½å¤Ÿé¡ºåˆ©åŠ å…¥ç½‘ç»œå¹¶å»ºç«‹æ­£ç¡®çš„è·¯ç”±å…³ç³»ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦æ£€æµ‹æ–°èŠ‚ç‚¹å¹¶å¼•å¯¼å…¶åŠ å…¥ç½‘ç»œ
- éœ€è¦ä¸ºæ–°èŠ‚ç‚¹åˆ†é…ç½‘ç»œåœ°å€å’Œè·¯ç”±ä¿¡æ¯
- éœ€è¦æ›´æ–°ç°æœ‰èŠ‚ç‚¹çš„è·¯ç”±è¡¨
- éœ€è¦å¤„ç†èŠ‚ç‚¹åŠ å…¥å¤±è´¥çš„æƒ…å†µ
- éœ€è¦ä¼˜åŒ–ç½‘ç»œæ‹“æ‰‘ä»¥é€‚åº”æ–°èŠ‚ç‚¹

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadNetworkManagerçš„èŠ‚ç‚¹åŠ å…¥åŠŸèƒ½ï¼Œç»“åˆè·¯ç”±ç®¡ç†å™¨çš„è‡ªåŠ¨è·¯ç”±æ›´æ–°ï¼Œ
å®ç°æ–°èŠ‚ç‚¹çš„æ— ç¼åŠ å…¥å’Œç½‘ç»œæ‰©å±•ã€‚

### 8.2 Schemaå®šä¹‰

**ç½‘ç»œæ‰©å±•Schema**ï¼š

```json
{
  "network_name": "SmartHomeNet",
  "new_nodes": [
    {
      "node_id": "000D6F0000ABCDEF",
      "node_type": "Router",
      "expected_role": "Router",
      "join_credentials": {
        "network_key": "00112233445566778899AABBCCDDEEFF",
        "pan_id": 4660,
        "channel": 15
      }
    },
    {
      "node_id": "000D6F0000FEDCBA",
      "node_type": "EndDevice",
      "expected_role": "EndDevice",
      "join_credentials": {
        "network_key": "00112233445566778899AABBCCDDEEFF",
        "pan_id": 4660,
        "channel": 15
      }
    }
  ],
  "expansion_strategy": {
    "update_routing_tables": true,
    "optimize_topology": true,
    "verify_connectivity": true
  }
}
```

### 8.3 å®ç°ä»£ç 

**å®Œæ•´çš„ç½‘ç»œæ‰©å±•å®ç°**ï¼š

```python
import logging
from datetime import datetime
from thread_network_manager import ThreadNetworkManager
from thread_routing_manager import ThreadRoutingManager
from thread_storage import ThreadStorage
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–ç»„ä»¶
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
network_manager = ThreadNetworkManager()
routing_manager = ThreadRoutingManager(network_manager)

def expand_thread_network(network_name: str, new_nodes: List[Dict]) -> Dict:
    """æ‰©å±•Threadç½‘ç»œ - æ·»åŠ æ–°èŠ‚ç‚¹"""
    results = {
        "network_name": network_name,
        "timestamp": datetime.now().isoformat(),
        "successful_joins": [],
        "failed_joins": [],
        "routing_updates": [],
        "topology_changes": {}
    }

    try:
        # 1. è·å–å½“å‰ç½‘ç»œçŠ¶æ€
        current_topology = network_manager.get_network_topology(network_name)
        initial_node_count = len(current_topology["nodes"])
        logger.info(f"Current network has {initial_node_count} nodes")

        # 2. æ·»åŠ æ–°èŠ‚ç‚¹
        for new_node in new_nodes:
            node_id = new_node["node_id"]
            node_type = new_node.get("node_type", "EndDevice")
            credentials = new_node.get("join_credentials", {})

            logger.info(f"Adding new node {node_id} (type: {node_type})...")

            try:
                # åŠ å…¥ç½‘ç»œ
                success = network_manager.join_network(
                    node_id,
                    network_name,
                    credentials.get("network_key"),
                    credentials.get("pan_id"),
                    credentials.get("channel")
                )

                if success:
                    # è·å–èŠ‚ç‚¹ä¿¡æ¯
                    node_info = network_manager.get_node_info(node_id)
                    if node_info:
                        # å­˜å‚¨èŠ‚ç‚¹ä¿¡æ¯
                        storage.store_node(node_info)

                        # è®°å½•äº‹ä»¶
                        storage.store_event(
                            network_name,
                            "node_joined",
                            {
                                "node_id": node_id,
                                "node_type": node_type,
                                "mesh_local_address": node_info.get("mesh_local_address")
                            },
                            node_id
                        )

                        results["successful_joins"].append({
                            "node_id": node_id,
                            "node_type": node_type,
                            "mesh_local_address": node_info.get("mesh_local_address"),
                            "parent_node_id": node_info.get("parent_node_id"),
                            "router_id": node_info.get("router_id")
                        })

                        logger.info(f"Node {node_id} joined successfully")

                        # ç­‰å¾…èŠ‚ç‚¹ç¨³å®š
                        time.sleep(2)
                    else:
                        raise Exception("Failed to get node info after join")
                else:
                    raise Exception("Failed to join network")

            except Exception as e:
                logger.error(f"Failed to add node {node_id}: {e}")
                results["failed_joins"].append({
                    "node_id": node_id,
                    "error": str(e)
                })

        # 3. æ›´æ–°è·¯ç”±è¡¨
        logger.info("Updating routing tables...")
        updated_count = routing_manager.update_all_routing_tables(network_name)
        results["routing_updates"] = {
            "updated_nodes": updated_count,
            "timestamp": datetime.now().isoformat()
        }

        # 4. ä¼˜åŒ–è·¯ç”±è¡¨
        logger.info("Optimizing routing tables...")
        topology = network_manager.get_network_topology(network_name)
        optimized_count = 0
        for node in topology["nodes"]:
            node_id = node["node_id"]
            if routing_manager.optimize_routing_table(node_id):
                optimized_count += 1

        results["routing_updates"]["optimized_nodes"] = optimized_count

        # 5. éªŒè¯è¿é€šæ€§
        logger.info("Verifying network connectivity...")
        final_topology = network_manager.get_network_topology(network_name)
        final_node_count = len(final_topology["nodes"])

        results["topology_changes"] = {
            "initial_node_count": initial_node_count,
            "final_node_count": final_node_count,
            "added_nodes": final_node_count - initial_node_count,
            "router_count": len(final_topology["routers"]),
            "end_device_count": len(final_topology["end_devices"])
        }

        # 6. ç½‘ç»œè¯Šæ–­
        logger.info("Running network diagnosis...")
        diagnosis = network_manager.diagnose_network(network_name)
        storage.store_diagnosis(network_name, diagnosis)

        results["diagnosis"] = {
            "issues_count": len(diagnosis.get("issues", [])),
            "connectivity_score": len(diagnosis["connectivity"]["connected_nodes"]) / final_node_count if final_node_count > 0 else 0,
            "routing_efficiency": diagnosis["routing"].get("complete_tables", [])
        }

        logger.info(f"Network expansion completed: {len(results['successful_joins'])} nodes added")
        return results

    except Exception as e:
        logger.error(f"Network expansion failed: {e}")
        results["error"] = str(e)
        return results

# æµ‹è¯•ç”¨ä¾‹
def test_network_expansion():
    """æµ‹è¯•ç½‘ç»œæ‰©å±•"""
    network_name = "SmartHomeNet"

    new_nodes = [
        {
            "node_id": "000D6F0000ABCDEF",
            "node_type": "Router",
            "join_credentials": {
                "network_key": "00112233445566778899AABBCCDDEEFF",
                "pan_id": 4660,
                "channel": 15
            }
        },
        {
            "node_id": "000D6F0000FEDCBA",
            "node_type": "EndDevice",
            "join_credentials": {
                "network_key": "00112233445566778899AABBCCDDEEFF",
                "pan_id": 4660,
                "channel": 15
            }
        }
    ]

    results = expand_thread_network(network_name, new_nodes)

    print(f"\nNetwork Expansion Results:")
    print(f"  Successful joins: {len(results['successful_joins'])}")
    print(f"  Failed joins: {len(results['failed_joins'])}")
    print(f"  Initial nodes: {results['topology_changes']['initial_node_count']}")
    print(f"  Final nodes: {results['topology_changes']['final_node_count']}")
    print(f"  Added nodes: {results['topology_changes']['added_nodes']}")

    if results.get("diagnosis"):
        print(f"  Connectivity score: {results['diagnosis']['connectivity_score']:.2%}")
        print(f"  Issues found: {results['diagnosis']['issues_count']}")

if __name__ == "__main__":
    test_network_expansion()
```

**è¿è¡Œç»“æœç¤ºä¾‹**ï¼š

```text
INFO:__main__:Current network has 3 nodes
INFO:__main__:Adding new node 000D6F0000ABCDEF (type: Router)...
INFO:__main__:Node 000D6F0000ABCDEF joined successfully
INFO:__main__:Adding new node 000D6F0000FEDCBA (type: EndDevice)...
INFO:__main__:Node 000D6F0000FEDCBA joined successfully
INFO:__main__:Updating routing tables...
INFO:__main__:Updated routing tables for 5/5 nodes
INFO:__main__:Optimizing routing tables...
INFO:__main__:Running network diagnosis...
INFO:__main__:Network expansion completed: 2 nodes added

Network Expansion Results:
  Successful joins: 2
  Failed joins: 0
  Initial nodes: 3
  Final nodes: 5
  Added nodes: 2
  Connectivity score: 100.00%
  Issues found: 0
```

---

## 9. æ¡ˆä¾‹8ï¼šThreadç½‘ç»œæ•…éšœæ¢å¤

### 9.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
Threadç½‘ç»œä¸­çš„æŸäº›èŠ‚ç‚¹å¯èƒ½å‡ºç°æ•…éšœï¼ˆå¦‚æ–­ç”µã€ä¿¡å·ä¸¢å¤±ã€ç¡¬ä»¶æ•…éšœï¼‰ï¼Œ
ç³»ç»Ÿéœ€è¦è‡ªåŠ¨æ£€æµ‹æ•…éšœï¼Œæ‰§è¡Œæ•…éšœæ¢å¤æµç¨‹ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹é‡æ–°åŠ å…¥ã€è·¯ç”±é‡å»ºã€
ç½‘ç»œåˆ†åŒºåˆå¹¶ç­‰æ“ä½œã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®æ—¶æ£€æµ‹èŠ‚ç‚¹æ•…éšœ
- éœ€è¦åŒºåˆ†ä¸´æ—¶æ•…éšœå’Œæ°¸ä¹…æ•…éšœ
- éœ€è¦è‡ªåŠ¨æ‰§è¡Œæ¢å¤æµç¨‹
- éœ€è¦å¤„ç†ç½‘ç»œåˆ†åŒºæƒ…å†µ
- éœ€è¦æ¢å¤è·¯ç”±è¡¨å®Œæ•´æ€§

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ç½‘ç»œè¯Šæ–­åŠŸèƒ½æ£€æµ‹æ•…éšœï¼Œç»“åˆåˆ†åŒºç®¡ç†å’Œè·¯ç”±é‡å»ºåŠŸèƒ½ï¼Œ
å®ç°è‡ªåŠ¨åŒ–çš„æ•…éšœæ£€æµ‹å’Œæ¢å¤æµç¨‹ã€‚

### 9.2 Schemaå®šä¹‰

**æ•…éšœæ¢å¤Schema**ï¼š

```json
{
  "network_name": "SmartHomeNet",
  "fault_detection": {
    "check_interval_seconds": 30,
    "fault_threshold": {
      "no_response_count": 3,
      "low_link_quality": 1,
      "low_rssi": -90
    }
  },
  "recovery_strategy": {
    "auto_rejoin": true,
    "rebuild_routes": true,
    "merge_partitions": true,
    "notify_administrator": true
  },
  "fault_types": [
    "node_unreachable",
    "low_link_quality",
    "routing_failure",
    "partition_detected"
  ]
}
```

### 9.3 å®ç°ä»£ç 

**å®Œæ•´çš„æ•…éšœæ¢å¤å®ç°**ï¼š

```python
import logging
from datetime import datetime, timedelta
from thread_network_manager import ThreadNetworkManager
from thread_routing_manager import ThreadRoutingManager
from thread_storage import ThreadStorage
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–ç»„ä»¶
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
network_manager = ThreadNetworkManager()
routing_manager = ThreadRoutingManager(network_manager)

class ThreadFaultRecovery:
    """Threadç½‘ç»œæ•…éšœæ¢å¤ç®¡ç†å™¨"""

    def __init__(self, network_name: str, check_interval: int = 30):
        self.network_name = network_name
        self.check_interval = check_interval
        self.fault_history = {}
        self.recovery_actions = []

    def detect_faults(self) -> List[Dict]:
        """æ£€æµ‹ç½‘ç»œæ•…éšœ"""
        faults = []

        try:
            # æ‰§è¡Œç½‘ç»œè¯Šæ–­
            diagnosis = network_manager.diagnose_network(self.network_name)

            # 1. æ£€æµ‹æ–­å¼€è¿æ¥çš„èŠ‚ç‚¹
            disconnected_nodes = diagnosis["connectivity"].get("disconnected_nodes", [])
            for node_id in disconnected_nodes:
                fault = {
                    "node_id": node_id,
                    "fault_type": "node_unreachable",
                    "severity": "high",
                    "detected_at": datetime.now().isoformat(),
                    "details": "Node is not responding"
                }
                faults.append(fault)

            # 2. æ£€æµ‹ä¸å¥åº·èŠ‚ç‚¹
            unhealthy_nodes = diagnosis["node_health"].get("unhealthy_nodes", [])
            for node_id in unhealthy_nodes:
                health_details = diagnosis["node_health"]["health_details"].get(node_id, {})
                issues = health_details.get("issues", [])

                fault = {
                    "node_id": node_id,
                    "fault_type": "node_unhealthy",
                    "severity": "medium",
                    "detected_at": datetime.now().isoformat(),
                    "details": "; ".join(issues)
                }
                faults.append(fault)

            # 3. æ£€æµ‹è·¯ç”±è¡¨ä¸å®Œæ•´
            incomplete_tables = diagnosis["routing"].get("incomplete_tables", [])
            for node_id in incomplete_tables:
                fault = {
                    "node_id": node_id,
                    "fault_type": "routing_failure",
                    "severity": "medium",
                    "detected_at": datetime.now().isoformat(),
                    "details": "Routing table is incomplete"
                }
                faults.append(fault)

            # 4. æ£€æµ‹ç½‘ç»œåˆ†åŒº
            network = network_manager.networks.get(self.network_name)
            if network and "partitions" in network:
                partitions = network["partitions"]
                if len(partitions) > 1:
                    fault = {
                        "node_id": None,
                        "fault_type": "partition_detected",
                        "severity": "high",
                        "detected_at": datetime.now().isoformat(),
                        "details": f"Network partitioned into {len(partitions)} partitions",
                        "partition_count": len(partitions)
                    }
                    faults.append(fault)

            logger.info(f"Detected {len(faults)} faults in network {self.network_name}")
            return faults

        except Exception as e:
            logger.error(f"Fault detection failed: {e}")
            return []

    def recover_from_fault(self, fault: Dict) -> bool:
        """ä»æ•…éšœä¸­æ¢å¤"""
        fault_type = fault["fault_type"]
        node_id = fault.get("node_id")

        logger.info(f"Recovering from fault: {fault_type} (node: {node_id})")

        try:
            if fault_type == "node_unreachable":
                return self._recover_unreachable_node(node_id)
            elif fault_type == "node_unhealthy":
                return self._recover_unhealthy_node(node_id)
            elif fault_type == "routing_failure":
                return self._recover_routing_failure(node_id)
            elif fault_type == "partition_detected":
                return self._recover_partition()
            else:
                logger.warning(f"Unknown fault type: {fault_type}")
                return False
        except Exception as e:
            logger.error(f"Recovery failed for fault {fault_type}: {e}")
            return False

    def _recover_unreachable_node(self, node_id: str) -> bool:
        """æ¢å¤ä¸å¯è¾¾èŠ‚ç‚¹"""
        try:
            # å°è¯•é‡æ–°åŠ å…¥ç½‘ç»œ
            network = network_manager.networks.get(self.network_name)
            if not network:
                return False

            logger.info(f"Attempting to rejoin node {node_id}...")
            success = network_manager._rejoin_network(node_id, self.network_name)

            if success:
                # æ›´æ–°èŠ‚ç‚¹ä¿¡æ¯
                node_info = network_manager.get_node_info(node_id)
                if node_info:
                    storage.store_node(node_info)

                    # è®°å½•æ¢å¤äº‹ä»¶
                    storage.store_event(
                        self.network_name,
                        "node_recovered",
                        {
                            "node_id": node_id,
                            "recovery_type": "rejoin",
                            "mesh_local_address": node_info.get("mesh_local_address")
                        },
                        node_id
                    )

                    logger.info(f"Node {node_id} recovered successfully")
                    return True

            return False
        except Exception as e:
            logger.error(f"Failed to recover unreachable node {node_id}: {e}")
            return False

    def _recover_unhealthy_node(self, node_id: str) -> bool:
        """æ¢å¤ä¸å¥åº·èŠ‚ç‚¹"""
        try:
            # è·å–èŠ‚ç‚¹ä¿¡æ¯
            node_info = network_manager.get_node_info(node_id)
            if not node_info:
                return False

            # å¦‚æœèŠ‚ç‚¹æœ‰çˆ¶èŠ‚ç‚¹ï¼Œå°è¯•é‡æ–°é€‰æ‹©çˆ¶èŠ‚ç‚¹
            if node_info.get("node_type") == "EndDevice":
                # è®©èŠ‚ç‚¹é‡æ–°åŠ å…¥ä»¥é€‰æ‹©æ›´å¥½çš„çˆ¶èŠ‚ç‚¹
                return self._recover_unreachable_node(node_id)

            # å¯¹äºRouterèŠ‚ç‚¹ï¼Œæ›´æ–°è·¯ç”±è¡¨
            routing_manager.update_routing_table(node_id)
            routing_manager.optimize_routing_table(node_id)

            # è®°å½•æ¢å¤äº‹ä»¶
            storage.store_event(
                self.network_name,
                "node_health_improved",
                {"node_id": node_id},
                node_id
            )

            return True
        except Exception as e:
            logger.error(f"Failed to recover unhealthy node {node_id}: {e}")
            return False

    def _recover_routing_failure(self, node_id: str) -> bool:
        """æ¢å¤è·¯ç”±æ•…éšœ"""
        try:
            # æ›´æ–°è·¯ç”±è¡¨
            routing_manager.update_routing_table(node_id)

            # ä¼˜åŒ–è·¯ç”±è¡¨
            routing_manager.optimize_routing_table(node_id)

            # éªŒè¯è·¯ç”±è¡¨
            routes = routing_manager.routing_tables.get(node_id, [])
            if len(routes) > 0:
                # å­˜å‚¨è·¯ç”±è¡¨
                storage.store_routing_table(node_id, routes)

                # è®°å½•æ¢å¤äº‹ä»¶
                storage.store_event(
                    self.network_name,
                    "routing_recovered",
                    {
                        "node_id": node_id,
                        "route_count": len(routes)
                    },
                    node_id
                )

                logger.info(f"Routing recovered for node {node_id}: {len(routes)} routes")
                return True

            return False
        except Exception as e:
            logger.error(f"Failed to recover routing for node {node_id}: {e}")
            return False

    def _recover_partition(self) -> bool:
        """æ¢å¤ç½‘ç»œåˆ†åŒº"""
        try:
            logger.info("Attempting to merge network partitions...")

            # æ£€æµ‹åˆ†åŒº
            network_manager.manage_partition(self.network_name, "detect")

            # åˆå¹¶åˆ†åŒº
            success = network_manager.manage_partition(self.network_name, "merge")

            if success:
                # æ›´æ–°æ‰€æœ‰è·¯ç”±è¡¨
                routing_manager.update_all_routing_tables(self.network_name)

                # è®°å½•æ¢å¤äº‹ä»¶
                storage.store_event(
                    self.network_name,
                    "partition_merged",
                    {"recovery_type": "automatic"}
                )

                logger.info("Network partitions merged successfully")
                return True

            return False
        except Exception as e:
            logger.error(f"Failed to recover partition: {e}")
            return False

    def run_continuous_monitoring(self):
        """æŒç»­ç›‘æ§å’Œæ¢å¤"""
        logger.info(f"Starting continuous fault monitoring for {self.network_name}")

        while True:
            try:
                # æ£€æµ‹æ•…éšœ
                faults = self.detect_faults()

                # å¤„ç†æ¯ä¸ªæ•…éšœ
                for fault in faults:
                    # æ£€æŸ¥æ•…éšœå†å²ï¼Œé¿å…é‡å¤å¤„ç†
                    fault_key = f"{fault['fault_type']}_{fault.get('node_id', 'network')}"
                    last_handled = self.fault_history.get(fault_key)

                    if last_handled:
                        # å¦‚æœæœ€è¿‘å¤„ç†è¿‡ï¼Œè·³è¿‡
                        time_since_handled = datetime.now() - datetime.fromisoformat(last_handled)
                        if time_since_handled.total_seconds() < 300:  # 5åˆ†é’Ÿå†…ä¸é‡å¤å¤„ç†
                            continue

                    # æ‰§è¡Œæ¢å¤
                    recovery_success = self.recover_from_fault(fault)

                    # è®°å½•æ¢å¤æ“ä½œ
                    self.recovery_actions.append({
                        "fault": fault,
                        "recovery_success": recovery_success,
                        "timestamp": datetime.now().isoformat()
                    })

                    # æ›´æ–°æ•…éšœå†å²
                    if recovery_success:
                        self.fault_history[fault_key] = datetime.now().isoformat()

                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                time.sleep(self.check_interval)

            except KeyboardInterrupt:
                logger.info("Stopping fault monitoring...")
                break
            except Exception as e:
                logger.error(f"Error in continuous monitoring: {e}")
                time.sleep(self.check_interval)

# æµ‹è¯•ç”¨ä¾‹
def test_fault_recovery():
    """æµ‹è¯•æ•…éšœæ¢å¤"""
    recovery_manager = ThreadFaultRecovery("SmartHomeNet", check_interval=30)

    # å•æ¬¡æ•…éšœæ£€æµ‹å’Œæ¢å¤
    faults = recovery_manager.detect_faults()
    print(f"\nDetected {len(faults)} faults:")

    for fault in faults:
        print(f"  - {fault['fault_type']}: {fault.get('node_id', 'N/A')} ({fault['severity']})")
        recovery_success = recovery_manager.recover_from_fault(fault)
        print(f"    Recovery: {'Success' if recovery_success else 'Failed'}")

if __name__ == "__main__":
    test_fault_recovery()
    # å–æ¶ˆæ³¨é‡Šä»¥è¿è¡ŒæŒç»­ç›‘æ§
    # recovery_manager = ThreadFaultRecovery("SmartHomeNet")
    # recovery_manager.run_continuous_monitoring()
```

**è¿è¡Œç»“æœç¤ºä¾‹**ï¼š

```text
INFO:__main__:Detected 2 faults in network SmartHomeNet
  - node_unreachable: 000D6F0000123456 (high)
    Recovery: Success
INFO:__main__:Attempting to rejoin node 000D6F0000123456...
INFO:__main__:Node 000D6F0000123456 recovered successfully
  - routing_failure: 000D6F0000654321 (medium)
    Recovery: Success
INFO:__main__:Routing recovered for node 000D6F0000654321: 5 routes
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `04_Transformation.md` - è½¬æ¢ä½“ç³»

---

## 10. æ¡ˆä¾‹9ï¼šå¤§è§„æ¨¡Threadç½‘ç»œç®¡ç†

### 10.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
å¤§è§„æ¨¡Threadç½‘ç»œç®¡ç†ç³»ç»Ÿç®¡ç†æ•°ç™¾ç”šè‡³æ•°åƒä¸ªThreadèŠ‚ç‚¹ï¼Œ
å®ç°ç½‘ç»œæ‹“æ‰‘ç®¡ç†ã€èŠ‚ç‚¹ç›‘æ§ã€èµ„æºåˆ†é…ç­‰åŠŸèƒ½ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å¤§è§„æ¨¡ç½‘ç»œæ‹“æ‰‘ç®¡ç†
- éœ€è¦é«˜æ•ˆçš„èŠ‚ç‚¹ç›‘æ§
- éœ€è¦èµ„æºåˆ†é…ä¼˜åŒ–
- éœ€è¦ç½‘ç»œæ€§èƒ½ä¿è¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨Thread_Schemaå®šä¹‰å¤§è§„æ¨¡ç½‘ç»œç®¡ç†ç»“æ„ï¼Œ
ä½¿ç”¨åˆ†å¸ƒå¼ç®¡ç†æ¶æ„è¿›è¡Œç½‘ç»œç®¡ç†ï¼Œ
ä½¿ç”¨ThreadStorageå­˜å‚¨ç½‘ç»œæ•°æ®ã€‚

### 10.2 Schemaå®šä¹‰

**å¤§è§„æ¨¡Threadç½‘ç»œç®¡ç†Schema**ï¼š

```dsl
schema LargeScaleThreadNetworkManagement {
  management_session_id: String @value("LARGE-NET-20250121-001") @required
  network_name: String @value("LargeScaleNetwork") @required
  management_time: DateTime @value("2025-01-21T10:00:00") @required

  network_scale: {
    total_nodes: Integer @value(1000)
    router_nodes: Integer @value(100)
    end_devices: Integer @value(900)
    network_depth: Integer @value(5)
    average_children_per_router: Decimal @value(9.0)
  } @required

  network_topology: {
    partitions: Integer @value(1)
    isolated_nodes: Integer @value(5)
    connectivity_rate: Decimal @value(0.995) @range(0.0, 1.0)
    average_hops: Decimal @value(3.5)
    max_hops: Integer @value(7)
  } @required

  management_strategies: {
    hierarchical_management: Boolean @value(true)
    distributed_monitoring: Boolean @value(true)
    load_balancing: Boolean @value(true)
    auto_scaling: Boolean @value(true)
  } @required

  performance_metrics: {
    average_latency: Decimal @value(50.0) @unit("ms")
    packet_loss_rate: Decimal @value(0.001) @range(0.0, 1.0)
    network_throughput: Decimal @value(1000.0) @unit("packets/s")
    resource_utilization: Decimal @value(0.75) @range(0.0, 1.0)
  } @required
} @standard("Thread")
```

### 10.3 å®ç°ä»£ç 

```python
from thread_storage import ThreadStorage
from thread_network_manager import ThreadNetworkManager
from datetime import datetime

def large_scale_thread_network_management():
    """å¤§è§„æ¨¡Threadç½‘ç»œç®¡ç†ç¤ºä¾‹"""
    storage = ThreadStorage("postgresql://user:password@localhost/thread_db")
    network_manager = ThreadNetworkManager()

    # ç½‘ç»œè§„æ¨¡
    network_scale = {
        "network_name": "LargeScaleNetwork",
        "total_nodes": 1000,
        "router_nodes": 100,
        "end_devices": 900,
        "network_depth": 5,
        "average_children_per_router": 9.0
    }

    # ç½‘ç»œæ‹“æ‰‘åˆ†æ
    def analyze_network_topology(network_name):
        """åˆ†æç½‘ç»œæ‹“æ‰‘"""
        nodes = network_manager.get_all_nodes(network_name)

        router_count = sum(1 for n in nodes if n.get("node_type") == "Router")
        end_device_count = sum(1 for n in nodes if n.get("node_type") == "EndDevice")

        # è®¡ç®—ç½‘ç»œæ·±åº¦
        max_depth = 0
        for node in nodes:
            depth = calculate_node_depth(node, nodes)
            max_depth = max(max_depth, depth)

        # è®¡ç®—è¿æ¥ç‡
        connected_nodes = sum(1 for n in nodes if n.get("connected", False))
        connectivity_rate = connected_nodes / len(nodes) if nodes else 0.0

        # è®¡ç®—å¹³å‡è·³æ•°
        total_hops = 0
        hop_count = 0
        for node in nodes:
            if node.get("connected"):
                hops = calculate_hops_to_border_router(node, nodes)
                total_hops += hops
                hop_count += 1
        average_hops = total_hops / hop_count if hop_count > 0 else 0.0

        return {
            "partitions": 1,  # ç®€åŒ–ï¼šå‡è®¾å•åˆ†åŒº
            "isolated_nodes": len(nodes) - connected_nodes,
            "connectivity_rate": connectivity_rate,
            "average_hops": average_hops,
            "max_hops": max_depth
        }

    def calculate_node_depth(node, nodes):
        """è®¡ç®—èŠ‚ç‚¹æ·±åº¦"""
        depth = 0
        current = node
        while current.get("parent_id"):
            depth += 1
            parent_id = current["parent_id"]
            current = next((n for n in nodes if n["node_id"] == parent_id), None)
            if not current:
                break
        return depth

    def calculate_hops_to_border_router(node, nodes):
        """è®¡ç®—åˆ°è¾¹ç•Œè·¯ç”±å™¨çš„è·³æ•°"""
        hops = 0
        current = node
        while current.get("parent_id"):
            hops += 1
            parent_id = current["parent_id"]
            current = next((n for n in nodes if n["node_id"] == parent_id), None)
            if not current or current.get("is_border_router"):
                break
        return hops

    # æ‰§è¡Œæ‹“æ‰‘åˆ†æ
    topology = analyze_network_topology(network_scale["network_name"])

    # ç®¡ç†ç­–ç•¥
    management_strategies = {
        "hierarchical_management": True,
        "distributed_monitoring": True,
        "load_balancing": True,
        "auto_scaling": True
    }

    # æ€§èƒ½æŒ‡æ ‡
    performance_metrics = {
        "average_latency": 50.0,
        "packet_loss_rate": 0.001,
        "network_throughput": 1000.0,
        "resource_utilization": 0.75
    }

    # å­˜å‚¨ç®¡ç†æ•°æ®
    management_data = {
        "management_session_id": "LARGE-NET-20250121-001",
        "network_name": network_scale["network_name"],
        "management_time": datetime.now(),
        "total_nodes": network_scale["total_nodes"],
        "router_nodes": network_scale["router_nodes"],
        "end_devices": network_scale["end_devices"],
        "network_depth": network_scale["network_depth"],
        "partitions": topology["partitions"],
        "isolated_nodes": topology["isolated_nodes"],
        "connectivity_rate": topology["connectivity_rate"],
        "average_hops": topology["average_hops"],
        "max_hops": topology["max_hops"],
        "hierarchical_management": management_strategies["hierarchical_management"],
        "distributed_monitoring": management_strategies["distributed_monitoring"],
        "load_balancing": management_strategies["load_balancing"],
        "auto_scaling": management_strategies["auto_scaling"],
        "average_latency": performance_metrics["average_latency"],
        "packet_loss_rate": performance_metrics["packet_loss_rate"],
        "network_throughput": performance_metrics["network_throughput"],
        "resource_utilization": performance_metrics["resource_utilization"]
    }

    # å­˜å‚¨åˆ°æ•°æ®åº“
    management_id = storage.store_network_data(management_data)
    print(f"Large-scale network management data stored: {management_id}")

    print(f"\nLarge-Scale Thread Network Management:")
    print(f"  Network: {network_scale['network_name']}")
    print(f"  Total nodes: {network_scale['total_nodes']}")
    print(f"  Router nodes: {network_scale['router_nodes']}")
    print(f"  End devices: {network_scale['end_devices']}")
    print(f"  Connectivity rate: {topology['connectivity_rate']*100:.2f}%")
    print(f"  Average hops: {topology['average_hops']:.2f}")
    print(f"  Average latency: {performance_metrics['average_latency']:.1f} ms")

    return management_data

if __name__ == "__main__":
    large_scale_thread_network_management()
```

---

## 11. æ¡ˆä¾‹10ï¼šThreadç½‘ç»œæ€§èƒ½ä¼˜åŒ–

### 11.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
Threadç½‘ç»œæ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿç›‘æµ‹å’Œåˆ†æç½‘ç»œæ€§èƒ½ï¼Œ
è¯†åˆ«æ€§èƒ½ç“¶é¢ˆï¼Œä¼˜åŒ–ç½‘ç»œé…ç½®ï¼Œæé«˜ç½‘ç»œæ•ˆç‡ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦æ€§èƒ½æ•°æ®æ”¶é›†
- éœ€è¦æ€§èƒ½åˆ†æç®—æ³•
- éœ€è¦ä¼˜åŒ–ç­–ç•¥
- éœ€è¦æ•ˆæœè¯„ä¼°

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨Thread_Schemaæ”¶é›†æ€§èƒ½æ•°æ®ï¼Œ
ä½¿ç”¨ä¼˜åŒ–ç®—æ³•è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ï¼Œ
ä½¿ç”¨ThreadStorageå­˜å‚¨æ€§èƒ½æ•°æ®ã€‚

### 11.2 Schemaå®šä¹‰

**Threadç½‘ç»œæ€§èƒ½ä¼˜åŒ–Schema**ï¼š

```dsl
schema ThreadNetworkPerformanceOptimization {
  optimization_session_id: String @value("PERF-OPT-20250121-001") @required
  network_name: String @value("OptimizedNetwork") @required
  optimization_time: DateTime @value("2025-01-21T10:00:00") @required

  performance_baseline: {
    average_latency: Decimal @value(80.0) @unit("ms")
    packet_loss_rate: Decimal @value(0.005) @range(0.0, 1.0)
    network_throughput: Decimal @value(800.0) @unit("packets/s")
    energy_consumption: Decimal @value(100.0) @unit("mW")
  } @required

  performance_bottlenecks: [
    {
      bottleneck_type: String @value("High latency")
      location: String @value("Router-001")
      severity: Enum { Medium } @value(Medium)
      impact: Decimal @value(0.15) @unit("15% performance degradation")
    }
  ] @required

  optimization_strategies: [
    {
      strategy: String @value("è·¯ç”±è¡¨ä¼˜åŒ–")
      expected_improvement: Decimal @value(0.20) @unit("20% latency reduction")
      implementation_complexity: Enum { Medium } @value(Medium)
    },
    {
      strategy: String @value("è´Ÿè½½å‡è¡¡")
      expected_improvement: Decimal @value(0.15) @unit("15% throughput increase")
      implementation_complexity: Enum { Low } @value(Low)
    }
  ] @required

  optimization_results: {
    latency_improvement: Decimal @value(0.18) @unit("18% reduction")
    throughput_improvement: Decimal @value(0.12) @unit("12% increase")
    energy_savings: Decimal @value(0.10) @unit("10% reduction")
    overall_improvement: Decimal @value(0.15) @unit("15% improvement")
  } @required
} @standard("Thread")
```

### 11.3 å®ç°ä»£ç 

```python
from thread_storage import ThreadStorage
from thread_network_manager import ThreadNetworkManager
from datetime import datetime

def thread_network_performance_optimization():
    """Threadç½‘ç»œæ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹"""
    storage = ThreadStorage("postgresql://user:password@localhost/thread_db")
    network_manager = ThreadNetworkManager()

    # æ€§èƒ½åŸºçº¿
    performance_baseline = {
        "network_name": "OptimizedNetwork",
        "average_latency": 80.0,
        "packet_loss_rate": 0.005,
        "network_throughput": 800.0,
        "energy_consumption": 100.0
    }

    # æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
    def identify_bottlenecks(network_name):
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        nodes = network_manager.get_all_nodes(network_name)

        for node in nodes:
            node_latency = node.get("average_latency", 0)
            node_load = node.get("load", 0)

            # è¯†åˆ«é«˜å»¶è¿ŸèŠ‚ç‚¹
            if node_latency > 100:
                bottlenecks.append({
                    "bottleneck_type": "High latency",
                    "location": node["node_id"],
                    "severity": "Medium",
                    "impact": 0.15
                })

            # è¯†åˆ«é«˜è´Ÿè½½èŠ‚ç‚¹
            if node_load > 0.8:
                bottlenecks.append({
                    "bottleneck_type": "High load",
                    "location": node["node_id"],
                    "severity": "High",
                    "impact": 0.25
                })

        return bottlenecks

    # è¯†åˆ«ç“¶é¢ˆ
    bottlenecks = identify_bottlenecks(performance_baseline["network_name"])

    # ä¼˜åŒ–ç­–ç•¥
    optimization_strategies = []

    if any(b["bottleneck_type"] == "High latency" for b in bottlenecks):
        optimization_strategies.append({
            "strategy": "è·¯ç”±è¡¨ä¼˜åŒ–",
            "expected_improvement": 0.20,
            "implementation_complexity": "Medium"
        })

    if any(b["bottleneck_type"] == "High load" for b in bottlenecks):
        optimization_strategies.append({
            "strategy": "è´Ÿè½½å‡è¡¡",
            "expected_improvement": 0.15,
            "implementation_complexity": "Low"
        })

    # æ‰§è¡Œä¼˜åŒ–ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
    optimization_results = {
        "latency_improvement": 0.18,
        "throughput_improvement": 0.12,
        "energy_savings": 0.10,
        "overall_improvement": 0.15
    }

    # å­˜å‚¨ä¼˜åŒ–æ•°æ®
    optimization_data = {
        "optimization_session_id": "PERF-OPT-20250121-001",
        "network_name": performance_baseline["network_name"],
        "optimization_time": datetime.now(),
        "baseline_latency": performance_baseline["average_latency"],
        "baseline_packet_loss": performance_baseline["packet_loss_rate"],
        "baseline_throughput": performance_baseline["network_throughput"],
        "baseline_energy": performance_baseline["energy_consumption"],
        "bottlenecks": bottlenecks,
        "optimization_strategies": optimization_strategies,
        "latency_improvement": optimization_results["latency_improvement"],
        "throughput_improvement": optimization_results["throughput_improvement"],
        "energy_savings": optimization_results["energy_savings"],
        "overall_improvement": optimization_results["overall_improvement"]
    }

    # å­˜å‚¨åˆ°æ•°æ®åº“
    optimization_id = storage.store_network_data(optimization_data)
    print(f"Performance optimization data stored: {optimization_id}")

    print(f"\nThread Network Performance Optimization:")
    print(f"  Network: {performance_baseline['network_name']}")
    print(f"  Baseline latency: {performance_baseline['average_latency']:.1f} ms")
    print(f"  Bottlenecks identified: {len(bottlenecks)}")
    print(f"  Optimization strategies: {len(optimization_strategies)}")
    print(f"  Latency improvement: {optimization_results['latency_improvement']*100:.1f}%")
    print(f"  Throughput improvement: {optimization_results['throughput_improvement']*100:.1f}%")
    print(f"  Overall improvement: {optimization_results['overall_improvement']*100:.1f}%")

    return optimization_data

if __name__ == "__main__":
    thread_network_performance_optimization()
```

---

## 12. æ¡ˆä¾‹11ï¼šThreadç½‘ç»œå®‰å…¨åŠ å›º

### 12.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
Threadç½‘ç»œå®‰å…¨åŠ å›ºç³»ç»Ÿå¢å¼ºç½‘ç»œå®‰å…¨æ€§ï¼Œ
å®ç°å®‰å…¨ç­–ç•¥ç®¡ç†ã€å¨èƒæ£€æµ‹ã€å®‰å…¨äº‹ä»¶å“åº”ç­‰åŠŸèƒ½ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®‰å…¨ç­–ç•¥ç®¡ç†
- éœ€è¦å¨èƒæ£€æµ‹
- éœ€è¦å®‰å…¨äº‹ä»¶å“åº”
- éœ€è¦å®‰å…¨å®¡è®¡

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨Thread_Schemaå®šä¹‰å®‰å…¨ç­–ç•¥ï¼Œ
ä½¿ç”¨å®‰å…¨ç®—æ³•è¿›è¡Œå¨èƒæ£€æµ‹ï¼Œ
ä½¿ç”¨ThreadStorageå­˜å‚¨å®‰å…¨æ•°æ®ã€‚

### 12.2 Schemaå®šä¹‰

**Threadç½‘ç»œå®‰å…¨åŠ å›ºSchema**ï¼š

```dsl
schema ThreadNetworkSecurityHardening {
  security_session_id: String @value("SEC-HARDEN-20250121-001") @required
  network_name: String @value("SecuredNetwork") @required
  security_time: DateTime @value("2025-01-21T10:00:00") @required

  security_policies: {
    authentication_enabled: Boolean @value(true)
    encryption_enabled: Boolean @value(true)
    key_rotation_interval: Integer @value(30) @unit("days")
    access_control_enabled: Boolean @value(true)
    intrusion_detection_enabled: Boolean @value(true)
  } @required

  security_threats: [
    {
      threat_type: String @value("Unauthorized access attempt")
      source_node: String @value("Node-001")
      severity: Enum { Medium } @value(Medium)
      detected_at: DateTime @value("2025-01-21T09:30:00")
      status: Enum { Blocked } @value(Blocked)
    }
  ] @required

  security_measures: [
    {
      measure: String @value("å¯†é’¥è½®æ¢")
      implementation_status: Enum { Implemented } @value(Implemented)
      effectiveness: Decimal @value(0.90) @range(0.0, 1.0)
    },
    {
      measure: String @value("è®¿é—®æ§åˆ¶åˆ—è¡¨")
      implementation_status: Enum { Implemented } @value(Implemented)
      effectiveness: Decimal @value(0.85) @range(0.0, 1.0)
    }
  ] @required

  security_metrics: {
    threat_detection_rate: Decimal @value(0.95) @range(0.0, 1.0)
    false_positive_rate: Decimal @value(0.05) @range(0.0, 1.0)
    security_incidents: Integer @value(3)
    security_score: Decimal @value(0.88) @range(0.0, 1.0)
  } @required
} @standard("Thread")
```

### 12.3 å®ç°ä»£ç 

```python
from thread_storage import ThreadStorage
from thread_network_manager import ThreadNetworkManager
from datetime import datetime

def thread_network_security_hardening():
    """Threadç½‘ç»œå®‰å…¨åŠ å›ºç¤ºä¾‹"""
    storage = ThreadStorage("postgresql://user:password@localhost/thread_db")
    network_manager = ThreadNetworkManager()

    # å®‰å…¨ç­–ç•¥
    security_policies = {
        "network_name": "SecuredNetwork",
        "authentication_enabled": True,
        "encryption_enabled": True,
        "key_rotation_interval": 30,
        "access_control_enabled": True,
        "intrusion_detection_enabled": True
    }

    # å¨èƒæ£€æµ‹
    def detect_security_threats(network_name):
        """æ£€æµ‹å®‰å…¨å¨èƒ"""
        threats = []
        nodes = network_manager.get_all_nodes(network_name)

        for node in nodes:
            # æ£€æµ‹æœªæˆæƒè®¿é—®å°è¯•
            unauthorized_attempts = node.get("unauthorized_attempts", 0)
            if unauthorized_attempts > 0:
                threats.append({
                    "threat_type": "Unauthorized access attempt",
                    "source_node": node["node_id"],
                    "severity": "Medium" if unauthorized_attempts < 5 else "High",
                    "detected_at": datetime.now(),
                    "status": "Blocked"
                })

            # æ£€æµ‹å¼‚å¸¸è¡Œä¸º
            if node.get("anomaly_score", 0) > 0.7:
                threats.append({
                    "threat_type": "Anomalous behavior",
                    "source_node": node["node_id"],
                    "severity": "High",
                    "detected_at": datetime.now(),
                    "status": "Under investigation"
                })

        return threats

    # æ£€æµ‹å¨èƒ
    threats = detect_security_threats(security_policies["network_name"])

    # å®‰å…¨æªæ–½
    security_measures = [
        {
            "measure": "å¯†é’¥è½®æ¢",
            "implementation_status": "Implemented",
            "effectiveness": 0.90
        },
        {
            "measure": "è®¿é—®æ§åˆ¶åˆ—è¡¨",
            "implementation_status": "Implemented",
            "effectiveness": 0.85
        },
        {
            "measure": "å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ",
            "implementation_status": "Implemented",
            "effectiveness": 0.88
        }
    ]

    # å®‰å…¨æŒ‡æ ‡
    security_metrics = {
        "threat_detection_rate": 0.95,
        "false_positive_rate": 0.05,
        "security_incidents": len(threats),
        "security_score": 0.88
    }

    # å­˜å‚¨å®‰å…¨æ•°æ®
    security_data = {
        "security_session_id": "SEC-HARDEN-20250121-001",
        "network_name": security_policies["network_name"],
        "security_time": datetime.now(),
        "authentication_enabled": security_policies["authentication_enabled"],
        "encryption_enabled": security_policies["encryption_enabled"],
        "key_rotation_interval": security_policies["key_rotation_interval"],
        "access_control_enabled": security_policies["access_control_enabled"],
        "intrusion_detection_enabled": security_policies["intrusion_detection_enabled"],
        "threats": threats,
        "security_measures": security_measures,
        "threat_detection_rate": security_metrics["threat_detection_rate"],
        "false_positive_rate": security_metrics["false_positive_rate"],
        "security_incidents": security_metrics["security_incidents"],
        "security_score": security_metrics["security_score"]
    }

    # å­˜å‚¨åˆ°æ•°æ®åº“
    security_id = storage.store_network_data(security_data)
    print(f"Security hardening data stored: {security_id}")

    print(f"\nThread Network Security Hardening:")
    print(f"  Network: {security_policies['network_name']}")
    print(f"  Authentication: {'Enabled' if security_policies['authentication_enabled'] else 'Disabled'}")
    print(f"  Encryption: {'Enabled' if security_policies['encryption_enabled'] else 'Disabled'}")
    print(f"  Threats detected: {len(threats)}")
    print(f"  Security measures: {len(security_measures)}")
    print(f"  Threat detection rate: {security_metrics['threat_detection_rate']*100:.1f}%")
    print(f"  Security score: {security_metrics['security_score']:.2f}")

    return security_data

if __name__ == "__main__":
    thread_network_security_hardening()
```

---

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
