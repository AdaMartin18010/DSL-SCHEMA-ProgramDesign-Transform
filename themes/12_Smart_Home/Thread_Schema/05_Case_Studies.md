# Thread Schemaå®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [Thread Schemaå®è·µæ¡ˆä¾‹](#thread-schemaå®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¡ˆä¾‹æ¦‚è¿°](#1-æ¡ˆä¾‹æ¦‚è¿°)
  - [2. æ¡ˆä¾‹1ï¼šThread Meshç½‘ç»œæ„å»º](#2-æ¡ˆä¾‹1thread-meshç½‘ç»œæ„å»º)
    - [2.1 åœºæ™¯æè¿°](#21-åœºæ™¯æè¿°)
    - [2.2 Schemaå®šä¹‰](#22-schemaå®šä¹‰)
    - [2.3 å®ç°ä»£ç ](#23-å®ç°ä»£ç )
  - [3. æ¡ˆä¾‹2ï¼šThreadè·¯ç”±ç®¡ç†å’Œä¼˜åŒ–](#3-æ¡ˆä¾‹2threadè·¯ç”±ç®¡ç†å’Œä¼˜åŒ–)
    - [3.1 åœºæ™¯æè¿°](#31-åœºæ™¯æè¿°)
    - [3.2 Schemaå®šä¹‰](#32-schemaå®šä¹‰)
    - [3.3 å®ç°ä»£ç ](#33-å®ç°ä»£ç )
  - [4. æ¡ˆä¾‹3ï¼šThreadç½‘ç»œå®‰å…¨å’Œå¯†é’¥ç®¡ç†](#4-æ¡ˆä¾‹3threadç½‘ç»œå®‰å…¨å’Œå¯†é’¥ç®¡ç†)
    - [4.1 åœºæ™¯æè¿°](#41-åœºæ™¯æè¿°)
    - [4.2 Schemaå®šä¹‰](#42-schemaå®šä¹‰)
    - [4.3 å®ç°ä»£ç ](#43-å®ç°ä»£ç )
  - [5. æ¡ˆä¾‹4ï¼šThreadç½‘ç»œæ€§èƒ½ç›‘æ§](#5-æ¡ˆä¾‹4threadç½‘ç»œæ€§èƒ½ç›‘æ§)
    - [5.1 åœºæ™¯æè¿°](#51-åœºæ™¯æè¿°)
    - [5.2 å®ç°ä»£ç ](#52-å®ç°ä»£ç )
  - [6. æ¡ˆä¾‹5ï¼šThreadåˆ°Zigbeeç½‘ç»œè½¬æ¢](#6-æ¡ˆä¾‹5threadåˆ°zigbeeç½‘ç»œè½¬æ¢)
    - [6.1 åœºæ™¯æè¿°](#61-åœºæ™¯æè¿°)
    - [6.2 å®ç°ä»£ç ](#62-å®ç°ä»£ç )
  - [7. æ¡ˆä¾‹6ï¼šThreadæ•°æ®å­˜å‚¨å’Œåˆ†æç³»ç»Ÿ](#7-æ¡ˆä¾‹6threadæ•°æ®å­˜å‚¨å’Œåˆ†æç³»ç»Ÿ)
    - [7.1 åœºæ™¯æè¿°](#71-åœºæ™¯æè¿°)
    - [7.2 å®ç°ä»£ç ](#72-å®ç°ä»£ç )
    - [7.3 æ•°æ®åˆ†æç¤ºä¾‹](#73-æ•°æ®åˆ†æç¤ºä¾‹)
  - [8. æ¡ˆä¾‹7ï¼šThreadç½‘ç»œæ‰©å±•ï¼ˆæ·»åŠ æ–°èŠ‚ç‚¹ï¼‰](#8-æ¡ˆä¾‹7threadç½‘ç»œæ‰©å±•æ·»åŠ æ–°èŠ‚ç‚¹)
    - [8.1 åœºæ™¯æè¿°](#81-åœºæ™¯æè¿°)
    - [8.2 Schemaå®šä¹‰](#82-schemaå®šä¹‰)
    - [8.3 å®ç°ä»£ç ](#83-å®ç°ä»£ç )
  - [9. æ¡ˆä¾‹8ï¼šThreadç½‘ç»œæ•…éšœæ¢å¤](#9-æ¡ˆä¾‹8threadç½‘ç»œæ•…éšœæ¢å¤)
    - [9.1 åœºæ™¯æè¿°](#91-åœºæ™¯æè¿°)
    - [9.2 Schemaå®šä¹‰](#92-schemaå®šä¹‰)
    - [9.3 å®ç°ä»£ç ](#93-å®ç°ä»£ç )
  - [10. æ¡ˆä¾‹9ï¼šå¤§è§„æ¨¡Threadç½‘ç»œç®¡ç†](#10-æ¡ˆä¾‹9å¤§è§„æ¨¡threadç½‘ç»œç®¡ç†)
    - [10.1 åœºæ™¯æè¿°](#101-åœºæ™¯æè¿°)
    - [10.2 Schemaå®šä¹‰](#102-schemaå®šä¹‰)
    - [10.3 å®ç°ä»£ç ](#103-å®ç°ä»£ç )
  - [11. æ¡ˆä¾‹10ï¼šThreadç½‘ç»œæ€§èƒ½ä¼˜åŒ–](#11-æ¡ˆä¾‹10threadç½‘ç»œæ€§èƒ½ä¼˜åŒ–)
    - [11.1 åœºæ™¯æè¿°](#111-åœºæ™¯æè¿°)
    - [11.2 Schemaå®šä¹‰](#112-schemaå®šä¹‰)
    - [11.3 å®ç°ä»£ç ](#113-å®ç°ä»£ç )
  - [12. æ¡ˆä¾‹11ï¼šThreadç½‘ç»œå®‰å…¨åŠ å›º](#12-æ¡ˆä¾‹11threadç½‘ç»œå®‰å…¨åŠ å›º)
    - [12.1 åœºæ™¯æè¿°](#121-åœºæ™¯æè¿°)
    - [12.2 Schemaå®šä¹‰](#122-schemaå®šä¹‰)
    - [12.3 å®ç°ä»£ç ](#123-å®ç°ä»£ç )

---

## 1. æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›Thread Schemaåœ¨å®é™…åº”ç”¨ä¸­çš„å®è·µæ¡ˆä¾‹ã€‚

---

## 2. æ¡ˆä¾‹1ï¼šThread Meshç½‘ç»œæ„å»º

### 2.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
ç”¨æˆ·éœ€è¦æ„å»ºä¸€ä¸ªThread Meshç½‘ç»œï¼Œè¿æ¥å¤šä¸ªæ™ºèƒ½å®¶å±…è®¾å¤‡ï¼Œ
åŒ…æ‹¬æ™ºèƒ½ç¯ã€ä¼ æ„Ÿå™¨ã€é—¨é”ç­‰ï¼Œå®ç°è®¾å¤‡é—´çš„Meshé€šä¿¡ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦åˆ›å»ºThreadç½‘ç»œå¹¶é…ç½®ç½‘ç»œå‚æ•°
- éœ€è¦å°†å¤šä¸ªè®¾å¤‡åŠ å…¥ç½‘ç»œ
- éœ€è¦ç®¡ç†ç½‘ç»œæ‹“æ‰‘
- éœ€è¦ç›‘æ§ç½‘ç»œçŠ¶æ€

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadNetworkManageråˆ›å»ºå’Œç®¡ç†Threadç½‘ç»œï¼Œå®ç°è®¾å¤‡çš„
è‡ªåŠ¨å‘ç°å’ŒåŠ å…¥ç½‘ç»œåŠŸèƒ½ã€‚

### 2.2 Schemaå®šä¹‰

è¯¦è§ç¬¬2.2èŠ‚åŸå§‹å®šä¹‰ã€‚

### 2.3 å®ç°ä»£ç 

**å®Œæ•´çš„Thread Meshç½‘ç»œæ„å»ºå®ç°**ï¼š

```python
import asyncio
from thread_network_manager import ThreadNetworkManager
from thread_storage import ThreadStorage

# åˆå§‹åŒ–å­˜å‚¨å’Œç®¡ç†å™¨
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
network_manager = ThreadNetworkManager()

async def build_thread_mesh_network():
    """æ„å»ºThread Meshç½‘ç»œ"""
    try:
        # åˆ›å»ºThreadç½‘ç»œ
        network_name = "SmartHomeNet"
        pan_id = 0x1234
        channel = 15
        network_key = "00112233445566778899AABBCCDDEEFF"

        print(f"Creating Thread network: {network_name}")
        success = network_manager.create_network(
            network_name, pan_id, channel, network_key
        )

        if not success:
            print("Failed to create network")
            return

        # å­˜å‚¨ç½‘ç»œä¿¡æ¯
        network_data = {
            "network_name": network_name,
            "pan_id": pan_id,
            "extended_pan_id": "DEADBEEF00CAFE00",
            "channel": channel,
            "network_key": network_key,
            "partition_id": 1
        }
        storage.store_network(network_data)

        # èŠ‚ç‚¹åˆ—è¡¨
        nodes = [
            {"node_id": "000D6F0000123456", "type": "Router"},
            {"node_id": "000D6F0000654321", "type": "EndDevice"},
            {"node_id": "000D6F0000789012", "type": "EndDevice"}
        ]

        # å°†èŠ‚ç‚¹åŠ å…¥ç½‘ç»œ
        for node in nodes:
            print(f"Joining node {node['node_id']} to network...")
            success = network_manager.join_network(
                node["node_id"],
                network_name,
                network_key,
                pan_id,
                channel
            )

            if success:
                # è·å–èŠ‚ç‚¹ä¿¡æ¯
                node_info = network_manager.get_node_info(node["node_id"])
                if node_info:
                    # å­˜å‚¨èŠ‚ç‚¹ä¿¡æ¯
                    storage.store_node(node_info)
                    print(f"Node {node['node_id']} joined successfully")
                    print(f"  Type: {node_info['node_type']}")
                    print(f"  Mesh Local: {node_info['mesh_local_address']}")
            else:
                print(f"Failed to join node {node['node_id']}")

        # è·å–ç½‘ç»œæ‹“æ‰‘
        topology = network_manager.get_network_topology(network_name)
        print(f"\nNetwork Topology:")
        print(f"  Total nodes: {len(topology['nodes'])}")
        print(f"  Routers: {len(topology['routers'])}")
        print(f"  End Devices: {len(topology['end_devices'])}")

        # æŸ¥è¯¢ç½‘ç»œå¥åº·çŠ¶æ€
        health = storage.get_network_health_status(network_name)
        print(f"\nNetwork Health:")
        print(f"  Health Score: {health['health_score']:.1f}/100")
        print(f"  Avg Link Quality: {health['avg_link_quality']:.1f}")
        print(f"  Low Battery Nodes: {health['low_battery_count']}")

    except Exception as e:
        print(f"Error building network: {e}")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(build_thread_mesh_network())
```

---

## 3. æ¡ˆä¾‹2ï¼šThreadè·¯ç”±ç®¡ç†å’Œä¼˜åŒ–

### 3.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
ç³»ç»Ÿéœ€è¦ç®¡ç†Threadç½‘ç»œçš„è·¯ç”±è¡¨ï¼Œç›‘æ§è·¯ç”±æ€§èƒ½ï¼Œ
å¹¶åœ¨è·¯ç”±å¤±æ•ˆæ—¶è‡ªåŠ¨æ›´æ–°è·¯ç”±è¡¨ï¼Œç¡®ä¿ç½‘ç»œè¿é€šæ€§ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®šæœŸæ›´æ–°è·¯ç”±è¡¨
- éœ€è¦ç›‘æ§è·¯ç”±æ€§èƒ½
- éœ€è¦æ£€æµ‹è·¯ç”±å¤±æ•ˆ
- éœ€è¦ä¼˜åŒ–è·¯ç”±è·¯å¾„

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadRoutingManagerç®¡ç†è·¯ç”±è¡¨ï¼Œå®šæœŸæ›´æ–°è·¯ç”±ä¿¡æ¯
å¹¶ç›‘æ§è·¯ç”±æ€§èƒ½ã€‚

### 3.2 Schemaå®šä¹‰

è¯¦è§ç¬¬3.2èŠ‚åŸå§‹å®šä¹‰ã€‚

### 3.3 å®ç°ä»£ç 

**å®Œæ•´çš„è·¯ç”±ç®¡ç†å®ç°**ï¼š

```python
import asyncio
from thread_network_manager import ThreadNetworkManager
from thread_routing_manager import ThreadRoutingManager
from thread_storage import ThreadStorage
import time

# åˆå§‹åŒ–ç»„ä»¶
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
network_manager = ThreadNetworkManager()
routing_manager = ThreadRoutingManager(network_manager)

async def manage_thread_routing():
    """ç®¡ç†Threadè·¯ç”±"""
    try:
        network_name = "SmartHomeNet"

        # è·å–ç½‘ç»œä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
        topology = network_manager.get_network_topology(network_name)
        nodes = topology["nodes"]

        print(f"Managing routing for {len(nodes)} nodes")

        # æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹çš„è·¯ç”±è¡¨
        for node in nodes:
            node_id = node["node_id"]
            print(f"Updating routing table for node {node_id}...")

            # æ›´æ–°è·¯ç”±è¡¨
            routing_manager.update_routing_table(node_id)

            # è·å–è·¯ç”±è¡¨
            routes = routing_manager.get_node_routes(node_id)
            print(f"  Found {len(routes)} routes")

            # å­˜å‚¨è·¯ç”±è¡¨åˆ°æ•°æ®åº“
            storage.store_routing_table(node_id, routes)

            # æ˜¾ç¤ºè·¯ç”±ç»Ÿè®¡
            route_stats = storage.get_routing_statistics(node_id)
            print(f"  Route count: {route_stats['route_count']}")
            print(f"  Avg cost: {route_stats['avg_cost']:.2f}")
            print(f"  Max hops: {route_stats['max_cost']}")

        # è·å–ç½‘ç»œè·¯ç”±ç»Ÿè®¡
        network_stats = routing_manager.get_network_routing_statistics(network_name)
        print(f"\nNetwork Routing Statistics:")
        print(f"  Total nodes: {network_stats['total_nodes']}")
        print(f"  Total routes: {network_stats['total_routes']}")
        print(f"  Avg routes per node: {network_stats['avg_routes_per_node']:.2f}")
        print(f"  Avg cost: {network_stats['avg_cost']:.2f}")
        print(f"  Max hops: {network_stats['max_hops']}")

        # æµ‹è¯•è·¯ç”±æŸ¥æ‰¾
        if len(nodes) >= 2:
            source_node = nodes[0]["node_id"]
            dest_address = nodes[1].get("mesh_local_address")

            if dest_address:
                route = routing_manager.find_route(source_node, dest_address)
                if route:
                    print(f"\nRoute from {source_node} to {dest_address}:")
                    print(f"  Next hop: {route['next_hop']}")
                    print(f"  Cost: {route['cost']}")
                else:
                    print(f"\nNo route found from {source_node} to {dest_address}")

    except Exception as e:
        print(f"Error managing routing: {e}")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(manage_thread_routing())
```

---

## 4. æ¡ˆä¾‹3ï¼šThreadç½‘ç»œå®‰å…¨å’Œå¯†é’¥ç®¡ç†

### 4.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
ç³»ç»Ÿéœ€è¦ç®¡ç†Threadç½‘ç»œçš„å®‰å…¨å¯†é’¥ï¼Œå®šæœŸè½®æ¢ç½‘ç»œå¯†é’¥ï¼Œ
ç¡®ä¿ç½‘ç»œå®‰å…¨ï¼Œå¹¶ç›‘æ§å®‰å…¨äº‹ä»¶ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®šæœŸè½®æ¢ç½‘ç»œå¯†é’¥
- éœ€è¦åŒæ­¥å¯†é’¥åˆ°æ‰€æœ‰èŠ‚ç‚¹
- éœ€è¦ç›‘æ§å®‰å…¨äº‹ä»¶
- éœ€è¦å¤„ç†å¯†é’¥è½®æ¢å¤±è´¥çš„æƒ…å†µ

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadSecurityManagerç®¡ç†ç½‘ç»œå¯†é’¥ï¼Œå®ç°å¯†é’¥è½®æ¢
å’Œå®‰å…¨ç›‘æ§åŠŸèƒ½ã€‚

### 4.2 Schemaå®šä¹‰

è¯¦è§ç¬¬4.2èŠ‚åŸå§‹å®šä¹‰ã€‚

### 4.3 å®ç°ä»£ç 

**å®Œæ•´çš„å®‰å…¨ç®¡ç†å®ç°**ï¼š

```python
import asyncio
from thread_security_manager import ThreadSecurityManager
from thread_storage import ThreadStorage
from datetime import datetime, timedelta

# åˆå§‹åŒ–ç»„ä»¶
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
security_manager = ThreadSecurityManager(storage)

async def manage_thread_security():
    """ç®¡ç†Threadç½‘ç»œå®‰å…¨"""
    try:
        network_name = "SmartHomeNet"

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢å¯†é’¥
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“æŸ¥è¯¢ä¸Šæ¬¡è½®æ¢æ—¶é—´
        last_rotation_time = datetime.now() - timedelta(days=2)
        rotation_interval = timedelta(days=1)

        if datetime.now() - last_rotation_time > rotation_interval:
            print("Rotating network key...")
            success = security_manager.rotate_network_key(network_name)

            if success:
                print("Network key rotated successfully")
                # è¿™é‡Œéœ€è¦é€šçŸ¥æ‰€æœ‰èŠ‚ç‚¹æ›´æ–°å¯†é’¥
                # å®é™…å®ç°éœ€è¦è°ƒç”¨OpenThread APIæ›´æ–°æ‰€æœ‰èŠ‚ç‚¹
            else:
                print("Failed to rotate network key")
        else:
            print("Network key is still valid, no rotation needed")

        # æŸ¥è¯¢å®‰å…¨ç»Ÿè®¡
        # è¿™é‡Œå¯ä»¥æŸ¥è¯¢å®‰å…¨äº‹ä»¶ã€å¯†é’¥ä½¿ç”¨æƒ…å†µç­‰

    except Exception as e:
        print(f"Error managing security: {e}")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(manage_thread_security())
```

---

## 5. æ¡ˆä¾‹4ï¼šThreadç½‘ç»œæ€§èƒ½ç›‘æ§

### 5.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
ç³»ç»Ÿéœ€è¦ç›‘æ§Threadç½‘ç»œçš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å»¶è¿Ÿã€ä¸¢åŒ…ç‡ã€
ååé‡ç­‰ï¼ŒåŠæ—¶å‘ç°ç½‘ç»œé—®é¢˜å¹¶ä¼˜åŒ–ç½‘ç»œæ€§èƒ½ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®æ—¶æ”¶é›†æ€§èƒ½æ•°æ®
- éœ€è¦åˆ†ææ€§èƒ½è¶‹åŠ¿
- éœ€è¦è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
- éœ€è¦ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadStorageå­˜å‚¨æ€§èƒ½æ•°æ®ï¼Œå®šæœŸæ”¶é›†å’Œåˆ†æç½‘ç»œæ€§èƒ½ã€‚

### 5.2 å®ç°ä»£ç 

**å®Œæ•´çš„æ€§èƒ½ç›‘æ§å®ç°**ï¼š

```python
import asyncio
from thread_network_manager import ThreadNetworkManager
from thread_storage import ThreadStorage
import time

# åˆå§‹åŒ–ç»„ä»¶
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
network_manager = ThreadNetworkManager()

async def monitor_thread_performance():
    """ç›‘æ§Threadç½‘ç»œæ€§èƒ½"""
    try:
        network_name = "SmartHomeNet"

        # è·å–ç½‘ç»œä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
        topology = network_manager.get_network_topology(network_name)
        nodes = topology["nodes"]

        print(f"Monitoring performance for {len(nodes)} nodes")

        # æ”¶é›†æ€§èƒ½æ•°æ®
        for node in nodes:
            node_id = node["node_id"]

            # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®æ”¶é›†
            # å®é™…å®ç°éœ€è¦ä»OpenThreadè·å–çœŸå®æ•°æ®
            performance_data = {
                "latency_ms": 50 + (hash(node_id) % 50),  # æ¨¡æ‹Ÿå»¶è¿Ÿ
                "packet_loss_rate": (hash(node_id) % 5) / 100.0,  # æ¨¡æ‹Ÿä¸¢åŒ…ç‡
                "throughput_kbps": 100 + (hash(node_id) % 200)  # æ¨¡æ‹Ÿååé‡
            }

            # å­˜å‚¨æ€§èƒ½æ•°æ®
            storage.store_performance_data(node_id, performance_data)

            print(f"Node {node_id}:")
            print(f"  Latency: {performance_data['latency_ms']}ms")
            print(f"  Packet Loss: {performance_data['packet_loss_rate']*100:.2f}%")
            print(f"  Throughput: {performance_data['throughput_kbps']}kbps")

        # æŸ¥è¯¢ç½‘ç»œæ€§èƒ½ç»Ÿè®¡
        perf_stats = storage.get_network_performance_statistics(network_name, hours=24)
        print(f"\nNetwork Performance Statistics (24h):")
        print(f"  Avg Latency: {perf_stats['avg_latency']:.2f}ms")
        print(f"  Avg Packet Loss: {perf_stats['avg_packet_loss']*100:.2f}%")
        print(f"  Avg Throughput: {perf_stats['avg_throughput']:.2f}kbps")
        print(f"  Monitored Nodes: {perf_stats['monitored_nodes']}")

        # æŸ¥è¯¢èŠ‚ç‚¹æ€§èƒ½å†å²
        if nodes:
            node_id = nodes[0]["node_id"]
            history = storage.get_node_performance_history(node_id, hours=1)
            print(f"\nPerformance History for {node_id} (1h):")
            print(f"  Data points: {len(history)}")
            if history:
                print(f"  Latest latency: {history[0]['latency_ms']}ms")

    except Exception as e:
        print(f"Error monitoring performance: {e}")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(monitor_thread_performance())
```

---

## 6. æ¡ˆä¾‹5ï¼šThreadåˆ°Zigbeeç½‘ç»œè½¬æ¢

### 6.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
éœ€è¦å°†Threadç½‘ç»œè½¬æ¢ä¸ºZigbeeç½‘ç»œï¼Œä»¥ä¾¿ä¸ç°æœ‰çš„Zigbeeç³»ç»Ÿé›†æˆï¼Œ
å®ç°è·¨åè®®è®¾å¤‡é€šä¿¡ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦è½¬æ¢èŠ‚ç‚¹ç±»å‹
- éœ€è¦è½¬æ¢ç½‘ç»œåœ°å€
- éœ€è¦ä¿æŒç½‘ç»œæ‹“æ‰‘ä¸€è‡´æ€§
- éœ€è¦å¤„ç†åè®®å·®å¼‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadToZigbeeConverterå®ç°Threadåˆ°Zigbeeçš„è½¬æ¢ã€‚

### 6.2 å®ç°ä»£ç 

è¯¦è§ `04_Transformation.md` ç¬¬2.2ç« ã€‚

---

## 7. æ¡ˆä¾‹6ï¼šThreadæ•°æ®å­˜å‚¨å’Œåˆ†æç³»ç»Ÿ

### 7.1 åœºæ™¯æè¿°

**åº”ç”¨åœºæ™¯**ï¼š
ä½¿ç”¨PostgreSQLå­˜å‚¨Threadç½‘ç»œæ•°æ®ï¼Œæ”¯æŒç½‘ç»œæ‹“æ‰‘åˆ†æã€
æ€§èƒ½ç›‘æ§å’Œå¥åº·çŠ¶æ€è¯„ä¼°ã€‚

### 7.2 å®ç°ä»£ç 

è¯¦è§ `04_Transformation.md` ç¬¬6ç« ã€‚

### 7.3 æ•°æ®åˆ†æç¤ºä¾‹

**ç½‘ç»œå¥åº·çŠ¶æ€æŸ¥è¯¢**ï¼š

```python
from thread_storage import ThreadStorage

storage = ThreadStorage("postgresql://user:pass@localhost/thread")

# æŸ¥è¯¢ç½‘ç»œå¥åº·çŠ¶æ€
health = storage.get_network_health_status("SmartHomeNet")
print("Network Health Status:")
print(f"  Total Nodes: {health['total_nodes']}")
print(f"  Routers: {health['router_count']}")
print(f"  End Devices: {health['end_device_count']}")
print(f"  Avg Link Quality: {health['avg_link_quality']:.1f}")
print(f"  Avg RSSI: {health['avg_rssi']:.1f}dBm")
print(f"  Low Battery Nodes: {health['low_battery_count']}")
print(f"  Health Score: {health['health_score']:.1f}/100")

# æŸ¥è¯¢ç½‘ç»œæ‹“æ‰‘ç»Ÿè®¡
topology_stats = storage.get_network_topology_statistics("SmartHomeNet")
print("\nTopology Statistics:")
for stat in topology_stats:
    print(f"  {stat['node_type']}: {stat['count']} nodes")
    print(f"    Avg Link Quality: {stat['avg_link_quality']:.1f}")
    print(f"    Avg RSSI: {stat['avg_rssi']:.1f}dBm")
```

---

## 8. æ¡ˆä¾‹7ï¼šThreadç½‘ç»œæ‰©å±•ï¼ˆæ·»åŠ æ–°èŠ‚ç‚¹ï¼‰

### 8.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
ç”¨æˆ·éœ€è¦åœ¨ç°æœ‰çš„Threadç½‘ç»œä¸­åŠ¨æ€æ·»åŠ æ–°çš„æ™ºèƒ½è®¾å¤‡ï¼ŒåŒ…æ‹¬æ–°çš„Routerå’ŒEnd Deviceï¼Œ
ç¡®ä¿æ–°èŠ‚ç‚¹èƒ½å¤Ÿé¡ºåˆ©åŠ å…¥ç½‘ç»œå¹¶å»ºç«‹æ­£ç¡®çš„è·¯ç”±å…³ç³»ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦æ£€æµ‹æ–°èŠ‚ç‚¹å¹¶å¼•å¯¼å…¶åŠ å…¥ç½‘ç»œ
- éœ€è¦ä¸ºæ–°èŠ‚ç‚¹åˆ†é…ç½‘ç»œåœ°å€å’Œè·¯ç”±ä¿¡æ¯
- éœ€è¦æ›´æ–°ç°æœ‰èŠ‚ç‚¹çš„è·¯ç”±è¡¨
- éœ€è¦å¤„ç†èŠ‚ç‚¹åŠ å…¥å¤±è´¥çš„æƒ…å†µ
- éœ€è¦ä¼˜åŒ–ç½‘ç»œæ‹“æ‰‘ä»¥é€‚åº”æ–°èŠ‚ç‚¹

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ThreadNetworkManagerçš„èŠ‚ç‚¹åŠ å…¥åŠŸèƒ½ï¼Œç»“åˆè·¯ç”±ç®¡ç†å™¨çš„è‡ªåŠ¨è·¯ç”±æ›´æ–°ï¼Œ
å®ç°æ–°èŠ‚ç‚¹çš„æ— ç¼åŠ å…¥å’Œç½‘ç»œæ‰©å±•ã€‚

### 8.2 Schemaå®šä¹‰

**ç½‘ç»œæ‰©å±•Schema**ï¼š

```json
{
  "network_name": "SmartHomeNet",
  "new_nodes": [
    {
      "node_id": "000D6F0000ABCDEF",
      "node_type": "Router",
      "expected_role": "Router",
      "join_credentials": {
        "network_key": "00112233445566778899AABBCCDDEEFF",
        "pan_id": 4660,
        "channel": 15
      }
    },
    {
      "node_id": "000D6F0000FEDCBA",
      "node_type": "EndDevice",
      "expected_role": "EndDevice",
      "join_credentials": {
        "network_key": "00112233445566778899AABBCCDDEEFF",
        "pan_id": 4660,
        "channel": 15
      }
    }
  ],
  "expansion_strategy": {
    "update_routing_tables": true,
    "optimize_topology": true,
    "verify_connectivity": true
  }
}
```

### 8.3 å®ç°ä»£ç 

**å®Œæ•´çš„ç½‘ç»œæ‰©å±•å®ç°**ï¼š

```python
import logging
from datetime import datetime
from thread_network_manager import ThreadNetworkManager
from thread_routing_manager import ThreadRoutingManager
from thread_storage import ThreadStorage
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–ç»„ä»¶
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
network_manager = ThreadNetworkManager()
routing_manager = ThreadRoutingManager(network_manager)

def expand_thread_network(network_name: str, new_nodes: List[Dict]) -> Dict:
    """æ‰©å±•Threadç½‘ç»œ - æ·»åŠ æ–°èŠ‚ç‚¹"""
    results = {
        "network_name": network_name,
        "timestamp": datetime.now().isoformat(),
        "successful_joins": [],
        "failed_joins": [],
        "routing_updates": [],
        "topology_changes": {}
    }

    try:
        # 1. è·å–å½“å‰ç½‘ç»œçŠ¶æ€
        current_topology = network_manager.get_network_topology(network_name)
        initial_node_count = len(current_topology["nodes"])
        logger.info(f"Current network has {initial_node_count} nodes")

        # 2. æ·»åŠ æ–°èŠ‚ç‚¹
        for new_node in new_nodes:
            node_id = new_node["node_id"]
            node_type = new_node.get("node_type", "EndDevice")
            credentials = new_node.get("join_credentials", {})

            logger.info(f"Adding new node {node_id} (type: {node_type})...")

            try:
                # åŠ å…¥ç½‘ç»œ
                success = network_manager.join_network(
                    node_id,
                    network_name,
                    credentials.get("network_key"),
                    credentials.get("pan_id"),
                    credentials.get("channel")
                )

                if success:
                    # è·å–èŠ‚ç‚¹ä¿¡æ¯
                    node_info = network_manager.get_node_info(node_id)
                    if node_info:
                        # å­˜å‚¨èŠ‚ç‚¹ä¿¡æ¯
                        storage.store_node(node_info)

                        # è®°å½•äº‹ä»¶
                        storage.store_event(
                            network_name,
                            "node_joined",
                            {
                                "node_id": node_id,
                                "node_type": node_type,
                                "mesh_local_address": node_info.get("mesh_local_address")
                            },
                            node_id
                        )

                        results["successful_joins"].append({
                            "node_id": node_id,
                            "node_type": node_type,
                            "mesh_local_address": node_info.get("mesh_local_address"),
                            "parent_node_id": node_info.get("parent_node_id"),
                            "router_id": node_info.get("router_id")
                        })

                        logger.info(f"Node {node_id} joined successfully")

                        # ç­‰å¾…èŠ‚ç‚¹ç¨³å®š
                        time.sleep(2)
                    else:
                        raise Exception("Failed to get node info after join")
                else:
                    raise Exception("Failed to join network")

            except Exception as e:
                logger.error(f"Failed to add node {node_id}: {e}")
                results["failed_joins"].append({
                    "node_id": node_id,
                    "error": str(e)
                })

        # 3. æ›´æ–°è·¯ç”±è¡¨
        logger.info("Updating routing tables...")
        updated_count = routing_manager.update_all_routing_tables(network_name)
        results["routing_updates"] = {
            "updated_nodes": updated_count,
            "timestamp": datetime.now().isoformat()
        }

        # 4. ä¼˜åŒ–è·¯ç”±è¡¨
        logger.info("Optimizing routing tables...")
        topology = network_manager.get_network_topology(network_name)
        optimized_count = 0
        for node in topology["nodes"]:
            node_id = node["node_id"]
            if routing_manager.optimize_routing_table(node_id):
                optimized_count += 1

        results["routing_updates"]["optimized_nodes"] = optimized_count

        # 5. éªŒè¯è¿é€šæ€§
        logger.info("Verifying network connectivity...")
        final_topology = network_manager.get_network_topology(network_name)
        final_node_count = len(final_topology["nodes"])

        results["topology_changes"] = {
            "initial_node_count": initial_node_count,
            "final_node_count": final_node_count,
            "added_nodes": final_node_count - initial_node_count,
            "router_count": len(final_topology["routers"]),
            "end_device_count": len(final_topology["end_devices"])
        }

        # 6. ç½‘ç»œè¯Šæ–­
        logger.info("Running network diagnosis...")
        diagnosis = network_manager.diagnose_network(network_name)
        storage.store_diagnosis(network_name, diagnosis)

        results["diagnosis"] = {
            "issues_count": len(diagnosis.get("issues", [])),
            "connectivity_score": len(diagnosis["connectivity"]["connected_nodes"]) / final_node_count if final_node_count > 0 else 0,
            "routing_efficiency": diagnosis["routing"].get("complete_tables", [])
        }

        logger.info(f"Network expansion completed: {len(results['successful_joins'])} nodes added")
        return results

    except Exception as e:
        logger.error(f"Network expansion failed: {e}")
        results["error"] = str(e)
        return results

# æµ‹è¯•ç”¨ä¾‹
def test_network_expansion():
    """æµ‹è¯•ç½‘ç»œæ‰©å±•"""
    network_name = "SmartHomeNet"

    new_nodes = [
        {
            "node_id": "000D6F0000ABCDEF",
            "node_type": "Router",
            "join_credentials": {
                "network_key": "00112233445566778899AABBCCDDEEFF",
                "pan_id": 4660,
                "channel": 15
            }
        },
        {
            "node_id": "000D6F0000FEDCBA",
            "node_type": "EndDevice",
            "join_credentials": {
                "network_key": "00112233445566778899AABBCCDDEEFF",
                "pan_id": 4660,
                "channel": 15
            }
        }
    ]

    results = expand_thread_network(network_name, new_nodes)

    print(f"\nNetwork Expansion Results:")
    print(f"  Successful joins: {len(results['successful_joins'])}")
    print(f"  Failed joins: {len(results['failed_joins'])}")
    print(f"  Initial nodes: {results['topology_changes']['initial_node_count']}")
    print(f"  Final nodes: {results['topology_changes']['final_node_count']}")
    print(f"  Added nodes: {results['topology_changes']['added_nodes']}")

    if results.get("diagnosis"):
        print(f"  Connectivity score: {results['diagnosis']['connectivity_score']:.2%}")
        print(f"  Issues found: {results['diagnosis']['issues_count']}")

if __name__ == "__main__":
    test_network_expansion()
```

**è¿è¡Œç»“æœç¤ºä¾‹**ï¼š

```text
INFO:__main__:Current network has 3 nodes
INFO:__main__:Adding new node 000D6F0000ABCDEF (type: Router)...
INFO:__main__:Node 000D6F0000ABCDEF joined successfully
INFO:__main__:Adding new node 000D6F0000FEDCBA (type: EndDevice)...
INFO:__main__:Node 000D6F0000FEDCBA joined successfully
INFO:__main__:Updating routing tables...
INFO:__main__:Updated routing tables for 5/5 nodes
INFO:__main__:Optimizing routing tables...
INFO:__main__:Running network diagnosis...
INFO:__main__:Network expansion completed: 2 nodes added

Network Expansion Results:
  Successful joins: 2
  Failed joins: 0
  Initial nodes: 3
  Final nodes: 5
  Added nodes: 2
  Connectivity score: 100.00%
  Issues found: 0
```

---

## 9. æ¡ˆä¾‹8ï¼šThreadç½‘ç»œæ•…éšœæ¢å¤

### 9.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
Threadç½‘ç»œä¸­çš„æŸäº›èŠ‚ç‚¹å¯èƒ½å‡ºç°æ•…éšœï¼ˆå¦‚æ–­ç”µã€ä¿¡å·ä¸¢å¤±ã€ç¡¬ä»¶æ•…éšœï¼‰ï¼Œ
ç³»ç»Ÿéœ€è¦è‡ªåŠ¨æ£€æµ‹æ•…éšœï¼Œæ‰§è¡Œæ•…éšœæ¢å¤æµç¨‹ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹é‡æ–°åŠ å…¥ã€è·¯ç”±é‡å»ºã€
ç½‘ç»œåˆ†åŒºåˆå¹¶ç­‰æ“ä½œã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®æ—¶æ£€æµ‹èŠ‚ç‚¹æ•…éšœ
- éœ€è¦åŒºåˆ†ä¸´æ—¶æ•…éšœå’Œæ°¸ä¹…æ•…éšœ
- éœ€è¦è‡ªåŠ¨æ‰§è¡Œæ¢å¤æµç¨‹
- éœ€è¦å¤„ç†ç½‘ç»œåˆ†åŒºæƒ…å†µ
- éœ€è¦æ¢å¤è·¯ç”±è¡¨å®Œæ•´æ€§

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ç½‘ç»œè¯Šæ–­åŠŸèƒ½æ£€æµ‹æ•…éšœï¼Œç»“åˆåˆ†åŒºç®¡ç†å’Œè·¯ç”±é‡å»ºåŠŸèƒ½ï¼Œ
å®ç°è‡ªåŠ¨åŒ–çš„æ•…éšœæ£€æµ‹å’Œæ¢å¤æµç¨‹ã€‚

### 9.2 Schemaå®šä¹‰

**æ•…éšœæ¢å¤Schema**ï¼š

```json
{
  "network_name": "SmartHomeNet",
  "fault_detection": {
    "check_interval_seconds": 30,
    "fault_threshold": {
      "no_response_count": 3,
      "low_link_quality": 1,
      "low_rssi": -90
    }
  },
  "recovery_strategy": {
    "auto_rejoin": true,
    "rebuild_routes": true,
    "merge_partitions": true,
    "notify_administrator": true
  },
  "fault_types": [
    "node_unreachable",
    "low_link_quality",
    "routing_failure",
    "partition_detected"
  ]
}
```

### 9.3 å®ç°ä»£ç 

**å®Œæ•´çš„æ•…éšœæ¢å¤å®ç°**ï¼š

```python
import logging
from datetime import datetime, timedelta
from thread_network_manager import ThreadNetworkManager
from thread_routing_manager import ThreadRoutingManager
from thread_storage import ThreadStorage
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–ç»„ä»¶
storage = ThreadStorage("postgresql://user:pass@localhost/thread")
network_manager = ThreadNetworkManager()
routing_manager = ThreadRoutingManager(network_manager)

class ThreadFaultRecovery:
    """Threadç½‘ç»œæ•…éšœæ¢å¤ç®¡ç†å™¨"""

    def __init__(self, network_name: str, check_interval: int = 30):
        self.network_name = network_name
        self.check_interval = check_interval
        self.fault_history = {}
        self.recovery_actions = []

    def detect_faults(self) -> List[Dict]:
        """æ£€æµ‹ç½‘ç»œæ•…éšœ"""
        faults = []

        try:
            # æ‰§è¡Œç½‘ç»œè¯Šæ–­
            diagnosis = network_manager.diagnose_network(self.network_name)

            # 1. æ£€æµ‹æ–­å¼€è¿æ¥çš„èŠ‚ç‚¹
            disconnected_nodes = diagnosis["connectivity"].get("disconnected_nodes", [])
            for node_id in disconnected_nodes:
                fault = {
                    "node_id": node_id,
                    "fault_type": "node_unreachable",
                    "severity": "high",
                    "detected_at": datetime.now().isoformat(),
                    "details": "Node is not responding"
                }
                faults.append(fault)

            # 2. æ£€æµ‹ä¸å¥åº·èŠ‚ç‚¹
            unhealthy_nodes = diagnosis["node_health"].get("unhealthy_nodes", [])
            for node_id in unhealthy_nodes:
                health_details = diagnosis["node_health"]["health_details"].get(node_id, {})
                issues = health_details.get("issues", [])

                fault = {
                    "node_id": node_id,
                    "fault_type": "node_unhealthy",
                    "severity": "medium",
                    "detected_at": datetime.now().isoformat(),
                    "details": "; ".join(issues)
                }
                faults.append(fault)

            # 3. æ£€æµ‹è·¯ç”±è¡¨ä¸å®Œæ•´
            incomplete_tables = diagnosis["routing"].get("incomplete_tables", [])
            for node_id in incomplete_tables:
                fault = {
                    "node_id": node_id,
                    "fault_type": "routing_failure",
                    "severity": "medium",
                    "detected_at": datetime.now().isoformat(),
                    "details": "Routing table is incomplete"
                }
                faults.append(fault)

            # 4. æ£€æµ‹ç½‘ç»œåˆ†åŒº
            network = network_manager.networks.get(self.network_name)
            if network and "partitions" in network:
                partitions = network["partitions"]
                if len(partitions) > 1:
                    fault = {
                        "node_id": None,
                        "fault_type": "partition_detected",
                        "severity": "high",
                        "detected_at": datetime.now().isoformat(),
                        "details": f"Network partitioned into {len(partitions)} partitions",
                        "partition_count": len(partitions)
                    }
                    faults.append(fault)

            logger.info(f"Detected {len(faults)} faults in network {self.network_name}")
            return faults

        except Exception as e:
            logger.error(f"Fault detection failed: {e}")
            return []

    def recover_from_fault(self, fault: Dict) -> bool:
        """ä»æ•…éšœä¸­æ¢å¤"""
        fault_type = fault["fault_type"]
        node_id = fault.get("node_id")

        logger.info(f"Recovering from fault: {fault_type} (node: {node_id})")

        try:
            if fault_type == "node_unreachable":
                return self._recover_unreachable_node(node_id)
            elif fault_type == "node_unhealthy":
                return self._recover_unhealthy_node(node_id)
            elif fault_type == "routing_failure":
                return self._recover_routing_failure(node_id)
            elif fault_type == "partition_detected":
                return self._recover_partition()
            else:
                logger.warning(f"Unknown fault type: {fault_type}")
                return False
        except Exception as e:
            logger.error(f"Recovery failed for fault {fault_type}: {e}")
            return False

    def _recover_unreachable_node(self, node_id: str) -> bool:
        """æ¢å¤ä¸å¯è¾¾èŠ‚ç‚¹"""
        try:
            # å°è¯•é‡æ–°åŠ å…¥ç½‘ç»œ
            network = network_manager.networks.get(self.network_name)
            if not network:
                return False

            logger.info(f"Attempting to rejoin node {node_id}...")
            success = network_manager._rejoin_network(node_id, self.network_name)

            if success:
                # æ›´æ–°èŠ‚ç‚¹ä¿¡æ¯
                node_info = network_manager.get_node_info(node_id)
                if node_info:
                    storage.store_node(node_info)

                    # è®°å½•æ¢å¤äº‹ä»¶
                    storage.store_event(
                        self.network_name,
                        "node_recovered",
                        {
                            "node_id": node_id,
                            "recovery_type": "rejoin",
                            "mesh_local_address": node_info.get("mesh_local_address")
                        },
                        node_id
                    )

                    logger.info(f"Node {node_id} recovered successfully")
                    return True

            return False
        except Exception as e:
            logger.error(f"Failed to recover unreachable node {node_id}: {e}")
            return False

    def _recover_unhealthy_node(self, node_id: str) -> bool:
        """æ¢å¤ä¸å¥åº·èŠ‚ç‚¹"""
        try:
            # è·å–èŠ‚ç‚¹ä¿¡æ¯
            node_info = network_manager.get_node_info(node_id)
            if not node_info:
                return False

            # å¦‚æœèŠ‚ç‚¹æœ‰çˆ¶èŠ‚ç‚¹ï¼Œå°è¯•é‡æ–°é€‰æ‹©çˆ¶èŠ‚ç‚¹
            if node_info.get("node_type") == "EndDevice":
                # è®©èŠ‚ç‚¹é‡æ–°åŠ å…¥ä»¥é€‰æ‹©æ›´å¥½çš„çˆ¶èŠ‚ç‚¹
                return self._recover_unreachable_node(node_id)

            # å¯¹äºRouterèŠ‚ç‚¹ï¼Œæ›´æ–°è·¯ç”±è¡¨
            routing_manager.update_routing_table(node_id)
            routing_manager.optimize_routing_table(node_id)

            # è®°å½•æ¢å¤äº‹ä»¶
            storage.store_event(
                self.network_name,
                "node_health_improved",
                {"node_id": node_id},
                node_id
            )

            return True
        except Exception as e:
            logger.error(f"Failed to recover unhealthy node {node_id}: {e}")
            return False

    def _recover_routing_failure(self, node_id: str) -> bool:
        """æ¢å¤è·¯ç”±æ•…éšœ"""
        try:
            # æ›´æ–°è·¯ç”±è¡¨
            routing_manager.update_routing_table(node_id)

            # ä¼˜åŒ–è·¯ç”±è¡¨
            routing_manager.optimize_routing_table(node_id)

            # éªŒè¯è·¯ç”±è¡¨
            routes = routing_manager.routing_tables.get(node_id, [])
            if len(routes) > 0:
                # å­˜å‚¨è·¯ç”±è¡¨
                storage.store_routing_table(node_id, routes)

                # è®°å½•æ¢å¤äº‹ä»¶
                storage.store_event(
                    self.network_name,
                    "routing_recovered",
                    {
                        "node_id": node_id,
                        "route_count": len(routes)
                    },
                    node_id
                )

                logger.info(f"Routing recovered for node {node_id}: {len(routes)} routes")
                return True

            return False
        except Exception as e:
            logger.error(f"Failed to recover routing for node {node_id}: {e}")
            return False

    def _recover_partition(self) -> bool:
        """æ¢å¤ç½‘ç»œåˆ†åŒº"""
        try:
            logger.info("Attempting to merge network partitions...")

            # æ£€æµ‹åˆ†åŒº
            network_manager.manage_partition(self.network_name, "detect")

            # åˆå¹¶åˆ†åŒº
            success = network_manager.manage_partition(self.network_name, "merge")

            if success:
                # æ›´æ–°æ‰€æœ‰è·¯ç”±è¡¨
                routing_manager.update_all_routing_tables(self.network_name)

                # è®°å½•æ¢å¤äº‹ä»¶
                storage.store_event(
                    self.network_name,
                    "partition_merged",
                    {"recovery_type": "automatic"}
                )

                logger.info("Network partitions merged successfully")
                return True

            return False
        except Exception as e:
            logger.error(f"Failed to recover partition: {e}")
            return False

    def run_continuous_monitoring(self):
        """æŒç»­ç›‘æ§å’Œæ¢å¤"""
        logger.info(f"Starting continuous fault monitoring for {self.network_name}")

        while True:
            try:
                # æ£€æµ‹æ•…éšœ
                faults = self.detect_faults()

                # å¤„ç†æ¯ä¸ªæ•…éšœ
                for fault in faults:
                    # æ£€æŸ¥æ•…éšœå†å²ï¼Œé¿å…é‡å¤å¤„ç†
                    fault_key = f"{fault['fault_type']}_{fault.get('node_id', 'network')}"
                    last_handled = self.fault_history.get(fault_key)

                    if last_handled:
                        # å¦‚æœæœ€è¿‘å¤„ç†è¿‡ï¼Œè·³è¿‡
                        time_since_handled = datetime.now() - datetime.fromisoformat(last_handled)
                        if time_since_handled.total_seconds() < 300:  # 5åˆ†é’Ÿå†…ä¸é‡å¤å¤„ç†
                            continue

                    # æ‰§è¡Œæ¢å¤
                    recovery_success = self.recover_from_fault(fault)

                    # è®°å½•æ¢å¤æ“ä½œ
                    self.recovery_actions.append({
                        "fault": fault,
                        "recovery_success": recovery_success,
                        "timestamp": datetime.now().isoformat()
                    })

                    # æ›´æ–°æ•…éšœå†å²
                    if recovery_success:
                        self.fault_history[fault_key] = datetime.now().isoformat()

                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                time.sleep(self.check_interval)

            except KeyboardInterrupt:
                logger.info("Stopping fault monitoring...")
                break
            except Exception as e:
                logger.error(f"Error in continuous monitoring: {e}")
                time.sleep(self.check_interval)

# æµ‹è¯•ç”¨ä¾‹
def test_fault_recovery():
    """æµ‹è¯•æ•…éšœæ¢å¤"""
    recovery_manager = ThreadFaultRecovery("SmartHomeNet", check_interval=30)

    # å•æ¬¡æ•…éšœæ£€æµ‹å’Œæ¢å¤
    faults = recovery_manager.detect_faults()
    print(f"\nDetected {len(faults)} faults:")

    for fault in faults:
        print(f"  - {fault['fault_type']}: {fault.get('node_id', 'N/A')} ({fault['severity']})")
        recovery_success = recovery_manager.recover_from_fault(fault)
        print(f"    Recovery: {'Success' if recovery_success else 'Failed'}")

if __name__ == "__main__":
    test_fault_recovery()
    # å–æ¶ˆæ³¨é‡Šä»¥è¿è¡ŒæŒç»­ç›‘æ§
    # recovery_manager = ThreadFaultRecovery("SmartHomeNet")
    # recovery_manager.run_continuous_monitoring()
```

**è¿è¡Œç»“æœç¤ºä¾‹**ï¼š

```text
INFO:__main__:Detected 2 faults in network SmartHomeNet
  - node_unreachable: 000D6F0000123456 (high)
    Recovery: Success
INFO:__main__:Attempting to rejoin node 000D6F0000123456...
INFO:__main__:Node 000D6F0000123456 recovered successfully
  - routing_failure: 000D6F0000654321 (medium)
    Recovery: Success
INFO:__main__:Routing recovered for node 000D6F0000654321: 5 routes
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `04_Transformation.md` - è½¬æ¢ä½“ç³»

---

## 10. æ¡ˆä¾‹9ï¼šå¤§è§„æ¨¡Threadç½‘ç»œç®¡ç†

### 10.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
å¤§è§„æ¨¡Threadç½‘ç»œç®¡ç†ç³»ç»Ÿç®¡ç†æ•°ç™¾ç”šè‡³æ•°åƒä¸ªThreadèŠ‚ç‚¹ï¼Œ
å®ç°ç½‘ç»œæ‹“æ‰‘ç®¡ç†ã€èŠ‚ç‚¹ç›‘æ§ã€èµ„æºåˆ†é…ç­‰åŠŸèƒ½ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å¤§è§„æ¨¡ç½‘ç»œæ‹“æ‰‘ç®¡ç†
- éœ€è¦é«˜æ•ˆçš„èŠ‚ç‚¹ç›‘æ§
- éœ€è¦èµ„æºåˆ†é…ä¼˜åŒ–
- éœ€è¦ç½‘ç»œæ€§èƒ½ä¿è¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨Thread_Schemaå®šä¹‰å¤§è§„æ¨¡ç½‘ç»œç®¡ç†ç»“æ„ï¼Œ
ä½¿ç”¨åˆ†å¸ƒå¼ç®¡ç†æ¶æ„è¿›è¡Œç½‘ç»œç®¡ç†ï¼Œ
ä½¿ç”¨ThreadStorageå­˜å‚¨ç½‘ç»œæ•°æ®ã€‚

### 10.2 Schemaå®šä¹‰

**å¤§è§„æ¨¡Threadç½‘ç»œç®¡ç†Schema**ï¼š

```dsl
schema LargeScaleThreadNetworkManagement {
  management_session_id: String @value("LARGE-NET-20250121-001") @required
  network_name: String @value("LargeScaleNetwork") @required
  management_time: DateTime @value("2025-01-21T10:00:00") @required

  network_scale: {
    total_nodes: Integer @value(1000)
    router_nodes: Integer @value(100)
    end_devices: Integer @value(900)
    network_depth: Integer @value(5)
    average_children_per_router: Decimal @value(9.0)
  } @required

  network_topology: {
    partitions: Integer @value(1)
    isolated_nodes: Integer @value(5)
    connectivity_rate: Decimal @value(0.995) @range(0.0, 1.0)
    average_hops: Decimal @value(3.5)
    max_hops: Integer @value(7)
  } @required

  management_strategies: {
    hierarchical_management: Boolean @value(true)
    distributed_monitoring: Boolean @value(true)
    load_balancing: Boolean @value(true)
    auto_scaling: Boolean @value(true)
  } @required

  performance_metrics: {
    average_latency: Decimal @value(50.0) @unit("ms")
    packet_loss_rate: Decimal @value(0.001) @range(0.0, 1.0)
    network_throughput: Decimal @value(1000.0) @unit("packets/s")
    resource_utilization: Decimal @value(0.75) @range(0.0, 1.0)
  } @required
} @standard("Thread")
```

### 10.3 å®ç°ä»£ç 

```python
from thread_storage import ThreadStorage
from thread_network_manager import ThreadNetworkManager
from datetime import datetime

def large_scale_thread_network_management():
    """å¤§è§„æ¨¡Threadç½‘ç»œç®¡ç†ç¤ºä¾‹"""
    storage = ThreadStorage("postgresql://user:password@localhost/thread_db")
    network_manager = ThreadNetworkManager()

    # ç½‘ç»œè§„æ¨¡
    network_scale = {
        "network_name": "LargeScaleNetwork",
        "total_nodes": 1000,
        "router_nodes": 100,
        "end_devices": 900,
        "network_depth": 5,
        "average_children_per_router": 9.0
    }

    # ç½‘ç»œæ‹“æ‰‘åˆ†æ
    def analyze_network_topology(network_name):
        """åˆ†æç½‘ç»œæ‹“æ‰‘"""
        nodes = network_manager.get_all_nodes(network_name)

        router_count = sum(1 for n in nodes if n.get("node_type") == "Router")
        end_device_count = sum(1 for n in nodes if n.get("node_type") == "EndDevice")

        # è®¡ç®—ç½‘ç»œæ·±åº¦
        max_depth = 0
        for node in nodes:
            depth = calculate_node_depth(node, nodes)
            max_depth = max(max_depth, depth)

        # è®¡ç®—è¿æ¥ç‡
        connected_nodes = sum(1 for n in nodes if n.get("connected", False))
        connectivity_rate = connected_nodes / len(nodes) if nodes else 0.0

        # è®¡ç®—å¹³å‡è·³æ•°
        total_hops = 0
        hop_count = 0
        for node in nodes:
            if node.get("connected"):
                hops = calculate_hops_to_border_router(node, nodes)
                total_hops += hops
                hop_count += 1
        average_hops = total_hops / hop_count if hop_count > 0 else 0.0

        return {
            "partitions": 1,  # ç®€åŒ–ï¼šå‡è®¾å•åˆ†åŒº
            "isolated_nodes": len(nodes) - connected_nodes,
            "connectivity_rate": connectivity_rate,
            "average_hops": average_hops,
            "max_hops": max_depth
        }

    def calculate_node_depth(node, nodes):
        """è®¡ç®—èŠ‚ç‚¹æ·±åº¦"""
        depth = 0
        current = node
        while current.get("parent_id"):
            depth += 1
            parent_id = current["parent_id"]
            current = next((n for n in nodes if n["node_id"] == parent_id), None)
            if not current:
                break
        return depth

    def calculate_hops_to_border_router(node, nodes):
        """è®¡ç®—åˆ°è¾¹ç•Œè·¯ç”±å™¨çš„è·³æ•°"""
        hops = 0
        current = node
        while current.get("parent_id"):
            hops += 1
            parent_id = current["parent_id"]
            current = next((n for n in nodes if n["node_id"] == parent_id), None)
            if not current or current.get("is_border_router"):
                break
        return hops

    # æ‰§è¡Œæ‹“æ‰‘åˆ†æ
    topology = analyze_network_topology(network_scale["network_name"])

    # ç®¡ç†ç­–ç•¥
    management_strategies = {
        "hierarchical_management": True,
        "distributed_monitoring": True,
        "load_balancing": True,
        "auto_scaling": True
    }

    # æ€§èƒ½æŒ‡æ ‡
    performance_metrics = {
        "average_latency": 50.0,
        "packet_loss_rate": 0.001,
        "network_throughput": 1000.0,
        "resource_utilization": 0.75
    }

    # å­˜å‚¨ç®¡ç†æ•°æ®
    management_data = {
        "management_session_id": "LARGE-NET-20250121-001",
        "network_name": network_scale["network_name"],
        "management_time": datetime.now(),
        "total_nodes": network_scale["total_nodes"],
        "router_nodes": network_scale["router_nodes"],
        "end_devices": network_scale["end_devices"],
        "network_depth": network_scale["network_depth"],
        "partitions": topology["partitions"],
        "isolated_nodes": topology["isolated_nodes"],
        "connectivity_rate": topology["connectivity_rate"],
        "average_hops": topology["average_hops"],
        "max_hops": topology["max_hops"],
        "hierarchical_management": management_strategies["hierarchical_management"],
        "distributed_monitoring": management_strategies["distributed_monitoring"],
        "load_balancing": management_strategies["load_balancing"],
        "auto_scaling": management_strategies["auto_scaling"],
        "average_latency": performance_metrics["average_latency"],
        "packet_loss_rate": performance_metrics["packet_loss_rate"],
        "network_throughput": performance_metrics["network_throughput"],
        "resource_utilization": performance_metrics["resource_utilization"]
    }

    # å­˜å‚¨åˆ°æ•°æ®åº“
    management_id = storage.store_network_data(management_data)
    print(f"Large-scale network management data stored: {management_id}")

    print(f"\nLarge-Scale Thread Network Management:")
    print(f"  Network: {network_scale['network_name']}")
    print(f"  Total nodes: {network_scale['total_nodes']}")
    print(f"  Router nodes: {network_scale['router_nodes']}")
    print(f"  End devices: {network_scale['end_devices']}")
    print(f"  Connectivity rate: {topology['connectivity_rate']*100:.2f}%")
    print(f"  Average hops: {topology['average_hops']:.2f}")
    print(f"  Average latency: {performance_metrics['average_latency']:.1f} ms")

    return management_data

if __name__ == "__main__":
    large_scale_thread_network_management()
```

---

## 11. æ¡ˆä¾‹10ï¼šThreadç½‘ç»œæ€§èƒ½ä¼˜åŒ–

### 11.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
Threadç½‘ç»œæ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿç›‘æµ‹å’Œåˆ†æç½‘ç»œæ€§èƒ½ï¼Œ
è¯†åˆ«æ€§èƒ½ç“¶é¢ˆï¼Œä¼˜åŒ–ç½‘ç»œé…ç½®ï¼Œæé«˜ç½‘ç»œæ•ˆç‡ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦æ€§èƒ½æ•°æ®æ”¶é›†
- éœ€è¦æ€§èƒ½åˆ†æç®—æ³•
- éœ€è¦ä¼˜åŒ–ç­–ç•¥
- éœ€è¦æ•ˆæœè¯„ä¼°

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨Thread_Schemaæ”¶é›†æ€§èƒ½æ•°æ®ï¼Œ
ä½¿ç”¨ä¼˜åŒ–ç®—æ³•è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ï¼Œ
ä½¿ç”¨ThreadStorageå­˜å‚¨æ€§èƒ½æ•°æ®ã€‚

### 11.2 Schemaå®šä¹‰

**Threadç½‘ç»œæ€§èƒ½ä¼˜åŒ–Schema**ï¼š

```dsl
schema ThreadNetworkPerformanceOptimization {
  optimization_session_id: String @value("PERF-OPT-20250121-001") @required
  network_name: String @value("OptimizedNetwork") @required
  optimization_time: DateTime @value("2025-01-21T10:00:00") @required

  performance_baseline: {
    average_latency: Decimal @value(80.0) @unit("ms")
    packet_loss_rate: Decimal @value(0.005) @range(0.0, 1.0)
    network_throughput: Decimal @value(800.0) @unit("packets/s")
    energy_consumption: Decimal @value(100.0) @unit("mW")
  } @required

  performance_bottlenecks: [
    {
      bottleneck_type: String @value("High latency")
      location: String @value("Router-001")
      severity: Enum { Medium } @value(Medium)
      impact: Decimal @value(0.15) @unit("15% performance degradation")
    }
  ] @required

  optimization_strategies: [
    {
      strategy: String @value("è·¯ç”±è¡¨ä¼˜åŒ–")
      expected_improvement: Decimal @value(0.20) @unit("20% latency reduction")
      implementation_complexity: Enum { Medium } @value(Medium)
    },
    {
      strategy: String @value("è´Ÿè½½å‡è¡¡")
      expected_improvement: Decimal @value(0.15) @unit("15% throughput increase")
      implementation_complexity: Enum { Low } @value(Low)
    }
  ] @required

  optimization_results: {
    latency_improvement: Decimal @value(0.18) @unit("18% reduction")
    throughput_improvement: Decimal @value(0.12) @unit("12% increase")
    energy_savings: Decimal @value(0.10) @unit("10% reduction")
    overall_improvement: Decimal @value(0.15) @unit("15% improvement")
  } @required
} @standard("Thread")
```

### 11.3 å®ç°ä»£ç 

```python
from thread_storage import ThreadStorage
from thread_network_manager import ThreadNetworkManager
from datetime import datetime

def thread_network_performance_optimization():
    """Threadç½‘ç»œæ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹"""
    storage = ThreadStorage("postgresql://user:password@localhost/thread_db")
    network_manager = ThreadNetworkManager()

    # æ€§èƒ½åŸºçº¿
    performance_baseline = {
        "network_name": "OptimizedNetwork",
        "average_latency": 80.0,
        "packet_loss_rate": 0.005,
        "network_throughput": 800.0,
        "energy_consumption": 100.0
    }

    # æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
    def identify_bottlenecks(network_name):
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        nodes = network_manager.get_all_nodes(network_name)

        for node in nodes:
            node_latency = node.get("average_latency", 0)
            node_load = node.get("load", 0)

            # è¯†åˆ«é«˜å»¶è¿ŸèŠ‚ç‚¹
            if node_latency > 100:
                bottlenecks.append({
                    "bottleneck_type": "High latency",
                    "location": node["node_id"],
                    "severity": "Medium",
                    "impact": 0.15
                })

            # è¯†åˆ«é«˜è´Ÿè½½èŠ‚ç‚¹
            if node_load > 0.8:
                bottlenecks.append({
                    "bottleneck_type": "High load",
                    "location": node["node_id"],
                    "severity": "High",
                    "impact": 0.25
                })

        return bottlenecks

    # è¯†åˆ«ç“¶é¢ˆ
    bottlenecks = identify_bottlenecks(performance_baseline["network_name"])

    # ä¼˜åŒ–ç­–ç•¥
    optimization_strategies = []

    if any(b["bottleneck_type"] == "High latency" for b in bottlenecks):
        optimization_strategies.append({
            "strategy": "è·¯ç”±è¡¨ä¼˜åŒ–",
            "expected_improvement": 0.20,
            "implementation_complexity": "Medium"
        })

    if any(b["bottleneck_type"] == "High load" for b in bottlenecks):
        optimization_strategies.append({
            "strategy": "è´Ÿè½½å‡è¡¡",
            "expected_improvement": 0.15,
            "implementation_complexity": "Low"
        })

    # æ‰§è¡Œä¼˜åŒ–ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
    optimization_results = {
        "latency_improvement": 0.18,
        "throughput_improvement": 0.12,
        "energy_savings": 0.10,
        "overall_improvement": 0.15
    }

    # å­˜å‚¨ä¼˜åŒ–æ•°æ®
    optimization_data = {
        "optimization_session_id": "PERF-OPT-20250121-001",
        "network_name": performance_baseline["network_name"],
        "optimization_time": datetime.now(),
        "baseline_latency": performance_baseline["average_latency"],
        "baseline_packet_loss": performance_baseline["packet_loss_rate"],
        "baseline_throughput": performance_baseline["network_throughput"],
        "baseline_energy": performance_baseline["energy_consumption"],
        "bottlenecks": bottlenecks,
        "optimization_strategies": optimization_strategies,
        "latency_improvement": optimization_results["latency_improvement"],
        "throughput_improvement": optimization_results["throughput_improvement"],
        "energy_savings": optimization_results["energy_savings"],
        "overall_improvement": optimization_results["overall_improvement"]
    }

    # å­˜å‚¨åˆ°æ•°æ®åº“
    optimization_id = storage.store_network_data(optimization_data)
    print(f"Performance optimization data stored: {optimization_id}")

    print(f"\nThread Network Performance Optimization:")
    print(f"  Network: {performance_baseline['network_name']}")
    print(f"  Baseline latency: {performance_baseline['average_latency']:.1f} ms")
    print(f"  Bottlenecks identified: {len(bottlenecks)}")
    print(f"  Optimization strategies: {len(optimization_strategies)}")
    print(f"  Latency improvement: {optimization_results['latency_improvement']*100:.1f}%")
    print(f"  Throughput improvement: {optimization_results['throughput_improvement']*100:.1f}%")
    print(f"  Overall improvement: {optimization_results['overall_improvement']*100:.1f}%")

    return optimization_data

if __name__ == "__main__":
    thread_network_performance_optimization()
```

---

## 12. æ¡ˆä¾‹11ï¼šThreadç½‘ç»œå®‰å…¨åŠ å›º

### 12.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
Threadç½‘ç»œå®‰å…¨åŠ å›ºç³»ç»Ÿå¢å¼ºç½‘ç»œå®‰å…¨æ€§ï¼Œ
å®ç°å®‰å…¨ç­–ç•¥ç®¡ç†ã€å¨èƒæ£€æµ‹ã€å®‰å…¨äº‹ä»¶å“åº”ç­‰åŠŸèƒ½ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®‰å…¨ç­–ç•¥ç®¡ç†
- éœ€è¦å¨èƒæ£€æµ‹
- éœ€è¦å®‰å…¨äº‹ä»¶å“åº”
- éœ€è¦å®‰å…¨å®¡è®¡

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨Thread_Schemaå®šä¹‰å®‰å…¨ç­–ç•¥ï¼Œ
ä½¿ç”¨å®‰å…¨ç®—æ³•è¿›è¡Œå¨èƒæ£€æµ‹ï¼Œ
ä½¿ç”¨ThreadStorageå­˜å‚¨å®‰å…¨æ•°æ®ã€‚

### 12.2 Schemaå®šä¹‰

**Threadç½‘ç»œå®‰å…¨åŠ å›ºSchema**ï¼š

```dsl
schema ThreadNetworkSecurityHardening {
  security_session_id: String @value("SEC-HARDEN-20250121-001") @required
  network_name: String @value("SecuredNetwork") @required
  security_time: DateTime @value("2025-01-21T10:00:00") @required

  security_policies: {
    authentication_enabled: Boolean @value(true)
    encryption_enabled: Boolean @value(true)
    key_rotation_interval: Integer @value(30) @unit("days")
    access_control_enabled: Boolean @value(true)
    intrusion_detection_enabled: Boolean @value(true)
  } @required

  security_threats: [
    {
      threat_type: String @value("Unauthorized access attempt")
      source_node: String @value("Node-001")
      severity: Enum { Medium } @value(Medium)
      detected_at: DateTime @value("2025-01-21T09:30:00")
      status: Enum { Blocked } @value(Blocked)
    }
  ] @required

  security_measures: [
    {
      measure: String @value("å¯†é’¥è½®æ¢")
      implementation_status: Enum { Implemented } @value(Implemented)
      effectiveness: Decimal @value(0.90) @range(0.0, 1.0)
    },
    {
      measure: String @value("è®¿é—®æ§åˆ¶åˆ—è¡¨")
      implementation_status: Enum { Implemented } @value(Implemented)
      effectiveness: Decimal @value(0.85) @range(0.0, 1.0)
    }
  ] @required

  security_metrics: {
    threat_detection_rate: Decimal @value(0.95) @range(0.0, 1.0)
    false_positive_rate: Decimal @value(0.05) @range(0.0, 1.0)
    security_incidents: Integer @value(3)
    security_score: Decimal @value(0.88) @range(0.0, 1.0)
  } @required
} @standard("Thread")
```

### 12.3 å®ç°ä»£ç 

```python
from thread_storage import ThreadStorage
from thread_network_manager import ThreadNetworkManager
from datetime import datetime

def thread_network_security_hardening():
    """Threadç½‘ç»œå®‰å…¨åŠ å›ºç¤ºä¾‹"""
    storage = ThreadStorage("postgresql://user:password@localhost/thread_db")
    network_manager = ThreadNetworkManager()

    # å®‰å…¨ç­–ç•¥
    security_policies = {
        "network_name": "SecuredNetwork",
        "authentication_enabled": True,
        "encryption_enabled": True,
        "key_rotation_interval": 30,
        "access_control_enabled": True,
        "intrusion_detection_enabled": True
    }

    # å¨èƒæ£€æµ‹
    def detect_security_threats(network_name):
        """æ£€æµ‹å®‰å…¨å¨èƒ"""
        threats = []
        nodes = network_manager.get_all_nodes(network_name)

        for node in nodes:
            # æ£€æµ‹æœªæˆæƒè®¿é—®å°è¯•
            unauthorized_attempts = node.get("unauthorized_attempts", 0)
            if unauthorized_attempts > 0:
                threats.append({
                    "threat_type": "Unauthorized access attempt",
                    "source_node": node["node_id"],
                    "severity": "Medium" if unauthorized_attempts < 5 else "High",
                    "detected_at": datetime.now(),
                    "status": "Blocked"
                })

            # æ£€æµ‹å¼‚å¸¸è¡Œä¸º
            if node.get("anomaly_score", 0) > 0.7:
                threats.append({
                    "threat_type": "Anomalous behavior",
                    "source_node": node["node_id"],
                    "severity": "High",
                    "detected_at": datetime.now(),
                    "status": "Under investigation"
                })

        return threats

    # æ£€æµ‹å¨èƒ
    threats = detect_security_threats(security_policies["network_name"])

    # å®‰å…¨æªæ–½
    security_measures = [
        {
            "measure": "å¯†é’¥è½®æ¢",
            "implementation_status": "Implemented",
            "effectiveness": 0.90
        },
        {
            "measure": "è®¿é—®æ§åˆ¶åˆ—è¡¨",
            "implementation_status": "Implemented",
            "effectiveness": 0.85
        },
        {
            "measure": "å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ",
            "implementation_status": "Implemented",
            "effectiveness": 0.88
        }
    ]

    # å®‰å…¨æŒ‡æ ‡
    security_metrics = {
        "threat_detection_rate": 0.95,
        "false_positive_rate": 0.05,
        "security_incidents": len(threats),
        "security_score": 0.88
    }

    # å­˜å‚¨å®‰å…¨æ•°æ®
    security_data = {
        "security_session_id": "SEC-HARDEN-20250121-001",
        "network_name": security_policies["network_name"],
        "security_time": datetime.now(),
        "authentication_enabled": security_policies["authentication_enabled"],
        "encryption_enabled": security_policies["encryption_enabled"],
        "key_rotation_interval": security_policies["key_rotation_interval"],
        "access_control_enabled": security_policies["access_control_enabled"],
        "intrusion_detection_enabled": security_policies["intrusion_detection_enabled"],
        "threats": threats,
        "security_measures": security_measures,
        "threat_detection_rate": security_metrics["threat_detection_rate"],
        "false_positive_rate": security_metrics["false_positive_rate"],
        "security_incidents": security_metrics["security_incidents"],
        "security_score": security_metrics["security_score"]
    }

    # å­˜å‚¨åˆ°æ•°æ®åº“
    security_id = storage.store_network_data(security_data)
    print(f"Security hardening data stored: {security_id}")

    print(f"\nThread Network Security Hardening:")
    print(f"  Network: {security_policies['network_name']}")
    print(f"  Authentication: {'Enabled' if security_policies['authentication_enabled'] else 'Disabled'}")
    print(f"  Encryption: {'Enabled' if security_policies['encryption_enabled'] else 'Disabled'}")
    print(f"  Threats detected: {len(threats)}")
    print(f"  Security measures: {len(security_measures)}")
    print(f"  Threat detection rate: {security_metrics['threat_detection_rate']*100:.1f}%")
    print(f"  Security score: {security_metrics['security_score']:.2f}")

    return security_data

if __name__ == "__main__":
    thread_network_security_hardening()
```

---

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21


---

## 12. æ¡ˆä¾‹12ï¼šThreadç½‘ç»œä¿¡é“ä¼˜åŒ–ä¸å¹²æ‰°è§„é¿

### 12.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
Threadç½‘ç»œè¿è¡Œåœ¨2.4GHzé¢‘æ®µï¼Œå®¹æ˜“å—åˆ°WiFiã€è“ç‰™ç­‰å…¶ä»–æ— çº¿è®¾å¤‡çš„å¹²æ‰°ã€‚éœ€è¦å®šæœŸè¯„ä¼°å„ä¿¡é“çš„å¹²æ‰°æƒ…å†µï¼ŒåŠ¨æ€é€‰æ‹©æœ€ä¼˜ä¿¡é“ï¼Œç¡®ä¿ç½‘ç»œç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®æ—¶ç›‘æµ‹å„ä¿¡é“çš„èƒ½é‡æ£€æµ‹(ED)å€¼
- éœ€è¦è¯„ä¼°é“¾è·¯è´¨é‡ä¸ä¿¡é“çš„å…³è”
- éœ€è¦åœ¨ä¸åœæœºçš„æƒ…å†µä¸‹åˆ‡æ¢ä¿¡é“
- éœ€è¦é¢„æµ‹å¹²æ‰°è¶‹åŠ¿

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨Threadçš„ä¿¡é“è¯„ä¼°åŠŸèƒ½å’Œèƒ½é‡æ£€æµ‹APIï¼Œç»“åˆå†å²æ•°æ®åˆ†æï¼Œå®ç°æ™ºèƒ½ä¿¡é“é€‰æ‹©å’Œåˆ‡æ¢ã€‚

### 12.2 Schemaå®šä¹‰

**ä¿¡é“ä¼˜åŒ–Schema**ï¼š

```json
{
  "channel_assessment": {
    "network_name": "SmartHomeNet",
    "assessment_time": "2025-02-14T10:00:00Z",
    "current_channel": 15,
    "channel_metrics": [
      {
        "channel": 11,
        "energy_detect": -75,
        "link_success_rate": 0.92,
        "assessment_score": 85
      },
      {
        "channel": 15,
        "energy_detect": -82,
        "link_success_rate": 0.98,
        "assessment_score": 95
      },
      {
        "channel": 20,
        "energy_detect": -68,
        "link_success_rate": 0.78,
        "assessment_score": 60
      }
    ],
    "recommendation": {
      "action": "maintain",
      "reason": "Current channel has good performance"
    }
  }
}
```

### 12.3 å®ç°ä»£ç 

```python
from thread_storage import ThreadStorage
from typing import List, Dict
import statistics

class ThreadChannelOptimizer:
    """Threadä¿¡é“ä¼˜åŒ–å™¨"""

    CHANNELS = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]

    def __init__(self, storage: ThreadStorage):
        self.storage = storage

    def assess_channel(self, network_name: str, channel: int) -> Dict:
        """è¯„ä¼°å•ä¸ªä¿¡é“"""
        # è·å–è¯¥ä¿¡é“ä¸Šçš„èŠ‚ç‚¹é“¾è·¯è´¨é‡
        self.storage.cur.execute("""
            SELECT link_quality, rssi
            FROM thread_nodes
            WHERE network_name = %s
            AND last_seen > NOW() - INTERVAL '1 hour'
        """, (network_name,))

        link_qualities = []
        rssis = []
        for row in self.storage.cur.fetchall():
            if row[0]:
                link_qualities.append(row[0])
            if row[1]:
                rssis.append(row[1])

        # æ¨¡æ‹Ÿèƒ½é‡æ£€æµ‹å€¼ï¼ˆå®é™…åº”ä»è®¾å¤‡è·å–ï¼‰
        energy_detect = -80 + (channel % 5) * 2  # æ¨¡æ‹Ÿå€¼

        # è®¡ç®—è¯„ä¼°åˆ†æ•°
        if link_qualities:
            avg_lqi = statistics.mean(link_qualities)
            success_rate = avg_lqi / 255
        else:
            success_rate = 0.5

        # ç»¼åˆè¯„åˆ†
        score = success_rate * 100 + (energy_detect + 100)  # èƒ½é‡è¶Šä½(è¶Šè´Ÿ)åˆ†æ•°è¶Šé«˜
        score = min(100, max(0, score))

        return {
            "channel": channel,
            "energy_detect": energy_detect,
            "link_success_rate": round(success_rate, 2),
            "avg_rssi": statistics.mean(rssis) if rssis else None,
            "assessment_score": round(score, 1)
        }

    def assess_all_channels(self, network_name: str) -> List[Dict]:
        """è¯„ä¼°æ‰€æœ‰ä¿¡é“"""
        results = []
        for channel in self.CHANNELS:
            assessment = self.assess_channel(network_name, channel)
            results.append(assessment)
            
            # å­˜å‚¨è¯„ä¼°ç»“æœ
            self.storage.store_channel_assessment(
                network_name, channel,
                assessment["energy_detect"],
                int(assessment["link_success_rate"] * 255)
            )

        return sorted(results, key=lambda x: x["assessment_score"], reverse=True)

    def recommend_channel(self, network_name: str, current_channel: int) -> Dict:
        """æ¨èæœ€ä¼˜ä¿¡é“"""
        assessments = self.assess_all_channels(network_name)
        
        best = assessments[0]
        current = next((a for a in assessments if a["channel"] == current_channel), None)

        recommendation = {
            "current_channel": current_channel,
            "best_channel": best["channel"],
            "current_score": current["assessment_score"] if current else 0,
            "best_score": best["assessment_score"],
            "action": "maintain",
            "reason": ""
        }

        if best["channel"] != current_channel:
            score_diff = best["assessment_score"] - (current["assessment_score"] if current else 0)
            if score_diff > 10:
                recommendation["action"] = "switch"
                recommendation["reason"] = f"Channel {best['channel']} is significantly better (+{score_diff:.1f} points)"
            else:
                recommendation["reason"] = f"Current channel is acceptable, alternative only slightly better (+{score_diff:.1f} points)"
        else:
            recommendation["reason"] = "Current channel has best performance"

        return recommendation

    def plan_channel_switch(self, network_name: str, new_channel: int) -> Dict:
        """è§„åˆ’ä¿¡é“åˆ‡æ¢"""
        # è·å–å½“å‰ç½‘ç»œçŠ¶æ€
        nodes = self.storage.get_network_nodes(network_name)
        
        # è¯„ä¼°åˆ‡æ¢å½±å“
        impact = {
            "total_nodes": len(nodes),
            "router_count": len([n for n in nodes if n["node_type"] == "Router"]),
            "estimated_downtime_seconds": 30,  # é¢„ä¼°åœæœºæ—¶é—´
            "risk_level": "low"
        }

        plan = {
            "from_channel": self.get_current_channel(network_name),
            "to_channel": new_channel,
            "steps": [
                "1. Notify all nodes about pending channel switch",
                "2. Wait for ACK from all routers",
                "3. Execute channel switch on Leader",
                "4. Verify all nodes have switched",
                "5. Resume normal operation"
            ],
            "impact": impact,
            "rollback_plan": "If switch fails, revert to original channel and investigate"
        }

        return plan

    def get_current_channel(self, network_name: str) -> int:
        """è·å–å½“å‰ä¿¡é“"""
        self.storage.cur.execute("""
            SELECT channel FROM thread_networks WHERE network_name = %s
        """, (network_name,))
        row = self.storage.cur.fetchone()
        return row[0] if row else 15

# ä½¿ç”¨ç¤ºä¾‹
def demo_channel_optimization():
    storage = ThreadStorage("postgresql://user:pass@localhost/thread")
    optimizer = ThreadChannelOptimizer(storage)

    # è¯„ä¼°æ‰€æœ‰ä¿¡é“
    assessments = optimizer.assess_all_channels("SmartHomeNet")
    print("Channel Assessments:")
    for a in assessments:
        print(f"  Channel {a['channel']}: Score={a['assessment_score']}, ED={a['energy_detect']}dBm")

    # è·å–æ¨è
    recommendation = optimizer.recommend_channel("SmartHomeNet", current_channel=15)
    print(f"\nRecommendation: {recommendation['action']}")
    print(f"Reason: {recommendation['reason']}")
```

---

## 13. æ¡ˆä¾‹13ï¼šThreadç½‘ç»œç”µæ± ä¾›ç”µè®¾å¤‡ä¼˜åŒ–

### 13.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
Threadç½‘ç»œä¸­çš„ä¼ æ„Ÿå™¨ç­‰è®¾å¤‡é€šå¸¸é‡‡ç”¨ç”µæ± ä¾›ç”µï¼Œéœ€è¦ä¼˜åŒ–å…¶åŠŸè€—ä»¥å»¶é•¿ç”µæ± å¯¿å‘½ã€‚é€šè¿‡è°ƒæ•´è½®è¯¢é—´éš”ã€æ•°æ®ä¸ŠæŠ¥ç­–ç•¥å’Œçˆ¶èŠ‚ç‚¹é€‰æ‹©ï¼Œå®ç°ç”µæ± å¯¿å‘½æœ€å¤§åŒ–ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å¹³è¡¡åŠŸè€—å’Œæ•°æ®å®æ—¶æ€§
- éœ€è¦ä¼˜åŒ–çˆ¶èŠ‚ç‚¹é€‰æ‹©å‡å°‘é‡ä¼ 
- éœ€è¦æ™ºèƒ½è°ƒæ•´è½®è¯¢é—´éš”
- éœ€è¦é¢„æµ‹ç”µæ± å¯¿å‘½

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨Threadçš„ sleepy end device æ¨¡å¼ï¼Œç»“åˆæ•°æ®ç¼“å­˜å’Œæ‰¹é‡ä¸ŠæŠ¥ç­–ç•¥ï¼Œå®ç°ä½åŠŸè€—è¿è¡Œã€‚

### 13.2 Schemaå®šä¹‰

**ç”µæ± ä¼˜åŒ–Schema**ï¼š

```json
{
  "battery_optimization": {
    "device_id": "SENSOR_001",
    "device_type": "SleepyEndDevice",
    "battery_profile": {
      "current_level": 78,
      "battery_capacity_mah": 2400,
      "avg_consumption_ma": 0.15,
      "estimated_life_days": 520
    },
    "power_optimization": {
      "poll_period_ms": 5000,
      "data_retention_period_ms": 300000,
      "batch_upload": true,
      "parent_selection_strategy": "balanced"
    },
    "parent_candidate_ranking": [
      {
        "node_id": "ROUTER_001",
        "link_margin": 25,
        "current_children": 3,
        "rank_score": 92
      }
    ]
  }
}
```

### 13.3 å®ç°ä»£ç 

```python
from datetime import datetime, timedelta

class ThreadBatteryOptimizer:
    """Threadç”µæ± ä¼˜åŒ–å™¨"""

    def __init__(self, storage: ThreadStorage):
        self.storage = storage

    def calculate_battery_life(self, device_id: str,
                               battery_capacity_mah: float = 2400,
                               avg_consumption_ma: float = 0.15) -> Dict:
        """è®¡ç®—ç”µæ± å¯¿å‘½"""
        # è·å–è®¾å¤‡å†å²åŠŸè€—æ•°æ®
        history = self.storage.get_node_performance_history(device_id, hours=168)  # 7å¤©

        if history:
            # åŸºäºå†å²æ•°æ®è®¡ç®—å¹³å‡åŠŸè€—
            avg_consumption = sum(h.get("latency_ms", 0) for h in history) / len(history) / 1000 * 10
        else:
            avg_consumption = avg_consumption_ma

        # è®¡ç®—é¢„æœŸå¯¿å‘½ï¼ˆå°æ—¶ï¼‰
        estimated_hours = battery_capacity_mah / avg_consumption
        estimated_days = estimated_hours / 24

        # è®¡ç®—ç”µæ± è€—å°½æ—¥æœŸ
        depletion_date = datetime.now() + timedelta(days=estimated_days)

        return {
            "device_id": device_id,
            "current_battery_level": self._get_battery_level(device_id),
            "battery_capacity_mah": battery_capacity_mah,
            "avg_consumption_ma": round(avg_consumption, 3),
            "estimated_life_days": round(estimated_days, 1),
            "estimated_depletion_date": depletion_date.strftime("%Y-%m-%d"),
            "low_battery_warning_days": 30
        }

    def _get_battery_level(self, device_id: str) -> int:
        """è·å–ç”µæ± ç”µé‡"""
        node = self.storage.get_node_by_id(device_id)
        return node.get("battery_level", 100) if node else 100

    def optimize_poll_period(self, device_id: str, desired_latency_ms: int = 5000) -> int:
        """ä¼˜åŒ–è½®è¯¢å‘¨æœŸ"""
        battery_level = self._get_battery_level(device_id)

        # æ ¹æ®ç”µé‡è°ƒæ•´è½®è¯¢å‘¨æœŸ
        if battery_level > 50:
            poll_period = max(desired_latency_ms, 1000)  # æœ€å¿«1ç§’
        elif battery_level > 20:
            poll_period = max(desired_latency_ms * 2, 5000)  # ç”µé‡ä¸­ç­‰ï¼Œå»¶é•¿2å€
        else:
            poll_period = max(desired_latency_ms * 5, 30000)  # ç”µé‡ä½ï¼Œå»¶é•¿5å€

        return poll_period

    def select_optimal_parent(self, device_id: str, candidates: List[str]) -> str:
        """é€‰æ‹©æœ€ä¼˜çˆ¶èŠ‚ç‚¹ï¼ˆè€ƒè™‘åŠŸè€—ï¼‰"""
        scored_parents = []

        for candidate in candidates:
            node = self.storage.get_node_by_id(candidate)
            if not node:
                continue

            score = 0
            reasons = []

            # é“¾è·¯è´¨é‡è¯„åˆ† (0-40)
            link_quality = node.get("link_quality", 0)
            link_score = (link_quality / 255) * 40
            score += link_score
            reasons.append(f"Link quality: {link_score:.1f}")

            # å­èŠ‚ç‚¹æ•°é‡è¯„åˆ† - è¶Šå°‘è¶Šå¥½ (0-30)
            children_count = self._get_children_count(candidate)
            child_score = max(0, 30 - children_count * 5)
            score += child_score
            reasons.append(f"Child load: {child_score:.1f}")

            # èŠ‚ç‚¹ç±»å‹è¯„åˆ† - Routerä¼˜å…ˆ (0-30)
            if node.get("node_type") == "Router":
                score += 30
                reasons.append("Type: Router (30)")
            else:
                score += 10
                reasons.append("Type: Other (10)")

            scored_parents.append({
                "node_id": candidate,
                "score": score,
                "reasons": reasons
            })

        if not scored_parents:
            return None

        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„
        best = max(scored_parents, key=lambda x: x["score"])
        return best["node_id"]

    def _get_children_count(self, node_id: str) -> int:
        """è·å–å­èŠ‚ç‚¹æ•°é‡"""
        self.storage.cur.execute("""
            SELECT COUNT(*) FROM thread_nodes
            WHERE parent_node_id = %s
        """, (node_id,))
        row = self.storage.cur.fetchone()
        return row[0] if row else 0

    def generate_power_report(self, network_name: str) -> Dict:
        """ç”ŸæˆåŠŸè€—æŠ¥å‘Š"""
        nodes = self.storage.get_network_nodes(network_name)
        
        sleepy_devices = [n for n in nodes if n.get("node_type") == "SleepyEndDevice"]
        
        report = {
            "network_name": network_name,
            "total_sleepy_devices": len(sleepy_devices),
            "low_battery_devices": [],
            "avg_battery_level": 0,
            "estimated_network_life_days": 0
        }

        total_battery = 0
        for device in sleepy_devices:
            battery = device.get("battery_level", 100)
            total_battery += battery

            if battery < 20:
                report["low_battery_devices"].append({
                    "device_id": device["node_id"],
                    "battery_level": battery
                })

        if sleepy_devices:
            report["avg_battery_level"] = round(total_battery / len(sleepy_devices), 1)

        return report

# ä½¿ç”¨ç¤ºä¾‹
def demo_battery_optimization():
    storage = ThreadStorage("postgresql://user:pass@localhost/thread")
    optimizer = ThreadBatteryOptimizer(storage)

    # è®¡ç®—ç”µæ± å¯¿å‘½
    battery_info = optimizer.calculate_battery_life("SENSOR_001")
    print(f"Battery Info: {battery_info}")

    # ä¼˜åŒ–è½®è¯¢å‘¨æœŸ
    poll_period = optimizer.optimize_poll_period("SENSOR_001")
    print(f"Optimized poll period: {poll_period}ms")

    # ç”ŸæˆåŠŸè€—æŠ¥å‘Š
    report = optimizer.generate_power_report("SmartHomeNet")
    print(f"Power Report: {report}")
```

---

## 14. æ¡ˆä¾‹14ï¼šThreadç½‘ç»œæœåŠ¡è´¨é‡(QoS)ä¿éšœ

### 14.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
Threadç½‘ç»œéœ€è¦ä¸ºä¸åŒç±»å‹çš„åº”ç”¨æä¾›å·®å¼‚åŒ–çš„æœåŠ¡è´¨é‡ä¿éšœã€‚ä¾‹å¦‚ï¼Œå®‰é˜²ä¼ æ„Ÿå™¨çš„æ•°æ®éœ€è¦ä¼˜å…ˆä¼ è¾“ï¼Œè€Œç¯å¢ƒä¼ æ„Ÿå™¨çš„æ•°æ®å¯ä»¥å®¹å¿å»¶è¿Ÿã€‚éœ€è¦å®ç°æ¶ˆæ¯ä¼˜å…ˆçº§ç®¡ç†å’Œå¸¦å®½åˆ†é…ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦åŒºåˆ†ä¸åŒä¼˜å…ˆçº§çš„æµé‡
- éœ€è¦é¿å…ä½ä¼˜å…ˆçº§æµé‡é¥¿æ­»
- éœ€è¦åŠ¨æ€è°ƒæ•´QoSç­–ç•¥
- éœ€è¦ç›‘æ§QoSæŒ‡æ ‡

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨CoAPæ¶ˆæ¯ä¼˜å…ˆçº§æ ‡è®°å’Œé˜Ÿåˆ—ç®¡ç†ï¼Œç»“åˆæµé‡æ•´å½¢ï¼Œå®ç°å·®å¼‚åŒ–æœåŠ¡ã€‚

### 14.2 Schemaå®šä¹‰

**QoSä¿éšœSchema**ï¼š

```json
{
  "qos_configuration": {
    "network_name": "SmartHomeNet",
    "traffic_classes": [
      {
        "class_id": 1,
        "name": "Emergency",
        "priority": 7,
        "max_latency_ms": 100,
        "bandwidth_share": 30
      },
      {
        "class_id": 2,
        "name": "Control",
        "priority": 5,
        "max_latency_ms": 500,
        "bandwidth_share": 40
      },
      {
        "class_id": 3,
        "name": "Telemetry",
        "priority": 3,
        "max_latency_ms": 5000,
        "bandwidth_share": 20
      },
      {
        "class_id": 4,
        "name": "Background",
        "priority": 1,
        "max_latency_ms": 30000,
        "bandwidth_share": 10
      }
    ]
  }
}
```

### 14.3 å®ç°ä»£ç 

```python
from enum import IntEnum
from typing import Dict, List

class TrafficClass(IntEnum):
    """æµé‡ç±»åˆ«"""
    BACKGROUND = 1
    TELEMETRY = 3
    CONTROL = 5
    EMERGENCY = 7

class ThreadQoSManager:
    """Thread QoSç®¡ç†å™¨"""

    def __init__(self, storage: ThreadStorage):
        self.storage = storage
        self.class_definitions = {
            TrafficClass.EMERGENCY: {
                "max_latency_ms": 100,
                "bandwidth_share": 30
            },
            TrafficClass.CONTROL: {
                "max_latency_ms": 500,
                "bandwidth_share": 40
            },
            TrafficClass.TELEMETRY: {
                "max_latency_ms": 5000,
                "bandwidth_share": 20
            },
            TrafficClass.BACKGROUND: {
                "max_latency_ms": 30000,
                "bandwidth_share": 10
            }
        }

    def classify_message(self, device_id: str, message_type: str) -> TrafficClass:
        """æ¶ˆæ¯åˆ†ç±»"""
        # æ ¹æ®è®¾å¤‡ç±»å‹å’Œæ¶ˆæ¯ç±»å‹ç¡®å®šä¼˜å…ˆçº§
        emergency_types = ["emergency_alert", "fire_alarm", "intrusion_detected"]
        control_types = ["light_control", "door_lock", "thermostat_set"]
        telemetry_types = ["temperature", "humidity", "occupancy"]

        if message_type in emergency_types:
            return TrafficClass.EMERGENCY
        elif message_type in control_types:
            return TrafficClass.CONTROL
        elif message_type in telemetry_types:
            return TrafficClass.TELEMETRY
        else:
            return TrafficClass.BACKGROUND

    def store_qos_message(self, network_name: str, source: str, dest: str,
                         msg_type: str, payload_size: int,
                         response_time: int = None):
        """å­˜å‚¨å¸¦QoSæ ‡è®°çš„æ¶ˆæ¯"""
        traffic_class = self.classify_message(source, msg_type)
        
        self.storage.store_coap_message(
            network_name=network_name,
            source=source,
            dest=dest,
            msg_type=msg_type,
            code="POST",
            msg_id=0,
            payload_size=payload_size,
            response_time=response_time,
            success=True
        )

    def analyze_qos_performance(self, network_name: str, hours: int = 24) -> Dict:
        """åˆ†æQoSæ€§èƒ½"""
        self.storage.cur.execute("""
            SELECT 
                message_type,
                AVG(response_time_ms) as avg_latency,
                MAX(response_time_ms) as max_latency,
                COUNT(*) as message_count,
                SUM(CASE WHEN response_time_ms < 100 THEN 1 ELSE 0 END)::DECIMAL / COUNT(*) as emergency_sla
            FROM thread_coap_messages
            WHERE network_name = %s
            AND timestamp >= NOW() - INTERVAL '%s hours'
            GROUP BY message_type
        """, (network_name, hours))

        results = {}
        for row in self.storage.cur.fetchall():
            msg_type = row[0]
            results[msg_type] = {
                "avg_latency_ms": round(row[1], 2) if row[1] else None,
                "max_latency_ms": row[2],
                "message_count": row[3],
                "sla_compliance": round(row[4] * 100, 1) if row[4] else 0
            }

        return results

    def check_qos_violations(self, network_name: str) -> List[Dict]:
        """æ£€æŸ¥QoSè¿è§„"""
        violations = []

        performance = self.analyze_qos_performance(network_name)

        for msg_type, metrics in performance.items():
            traffic_class = self.classify_message("", msg_type)
            sla = self.class_definitions[traffic_class]["max_latency_ms"]

            if metrics["avg_latency_ms"] and metrics["avg_latency_ms"] > sla:
                violations.append({
                    "message_type": msg_type,
                    "traffic_class": traffic_class.name,
                    "sla_latency_ms": sla,
                    "actual_avg_latency_ms": metrics["avg_latency_ms"],
                    "violation_severity": "HIGH" if metrics["avg_latency_ms"] > sla * 2 else "MEDIUM"
                })

        return violations

    def generate_qos_report(self, network_name: str) -> Dict:
        """ç”ŸæˆQoSæŠ¥å‘Š"""
        performance = self.analyze_qos_performance(network_name)
        violations = self.check_qos_violations(network_name)

        return {
            "network_name": network_name,
            "generated_at": datetime.now().isoformat(),
            "performance_summary": performance,
            "sla_violations": violations,
            "overall_health": "DEGRADED" if violations else "HEALTHY",
            "recommendations": self._generate_qos_recommendations(violations)
        }

    def _generate_qos_recommendations(self, violations: List[Dict]) -> List[str]:
        """ç”ŸæˆQoSä¼˜åŒ–å»ºè®®"""
        recommendations = []

        if violations:
            high_violations = [v for v in violations if v["violation_severity"] == "HIGH"]
            if high_violations:
                recommendations.append(
                    f"{len(high_violations)} high-severity SLA violations detected. "
                    "Consider reducing network load or adding routers."
                )

            for v in violations[:3]:  # å‰3ä¸ªè¿è§„
                recommendations.append(
                    f"Review {v['message_type']} traffic: "
                    f"current avg {v['actual_avg_latency_ms']}ms, SLA is {v['sla_latency_ms']}ms"
                )
        else:
            recommendations.append("All traffic classes meeting SLA requirements")

        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
def demo_qos_management():
    storage = ThreadStorage("postgresql://user:pass@localhost/thread")
    qos_manager = ThreadQoSManager(storage)

    # å­˜å‚¨ä¸åŒä¼˜å…ˆçº§çš„æ¶ˆæ¯
    qos_manager.store_qos_message(
        "SmartHomeNet", "SENSOR_001", "ROUTER_001",
        "emergency_alert", 50, response_time=50
    )
    qos_manager.store_qos_message(
        "SmartHomeNet", "SENSOR_002", "ROUTER_001",
        "temperature", 20, response_time=2000
    )

    # åˆ†æQoSæ€§èƒ½
    performance = qos_manager.analyze_qos_performance("SmartHomeNet")
    print(f"QoS Performance: {performance}")

    # æ£€æŸ¥è¿è§„
    violations = qos_manager.check_qos_violations("SmartHomeNet")
    print(f"QoS Violations: {violations}")
```

---

## 15. æ¡ˆä¾‹15ï¼šThreadç½‘ç»œè‡ªåŠ¨åŒ–è¿ç»´å¹³å°

### 15.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
å¤§è§„æ¨¡Threadç½‘ç»œéœ€è¦è‡ªåŠ¨åŒ–è¿ç»´å¹³å°æ¥ç›‘æ§ç½‘ç»œå¥åº·ã€è‡ªåŠ¨å‘ç°æ•…éšœã€æ‰§è¡Œä¿®å¤æ“ä½œã€‚å¹³å°éœ€è¦é›†æˆç›‘æ§ã€å‘Šè­¦ã€è¯Šæ–­ã€ä¿®å¤çš„å®Œæ•´é—­ç¯ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦å®æ—¶ç›‘æ§å¤§é‡èŠ‚ç‚¹
- éœ€è¦è‡ªåŠ¨æ•…éšœè¯Šæ–­å’Œåˆ†ç±»
- éœ€è¦å®‰å…¨çš„è‡ªåŠ¨åŒ–ä¿®å¤
- éœ€è¦è¿ç»´å®¡è®¡å’ŒæŠ¥å‘Š

**è§£å†³æ–¹æ¡ˆ**ï¼š
æ„å»ºå®Œæ•´çš„è¿ç»´å¹³å°ï¼Œé›†æˆç›‘æ§é‡‡é›†ã€è§„åˆ™å¼•æ“ã€è‡ªåŠ¨ä¿®å¤å’ŒæŠ¥å‘Šç”Ÿæˆã€‚

### 15.2 Schemaå®šä¹‰

**è‡ªåŠ¨åŒ–è¿ç»´Schema**ï¼š

```json
{
  "ops_platform": {
    "network_name": "SmartHomeNet",
    "monitoring_config": {
      "check_interval_seconds": 60,
      "metrics_retention_days": 30,
      "alert_channels": ["email", "sms", "webhook"]
    },
    "automation_rules": [
      {
        "rule_id": "auto_rejoin_offline_node",
        "trigger": "node_offline > 5min",
        "condition": "last_seen < now - 5min AND node_type = 'EndDevice'",
        "action": "send_rejoin_command",
        "auto_execute": false
      }
    ],
    "maintenance_windows": [
      {
        "window_id": "MW001",
        "start": "02:00",
        "end": "04:00",
        "timezone": "Asia/Shanghai",
        "allowed_operations": ["firmware_update", "network_reconfigure"]
      }
    ]
  }
}
```

### 15.3 å®ç°ä»£ç 

```python
from datetime import datetime, time
import logging

logger = logging.getLogger(__name__)

class ThreadOpsPlatform:
    """Threadè¿ç»´å¹³å°"""

    def __init__(self, storage: ThreadStorage):
        self.storage = storage
        self.alert_rules = []

    def register_alert_rule(self, name: str, rule_type: str,
                           condition: Dict, severity: str) -> int:
        """æ³¨å†Œå‘Šè­¦è§„åˆ™"""
        rule_id = self.storage.create_alert_rule(name, rule_type, condition, severity)
        self.alert_rules.append({
            "id": rule_id,
            "name": name,
            "type": rule_type,
            "condition": condition,
            "severity": severity
        })
        return rule_id

    def run_health_check(self, network_name: str) -> Dict:
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        results = {
            "network_name": network_name,
            "check_time": datetime.now().isoformat(),
            "checks": [],
            "issues": [],
            "overall_status": "HEALTHY"
        }

        # 1. æ£€æŸ¥LeaderçŠ¶æ€
        leader_check = self._check_leader_status(network_name)
        results["checks"].append({"name": "leader_status", "result": leader_check})

        # 2. æ£€æŸ¥åˆ†åŒºæƒ…å†µ
        partition_check = self._check_partitions(network_name)
        results["checks"].append({"name": "partitions", "result": partition_check})

        # 3. æ£€æŸ¥èŠ‚ç‚¹ç¦»çº¿æƒ…å†µ
        offline_check = self._check_offline_nodes(network_name)
        results["checks"].append({"name": "offline_nodes", "result": offline_check})

        # 4. æ£€æŸ¥é“¾è·¯è´¨é‡
        link_check = self._check_link_quality(network_name)
        results["checks"].append({"name": "link_quality", "result": link_check})

        # æ±‡æ€»é—®é¢˜
        for check in results["checks"]:
            if check["result"].get("status") != "OK":
                results["issues"].append({
                    "check": check["name"],
                    "status": check["result"]["status"],
                    "details": check["result"].get("details", "")
                })

        # ç¡®å®šæ•´ä½“çŠ¶æ€
        critical_count = len([i for i in results["issues"] if "CRITICAL" in i["status"]])
        warning_count = len([i for i in results["issues"] if "WARNING" in i["status"]])

        if critical_count > 0:
            results["overall_status"] = "CRITICAL"
        elif warning_count > 0:
            results["overall_status"] = "WARNING"

        return results

    def _check_leader_status(self, network_name: str) -> Dict:
        """æ£€æŸ¥LeaderçŠ¶æ€"""
        self.storage.cur.execute("""
            SELECT leader_router_id, COUNT(*) as node_count
            FROM thread_nodes
            WHERE network_name = %s
            GROUP BY leader_router_id
        """, (network_name,))

        rows = self.storage.cur.fetchall()
        if len(rows) > 1:
            return {
                "status": "CRITICAL",
                "details": f"Network partitioned! {len(rows)} partitions detected"
            }

        return {"status": "OK", "leader_id": rows[0][0] if rows else None}

    def _check_partitions(self, network_name: str) -> Dict:
        """æ£€æŸ¥åˆ†åŒº"""
        self.storage.cur.execute("""
            SELECT COUNT(DISTINCT partition_id) FROM thread_partitions
            WHERE network_name = %s AND detected_at > NOW() - INTERVAL '1 hour'
        """, (network_name,))

        partition_count = self.storage.cur.fetchone()[0]

        if partition_count > 1:
            return {
                "status": "CRITICAL",
                "details": f"{partition_count} active partitions detected"
            }

        return {"status": "OK", "partition_count": partition_count}

    def _check_offline_nodes(self, network_name: str) -> Dict:
        """æ£€æŸ¥ç¦»çº¿èŠ‚ç‚¹"""
        self.storage.cur.execute("""
            SELECT COUNT(*) FROM thread_nodes
            WHERE network_name = %s
            AND updated_at < NOW() - INTERVAL '10 minutes'
        """, (network_name,))

        offline_count = self.storage.cur.fetchone()[0]

        if offline_count > 0:
            return {
                "status": "WARNING",
                "details": f"{offline_count} nodes appear offline"
            }

        return {"status": "OK", "offline_count": 0}

    def _check_link_quality(self, network_name: str) -> Dict:
        """æ£€æŸ¥é“¾è·¯è´¨é‡"""
        self.storage.cur.execute("""
            SELECT AVG(link_quality) FROM thread_nodes
            WHERE network_name = %s
        """, (network_name,))

        avg_lqi = self.storage.cur.fetchone()[0]

        if avg_lqi and avg_lqi < 150:
            return {
                "status": "WARNING",
                "details": f"Average LQI is low: {avg_lqi:.1f}"
            }

        return {"status": "OK", "avg_lqi": avg_lqi}

    def execute_remediation(self, network_name: str, issue: Dict) -> Dict:
        """æ‰§è¡Œä¿®å¤æ“ä½œ"""
        remediation_result = {
            "issue": issue,
            "action_taken": None,
            "success": False,
            "message": ""
        }

        if issue["check"] == "offline_nodes":
            # å°è¯•è®©ç¦»çº¿èŠ‚ç‚¹é‡æ–°åŠ å…¥
            remediation_result["action_taken"] = "attempt_rejoin"
            remediation_result["success"] = True
            remediation_result["message"] = "Rejoin command sent to offline nodes"

        elif issue["check"] == "partitions":
            # å°è¯•åˆå¹¶åˆ†åŒº
            remediation_result["action_taken"] = "attempt_merge"
            remediation_result["success"] = False
            remediation_result["message"] = "Partition merge requires manual intervention"

        return remediation_result

    def generate_ops_report(self, network_name: str, days: int = 7) -> Dict:
        """ç”Ÿæˆè¿ç»´æŠ¥å‘Š"""
        # è·å–å‘Šè­¦å†å²
        self.storage.cur.execute("""
            SELECT 
                severity,
                COUNT(*) as alert_count,
                COUNT(CASE WHEN acknowledged THEN 1 END) as acknowledged_count
            FROM thread_alert_history
            WHERE network_name = %s
            AND created_at >= NOW() - INTERVAL '%s days'
            GROUP BY severity
        """, (network_name, days))

        alert_summary = {}
        for row in self.storage.cur.fetchall():
            alert_summary[row[0]] = {
                "total": row[1],
                "acknowledged": row[2]
            }

        return {
            "network_name": network_name,
            "report_period_days": days,
            "generated_at": datetime.now().isoformat(),
            "alert_summary": alert_summary,
            "current_status": self.run_health_check(network_name)["overall_status"],
            "mttr_hours": 2.5,  # å¹³å‡ä¿®å¤æ—¶é—´
            "availability_percent": 99.5
        }

# ä½¿ç”¨ç¤ºä¾‹
def demo_ops_platform():
    storage = ThreadStorage("postgresql://user:pass@localhost/thread")
    ops = ThreadOpsPlatform(storage)

    # æ³¨å†Œå‘Šè­¦è§„åˆ™
    ops.register_alert_rule(
        "Node Offline Alert",
        "node_offline",
        {"condition": "last_seen > 10min", "threshold": 1},
        "HIGH"
    )

    # æ‰§è¡Œå¥åº·æ£€æŸ¥
    health = ops.run_health_check("SmartHomeNet")
    print(f"Health Check: {health['overall_status']}")
    for issue in health["issues"]:
        print(f"  Issue: {issue['check']} - {issue['status']}")

    # ç”Ÿæˆè¿ç»´æŠ¥å‘Š
    report = ops.generate_ops_report("SmartHomeNet", days=7)
    print(f"Ops Report: {report}")
```

---

## 16. æ¡ˆä¾‹16ï¼šThreadä¸Matterè¾¹ç•Œè·¯ç”±å™¨é›†æˆ

### 16.1 åœºæ™¯æè¿°

**ä¸šåŠ¡èƒŒæ™¯**ï¼š
Threadè¾¹ç•Œè·¯ç”±å™¨(Border Router)è¿æ¥Threadç½‘ç»œä¸å¤–éƒ¨ç½‘ç»œï¼ˆå¦‚WiFi/Ethernetï¼‰ï¼Œæ˜¯Matterè®¾å¤‡æ¥å…¥IPç½‘ç»œçš„å…³é”®ç»„ä»¶ã€‚éœ€è¦ç®¡ç†è¾¹ç•Œè·¯ç”±å™¨ã€å¤„ç†å‰ç¼€å§”æ´¾ã€å®ç°NAT64è½¬æ¢ã€‚

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š

- éœ€è¦ç®¡ç†å¤šä¸ªè¾¹ç•Œè·¯ç”±å™¨
- éœ€è¦å¤„ç†IPv6å‰ç¼€å§”æ´¾
- éœ€è¦å®ç°NAT64è½¬æ¢
- éœ€è¦ç¡®ä¿å¤–éƒ¨è·¯ç”±é€šå‘Šçš„æ­£ç¡®æ€§

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨Thread Border Routerå’ŒBackbone RouteråŠŸèƒ½ï¼Œç»“åˆPostgreSQLå­˜å‚¨è¾¹ç•Œè·¯ç”±å™¨çŠ¶æ€å’Œå¤–éƒ¨è·¯ç”±ä¿¡æ¯ã€‚

### 16.2 Schemaå®šä¹‰

**è¾¹ç•Œè·¯ç”±å™¨é›†æˆSchema**ï¼š

```json
{
  "border_router_config": {
    "network_name": "SmartHomeNet",
    "border_routers": [
      {
        "router_id": "BR_001",
        "rloc16": "0xA800",
        "ip_addresses": {
          "thread": "fd11:22::ff:fe00:a800",
          "infrastructure": "192.168.1.100"
        },
        "prefixes": [
          {
            "prefix": "fd11:22::",
            "length": 64,
            "preferred": true,
            "slaac": true,
            "dhcp": false
          }
        ],
        "routes": [
          {
            "prefix": "0.0.0.0/0",
            "preference": "high",
            "nat64": true
          }
        ],
        "last_seen": "2025-02-14T10:30:00Z"
      }
    ]
  }
}
```

### 16.3 å®ç°ä»£ç 

```python
class ThreadBorderRouterManager:
    """Threadè¾¹ç•Œè·¯ç”±å™¨ç®¡ç†å™¨"""

    def __init__(self, storage: ThreadStorage):
        self.storage = storage

    def register_border_router(self, network_name: str, router_id: str,
                              rloc16: int, prefix: str, prefix_len: int,
                              is_preferred: bool = True) -> int:
        """æ³¨å†Œè¾¹ç•Œè·¯ç”±å™¨"""
        self.storage.cur.execute("""
            INSERT INTO thread_border_routers (
                network_name, border_router_id, rloc16, prefix,
                prefix_length, preferred, last_seen
            ) VALUES (%s, %s, %s, %s, %s, %s, CURRENT_TIMESTAMP)
            ON CONFLICT (network_name, border_router_id) DO UPDATE SET
                rloc16 = EXCLUDED.rloc16,
                prefix = EXCLUDED.prefix,
                prefix_length = EXCLUDED.prefix_length,
                preferred = EXCLUDED.preferred,
                last_seen = CURRENT_TIMESTAMP
            RETURNING id
        """, (network_name, router_id, rloc16, prefix, prefix_len, is_preferred))
        br_id = self.storage.cur.fetchone()[0]
        self.storage.conn.commit()
        return br_id

    def add_external_route(self, network_name: str, prefix: str,
                          prefix_len: int, border_router_id: str,
                          preference: int = 0, nat64: bool = False) -> int:
        """æ·»åŠ å¤–éƒ¨è·¯ç”±"""
        self.storage.cur.execute("""
            INSERT INTO thread_external_routes (
                network_name, prefix, prefix_length,
                border_router_id, preference, nat64
            ) VALUES (%s, %s, %s, %s, %s, %s)
            RETURNING id
        """, (network_name, prefix, prefix_len, border_router_id, preference, nat64))
        route_id = self.storage.cur.fetchone()[0]
        self.storage.conn.commit()
        return route_id

    def get_active_border_routers(self, network_name: str) -> List[Dict]:
        """è·å–æ´»è·ƒè¾¹ç•Œè·¯ç”±å™¨"""
        self.storage.cur.execute("""
            SELECT border_router_id, rloc16, prefix, prefix_length,
                   preferred, slaac, dhcp, last_seen
            FROM thread_border_routers
            WHERE network_name = %s
            AND last_seen > NOW() - INTERVAL '5 minutes'
            ORDER BY preferred DESC, last_seen DESC
        """, (network_name,))
        return [
            {
                "router_id": row[0],
                "rloc16": row[1],
                "prefix": row[2],
                "prefix_length": row[3],
                "preferred": row[4],
                "slaac": row[5],
                "dhcp": row[6],
                "last_seen": row[7]
            }
            for row in self.storage.cur.fetchall()
        ]

    def check_nat64_availability(self, network_name: str) -> Dict:
        """æ£€æŸ¥NAT64å¯ç”¨æ€§"""
        self.storage.cur.execute("""
            SELECT border_router_id, prefix
            FROM thread_external_routes
            WHERE network_name = %s
            AND nat64 = TRUE
            AND preference >= 0
        """, (network_name,))

        rows = self.storage.cur.fetchall()
        if rows:
            return {
                "available": True,
                "border_routers": [row[0] for row in rows],
                "prefixes": [row[1] for row in rows]
            }

        return {"available": False}

    def configure_ipv6_prefix(self, network_name: str, prefix: str,
                             prefix_len: int, border_router_id: str) -> bool:
        """é…ç½®IPv6å‰ç¼€"""
        # å®é™…å®ç°ä¸­éœ€è¦é€šè¿‡Threadç®¡ç†æ¥å£é…ç½®
        logger.info(f"Configuring IPv6 prefix {prefix}/{prefix_len} on {border_router_id}")
        
        # å­˜å‚¨é…ç½®
        self.register_border_router(
            network_name, border_router_id,
            rloc16=0,  # å®é™…åº”ä»è®¾å¤‡è·å–
            prefix=prefix,
            prefix_len=prefix_len
        )
        return True

    def monitor_border_router_health(self, network_name: str) -> List[Dict]:
        """ç›‘æ§è¾¹ç•Œè·¯ç”±å™¨å¥åº·çŠ¶æ€"""
        brs = self.get_active_border_routers(network_name)
        results = []

        for br in brs:
            # æ£€æŸ¥æœ€åæ´»è·ƒæ—¶é—´
            last_seen = br.get("last_seen")
            if last_seen:
                age_seconds = (datetime.now() - last_seen).total_seconds()
                status = "healthy" if age_seconds < 300 else "stale"
            else:
                status = "unknown"

            results.append({
                "router_id": br["router_id"],
                "status": status,
                "prefix": f"{br['prefix']}/{br['prefix_length']}",
                "is_preferred": br["preferred"]
            })

        return results

    def handle_prefix_change(self, network_name: str, old_prefix: str, new_prefix: str):
        """å¤„ç†å‰ç¼€å˜æ›´"""
        # é€šçŸ¥æ‰€æœ‰èŠ‚ç‚¹å‰ç¼€å˜æ›´
        logger.info(f"Prefix change in {network_name}: {old_prefix} -> {new_prefix}")

        # è®°å½•äº‹ä»¶
        self.storage.store_event(
            network_name=network_name,
            event_type="prefix_change",
            event_data={
                "old_prefix": old_prefix,
                "new_prefix": new_prefix
            }
        )

# ä½¿ç”¨ç¤ºä¾‹
def demo_border_router():
    storage = ThreadStorage("postgresql://user:pass@localhost/thread")
    br_manager = ThreadBorderRouterManager(storage)

    # æ³¨å†Œè¾¹ç•Œè·¯ç”±å™¨
    br_manager.register_border_router(
        network_name="SmartHomeNet",
        router_id="BR_001",
        rloc16=0xA800,
        prefix="fd11:22::",
        prefix_len=64,
        is_preferred=True
    )

    # æ·»åŠ å¤–éƒ¨è·¯ç”±ï¼ˆNAT64ï¼‰
    br_manager.add_external_route(
        network_name="SmartHomeNet",
        prefix="64:ff9b::",
        prefix_len=96,
        border_router_id="BR_001",
        nat64=True
    )

    # æ£€æŸ¥NAT64å¯ç”¨æ€§
    nat64 = br_manager.check_nat64_availability("SmartHomeNet")
    print(f"NAT64 Available: {nat64['available']}")

    # ç›‘æ§è¾¹ç•Œè·¯ç”±å™¨å¥åº·
    health = br_manager.monitor_border_router_health("SmartHomeNet")
    for br in health:
        print(f"BR {br['router_id']}: {br['status']}")
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `04_Transformation.md` - è½¬æ¢ä½“ç³»

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-02-14ï¼ˆæ–°å¢5ä¸ªThreadé«˜çº§æ¡ˆä¾‹ï¼‰
