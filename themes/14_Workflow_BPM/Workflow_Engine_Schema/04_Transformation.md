# Workflow Engine Schemaè½¬æ¢ä½“ç³»

## ğŸ“‘ ç›®å½•

- [Workflow Engine Schemaè½¬æ¢ä½“ç³»](#workflow-engine-schemaè½¬æ¢ä½“ç³»)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. è½¬æ¢ä½“ç³»æ¦‚è¿°](#1-è½¬æ¢ä½“ç³»æ¦‚è¿°)
    - [1.1 è½¬æ¢ç›®æ ‡](#11-è½¬æ¢ç›®æ ‡)
  - [2. BPMNåˆ°å·¥ä½œæµå¼•æ“è½¬æ¢](#2-bpmnåˆ°å·¥ä½œæµå¼•æ“è½¬æ¢)
  - [3. å·¥ä½œæµå¼•æ“åˆ°XPDLè½¬æ¢](#3-å·¥ä½œæµå¼•æ“åˆ°xpdlè½¬æ¢)
  - [4. è½¬æ¢å·¥å…·](#4-è½¬æ¢å·¥å…·)
  - [5. è½¬æ¢éªŒè¯](#5-è½¬æ¢éªŒè¯)
  - [6. Workflow Engineæ•°æ®å­˜å‚¨ä¸åˆ†æ](#6-workflow-engineæ•°æ®å­˜å‚¨ä¸åˆ†æ)
    - [6.1 PostgreSQL Workflow Engineæ•°æ®å­˜å‚¨](#61-postgresql-workflow-engineæ•°æ®å­˜å‚¨)
    - [6.2 Workflow Engineæ•°æ®åˆ†ææŸ¥è¯¢](#62-workflow-engineæ•°æ®åˆ†ææŸ¥è¯¢)

---

## 1. è½¬æ¢ä½“ç³»æ¦‚è¿°

Workflow Engine Schemaè½¬æ¢ä½“ç³»æ”¯æŒBPMNåˆ°
å·¥ä½œæµå¼•æ“è½¬æ¢ã€å·¥ä½œæµå¼•æ“åˆ°XPDLè½¬æ¢ï¼Œä»¥åŠ
å·¥ä½œæµå¼•æ“æ•°æ®å­˜å‚¨ã€‚

### 1.1 è½¬æ¢ç›®æ ‡

1. **BPMNåˆ°å·¥ä½œæµå¼•æ“è½¬æ¢**ï¼šBPMNæµç¨‹å®šä¹‰åˆ°å·¥ä½œæµå¼•æ“æ ¼å¼
2. **å·¥ä½œæµå¼•æ“åˆ°XPDLè½¬æ¢**ï¼šå·¥ä½œæµå¼•æ“å®šä¹‰åˆ°XPDLæ ¼å¼
3. **æµç¨‹åˆ°æ•°æ®åº“è½¬æ¢**ï¼šå·¥ä½œæµå¼•æ“æ•°æ®åˆ°PostgreSQLå­˜å‚¨

---

## 2. BPMNåˆ°å·¥ä½œæµå¼•æ“è½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- BPMNæµç¨‹ â†’ å·¥ä½œæµå¼•æ“æµç¨‹å®šä¹‰
- BPMNä»»åŠ¡ â†’ å·¥ä½œæµå¼•æ“ä»»åŠ¡å®šä¹‰
- BPMNç½‘å…³ â†’ å·¥ä½œæµå¼•æ“ç½‘å…³å®šä¹‰

**è½¬æ¢ç¤ºä¾‹**ï¼š

```python
def convert_bpmn_to_workflow_engine(bpmn_process: BPMNProcess) -> WorkflowDefinition:
    """å°†BPMNæµç¨‹è½¬æ¢ä¸ºå·¥ä½œæµå¼•æ“å®šä¹‰"""
    workflow_def = WorkflowDefinition()

    # è½¬æ¢æµç¨‹åŸºæœ¬ä¿¡æ¯
    workflow_def.process_definition.process_id = bpmn_process.id
    workflow_def.process_definition.process_name = bpmn_process.name
    workflow_def.process_definition.process_key = bpmn_process.id
    workflow_def.process_definition.version = 1

    # è½¬æ¢æµç¨‹å…ƒç´ 
    for element in bpmn_process.elements:
        process_element = ProcessElement()
        process_element.element_id = element.id
        process_element.element_name = element.name
        process_element.element_type = convert_element_type(element.type)
        workflow_def.process_elements.append(process_element)

    # è½¬æ¢æµç¨‹å˜é‡
    for var_name, var_def in bpmn_process.variables.items():
        process_var = ProcessVariable()
        process_var.variable_name = var_name
        process_var.variable_type = var_def.type
        process_var.default_value = var_def.default_value
        workflow_def.process_variables.append(process_var)

    return workflow_def
```

---

## 3. å·¥ä½œæµå¼•æ“åˆ°XPDLè½¬æ¢

**è½¬æ¢è§„åˆ™**ï¼š

- å·¥ä½œæµå¼•æ“æµç¨‹å®šä¹‰ â†’ XPDLå·¥ä½œæµ
- å·¥ä½œæµå¼•æ“ä»»åŠ¡ â†’ XPDLæ´»åŠ¨
- å·¥ä½œæµå¼•æ“ç½‘å…³ â†’ XPDLè·¯ç”±

**è½¬æ¢ç¤ºä¾‹**ï¼š

```python
def convert_workflow_engine_to_xpdl(workflow_def: WorkflowDefinition) -> XPDLWorkflow:
    """å°†å·¥ä½œæµå¼•æ“å®šä¹‰è½¬æ¢ä¸ºXPDLå·¥ä½œæµ"""
    xpdl = XPDLWorkflow()

    # è½¬æ¢å·¥ä½œæµåŸºæœ¬ä¿¡æ¯
    xpdl.workflow_process.id = workflow_def.process_definition.process_id
    xpdl.workflow_process.name = workflow_def.process_definition.process_name

    # è½¬æ¢æ´»åŠ¨
    for element in workflow_def.process_elements:
        if element.element_type in ['UserTask', 'ServiceTask']:
            activity = XPDLActivity()
            activity.id = element.element_id
            activity.name = element.element_name
            activity.activity_type = convert_activity_type(element.element_type)
            xpdl.workflow_process.activities.append(activity)

    # è½¬æ¢è½¬ç§»
    for element in workflow_def.process_elements:
        if element.element_type == 'SequenceFlow':
            transition = XPDLTransition()
            transition.id = element.element_id
            transition.from_activity = element.source_ref
            transition.to_activity = element.target_ref
            xpdl.workflow_process.transitions.append(transition)

    return xpdl
```

---

## 4. è½¬æ¢å·¥å…·

- **Activiti Modeler**ï¼šActivitiæµç¨‹å»ºæ¨¡å’Œè½¬æ¢å·¥å…·
- **Camunda Modeler**ï¼šCamundaæµç¨‹å»ºæ¨¡å’Œè½¬æ¢å·¥å…·
- **jBPM Designer**ï¼šjBPMæµç¨‹è®¾è®¡å’Œè½¬æ¢å·¥å…·
- **è‡ªå®šä¹‰è½¬æ¢å™¨**ï¼šåŸºäºSchemaçš„è½¬æ¢å™¨

---

## 5. è½¬æ¢éªŒè¯

éªŒè¯è½¬æ¢çš„æµç¨‹å®Œæ•´æ€§ã€è¡Œä¸ºç­‰ä»·æ€§å’Œå¯æ‰§è¡Œæ€§ã€‚

---

## 6. Workflow Engineæ•°æ®å­˜å‚¨ä¸åˆ†æ

### 6.1 PostgreSQL Workflow Engineæ•°æ®å­˜å‚¨

**Workflow Engineæ•°æ®å­˜å‚¨æ–¹æ¡ˆ**ï¼š

```python
import psycopg2
import json
from typing import Dict, List, Optional
from datetime import datetime
from decimal import Decimal

class WorkflowEngineStorage:
    """Workflow Engineæ•°æ®å­˜å‚¨ç³»ç»Ÿ"""

    def __init__(self, connection_string: str):
        self.conn = psycopg2.connect(connection_string)
        self.cur = self.conn.cursor()
        self._create_tables()

    def _create_tables(self):
        """åˆ›å»ºWorkflow Engineæ•°æ®è¡¨"""
        # æµç¨‹å®šä¹‰è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS workflow_process_definitions (
                id VARCHAR(64) PRIMARY KEY,
                process_id VARCHAR(255) NOT NULL,
                process_name VARCHAR(255) NOT NULL,
                process_key VARCHAR(255) NOT NULL,
                version INTEGER NOT NULL,
                category VARCHAR(255),
                deployment_id VARCHAR(64) NOT NULL,
                resource_name VARCHAR(4000),
                diagram_resource_name VARCHAR(4000),
                is_suspended BOOLEAN DEFAULT FALSE,
                tenant_id VARCHAR(255),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(process_key, version)
            )
        """)

        # æµç¨‹å®ä¾‹è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS workflow_process_instances (
                id VARCHAR(64) PRIMARY KEY,
                process_definition_id VARCHAR(64) NOT NULL,
                process_definition_key VARCHAR(255) NOT NULL,
                business_key VARCHAR(255),
                parent_instance_id VARCHAR(64),
                root_process_instance_id VARCHAR(64),
                status VARCHAR(50) NOT NULL,
                start_time TIMESTAMP NOT NULL,
                end_time TIMESTAMP,
                start_user_id VARCHAR(255),
                start_activity_id VARCHAR(255),
                delete_reason VARCHAR(4000),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # æ‰§è¡ŒçŠ¶æ€è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS workflow_execution_states (
                id VARCHAR(64) PRIMARY KEY,
                process_instance_id VARCHAR(64) NOT NULL,
                parent_execution_id VARCHAR(64),
                activity_id VARCHAR(255),
                activity_name VARCHAR(255),
                is_active BOOLEAN NOT NULL,
                is_concurrent BOOLEAN DEFAULT FALSE,
                is_scope BOOLEAN DEFAULT FALSE,
                suspension_state INTEGER DEFAULT 1,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # ä»»åŠ¡å®ä¾‹è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS workflow_task_instances (
                id VARCHAR(64) PRIMARY KEY,
                process_instance_id VARCHAR(64) NOT NULL,
                execution_id VARCHAR(64),
                process_definition_id VARCHAR(64),
                process_definition_key VARCHAR(255),
                task_definition_key VARCHAR(255),
                name VARCHAR(255),
                assignee VARCHAR(255),
                owner VARCHAR(255),
                delegation_state VARCHAR(20),
                priority INTEGER DEFAULT 50,
                create_time TIMESTAMP NOT NULL,
                due_date TIMESTAMP,
                category VARCHAR(255),
                suspension_state INTEGER DEFAULT 1,
                tenant_id VARCHAR(255),
                form_key VARCHAR(255),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # æ‰§è¡Œå†å²è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS workflow_execution_history (
                id VARCHAR(64) PRIMARY KEY,
                process_instance_id VARCHAR(64) NOT NULL,
                execution_id VARCHAR(64) NOT NULL,
                activity_instance_id VARCHAR(64),
                activity_id VARCHAR(255),
                activity_name VARCHAR(255),
                activity_type VARCHAR(50) NOT NULL,
                task_id VARCHAR(64),
                assignee VARCHAR(255),
                start_time TIMESTAMP NOT NULL,
                end_time TIMESTAMP,
                duration_ms BIGINT,
                delete_reason VARCHAR(4000),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # æ‰§è¡Œå˜é‡è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS workflow_execution_variables (
                id VARCHAR(64) PRIMARY KEY,
                process_instance_id VARCHAR(64) NOT NULL,
                execution_id VARCHAR(64),
                task_id VARCHAR(64),
                variable_name VARCHAR(255) NOT NULL,
                variable_type VARCHAR(255),
                variable_value TEXT,
                byte_array_id VARCHAR(64),
                double_value NUMERIC(18, 2),
                long_value BIGINT,
                text_value TEXT,
                text_value2 TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(process_instance_id, execution_id, task_id, variable_name)
            )
        """)

        # ä»»åŠ¡è°ƒåº¦è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS workflow_task_schedules (
                id SERIAL PRIMARY KEY,
                task_definition_key VARCHAR(255) NOT NULL,
                assignment_type VARCHAR(50) NOT NULL,
                assignee VARCHAR(255),
                candidate_users JSONB,
                candidate_groups JSONB,
                priority INTEGER DEFAULT 50,
                scheduling_strategy VARCHAR(50),
                max_concurrent_tasks INTEGER DEFAULT 10,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # Workflow Engineç»Ÿè®¡è¡¨
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS workflow_engine_statistics (
                id SERIAL PRIMARY KEY,
                statistic_type VARCHAR(50) NOT NULL,
                process_definition_key VARCHAR(255),
                time_window TIMESTAMP NOT NULL,
                statistics JSONB NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(statistic_type, process_definition_key, time_window)
            )
        """)

        # åˆ›å»ºç´¢å¼•
        self.cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_process_instances_status
            ON workflow_process_instances(status, start_time DESC)
        """)
        self.cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_task_instances_assignee
            ON workflow_task_instances(assignee, create_time DESC)
        """)
        self.cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_execution_history_instance
            ON workflow_execution_history(process_instance_id, start_time DESC)
        """)
        self.cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_execution_variables_instance
            ON workflow_execution_variables(process_instance_id, variable_name)
        """)

        self.conn.commit()

    def store_process_definition(self, process_id: str, process_name: str,
                                 process_key: str, version: int,
                                 deployment_id: str, resource_name: str,
                                 diagram_resource_name: str = None,
                                 category: str = None, tenant_id: str = None):
        """å­˜å‚¨æµç¨‹å®šä¹‰"""
        self.cur.execute("""
            INSERT INTO workflow_process_definitions
            (id, process_id, process_name, process_key, version, category,
             deployment_id, resource_name, diagram_resource_name, tenant_id)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (process_key, version) DO UPDATE
            SET process_name = EXCLUDED.process_name,
                deployment_id = EXCLUDED.deployment_id,
                resource_name = EXCLUDED.resource_name,
                diagram_resource_name = EXCLUDED.diagram_resource_name,
                updated_at = CURRENT_TIMESTAMP
        """, (process_id, process_id, process_name, process_key, version,
              category, deployment_id, resource_name, diagram_resource_name, tenant_id))
        self.conn.commit()

    def calculate_process_statistics(self, process_definition_key: str,
                                    time_window: datetime):
        """è®¡ç®—æµç¨‹ç»Ÿè®¡ä¿¡æ¯"""
        self.cur.execute("""
            SELECT
                COUNT(*) as total_instances,
                COUNT(CASE WHEN status = 'Completed' THEN 1 END) as completed,
                COUNT(CASE WHEN status = 'Active' THEN 1 END) as active,
                COUNT(CASE WHEN status = 'Suspended' THEN 1 END) as suspended,
                AVG(EXTRACT(EPOCH FROM (end_time - start_time))) as avg_duration,
                MIN(EXTRACT(EPOCH FROM (end_time - start_time))) as min_duration,
                MAX(EXTRACT(EPOCH FROM (end_time - start_time))) as max_duration
            FROM workflow_process_instances
            WHERE process_definition_key = %s AND start_time >= %s
        """, (process_definition_key, time_window))

        stats = dict(zip([desc[0] for desc in self.cur.description],
                         self.cur.fetchone()))

        # ä»»åŠ¡ç»Ÿè®¡
        self.cur.execute("""
            SELECT
                task_definition_key,
                COUNT(*) as task_count,
                AVG(EXTRACT(EPOCH FROM (updated_at - create_time))) as avg_processing_time,
                COUNT(CASE WHEN assignee IS NOT NULL THEN 1 END) as assigned_count
            FROM workflow_task_instances
            WHERE process_definition_key = %s AND create_time >= %s
            GROUP BY task_definition_key
        """, (process_definition_key, time_window))

        task_stats = []
        for row in self.cur.fetchall():
            task_stats.append({
                'task_definition_key': row[0],
                'task_count': row[1],
                'avg_processing_time': row[2],
                'assigned_count': row[3]
            })

        stats['task_statistics'] = task_stats

        # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        self.cur.execute("""
            INSERT INTO workflow_engine_statistics
            (statistic_type, process_definition_key, time_window, statistics)
            VALUES (%s, %s, %s, %s::jsonb)
            ON CONFLICT (statistic_type, process_definition_key, time_window)
            DO UPDATE SET statistics = EXCLUDED.statistics
        """, ('process_performance', process_definition_key, time_window, json.dumps(stats)))
        self.conn.commit()

        return stats
```

### 6.2 Workflow Engineæ•°æ®åˆ†ææŸ¥è¯¢

**æŸ¥è¯¢ç¤ºä¾‹**ï¼š

```python
# æŸ¥è¯¢æµç¨‹å®ä¾‹
storage.cur.execute("""
    SELECT id, process_definition_key, status, start_time, end_time
    FROM workflow_process_instances
    WHERE process_definition_key = %s AND start_time >= %s
    ORDER BY start_time DESC
""", (process_definition_key, start_time))

# æŸ¥è¯¢ä»»åŠ¡åˆ†é…ç»Ÿè®¡
storage.cur.execute("""
    SELECT assignee, COUNT(*) as task_count,
           AVG(EXTRACT(EPOCH FROM (updated_at - create_time))) as avg_time
    FROM workflow_task_instances
    WHERE create_time >= %s AND assignee IS NOT NULL
    GROUP BY assignee
    ORDER BY task_count DESC
""", (start_time,))
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š

- `01_Overview.md` - æ¦‚è¿°
- `02_Formal_Definition.md` - å½¢å¼åŒ–å®šä¹‰
- `03_Standards.md` - æ ‡å‡†å¯¹æ ‡
- `05_Case_Studies.md` - å®è·µæ¡ˆä¾‹

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
