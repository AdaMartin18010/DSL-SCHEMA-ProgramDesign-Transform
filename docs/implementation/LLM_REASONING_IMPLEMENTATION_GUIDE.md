# LLMæ¨ç†å¼•æ“å®ç°æŒ‡å—

## ğŸ“‘ ç›®å½•

- [LLMæ¨ç†å¼•æ“å®ç°æŒ‡å—](#llmæ¨ç†å¼•æ“å®ç°æŒ‡å—)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. å®ç°æ¦‚è¿°](#1-å®ç°æ¦‚è¿°)
    - [1.1 å®ç°ç›®æ ‡](#11-å®ç°ç›®æ ‡)
    - [1.2 å®ç°æ¶æ„](#12-å®ç°æ¶æ„)
  - [2. æŠ€æœ¯æ ˆé€‰æ‹©](#2-æŠ€æœ¯æ ˆé€‰æ‹©)
    - [2.1 LLMé€‰æ‹©](#21-llmé€‰æ‹©)
    - [2.2 æ¡†æ¶](#22-æ¡†æ¶)
  - [3. LLMé›†æˆå®ç°](#3-llmé›†æˆå®ç°)
    - [3.1 LLMæ¥å£æŠ½è±¡](#31-llmæ¥å£æŠ½è±¡)
  - [4. çŸ¥è¯†å›¾è°±åµŒå…¥å®ç°](#4-çŸ¥è¯†å›¾è°±åµŒå…¥å®ç°)
    - [4.1 çŸ¥è¯†å›¾è°±åµŒå…¥](#41-çŸ¥è¯†å›¾è°±åµŒå…¥)
  - [5. æ¨ç†é“¾æ„å»ºå®ç°](#5-æ¨ç†é“¾æ„å»ºå®ç°)
    - [5.1 æ¨ç†é“¾æ„å»ºå™¨](#51-æ¨ç†é“¾æ„å»ºå™¨)
  - [6. ç»“æœéªŒè¯å®ç°](#6-ç»“æœéªŒè¯å®ç°)
    - [6.1 ç»“æœéªŒè¯å™¨](#61-ç»“æœéªŒè¯å™¨)
  - [7. APIæ¥å£å®ç°](#7-apiæ¥å£å®ç°)
    - [7.1 REST API](#71-rest-api)
  - [8. æµ‹è¯•ä¸éªŒè¯](#8-æµ‹è¯•ä¸éªŒè¯)
    - [8.1 å•å…ƒæµ‹è¯•](#81-å•å…ƒæµ‹è¯•)
  - [9. ç›¸å…³æ–‡æ¡£](#9-ç›¸å…³æ–‡æ¡£)
    - [æ¶æ„å’Œè®¾è®¡æ¨¡å¼å‚è€ƒ](#æ¶æ„å’Œè®¾è®¡æ¨¡å¼å‚è€ƒ)
    - [å…¶ä»–å®ç°æŒ‡å—](#å…¶ä»–å®ç°æŒ‡å—)

---

## 1. å®ç°æ¦‚è¿°

### 1.1 å®ç°ç›®æ ‡

- âœ… LLMé€‰æ‹©ä¸é›†æˆï¼ˆGPT-4ã€Claudeç­‰ï¼‰
- âœ… çŸ¥è¯†å›¾è°±åµŒå…¥å®ç°
- âœ… æ¨ç†é“¾æ„å»ºå®ç°
- âœ… ç»“æœéªŒè¯å®ç°

### 1.2 å®ç°æ¶æ„

```
LLMæ¨ç†å¼•æ“ç³»ç»Ÿ
â”œâ”€â”€ LLMå±‚
â”‚   â”œâ”€â”€ OpenAI GPT-4
â”‚   â”œâ”€â”€ Anthropic Claude
â”‚   â””â”€â”€ å¼€æºLLMï¼ˆLlama 2ï¼‰
â”œâ”€â”€ çŸ¥è¯†å›¾è°±å±‚
â”‚   â”œâ”€â”€ å®ä½“åµŒå…¥
â”‚   â”œâ”€â”€ å…³ç³»åµŒå…¥
â”‚   â””â”€â”€ å­å›¾æå–
â”œâ”€â”€ æ¨ç†å±‚
â”‚   â”œâ”€â”€ æ¨ç†é“¾æ„å»º
â”‚   â”œâ”€â”€ Promptå·¥ç¨‹
â”‚   â””â”€â”€ ç»“æœéªŒè¯
â””â”€â”€ APIå±‚
    â””â”€â”€ REST API
```

---

## 2. æŠ€æœ¯æ ˆé€‰æ‹©

### 2.1 LLMé€‰æ‹©

- **OpenAI GPT-4**ï¼šå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼ŒAPIç¨³å®š
- **Anthropic Claude 3**ï¼šé•¿ä¸Šä¸‹æ–‡æ”¯æŒï¼Œå®‰å…¨æ€§é«˜
- **å¼€æºLLM**ï¼šLlama 2ã€Mistralï¼ˆæœ¬åœ°éƒ¨ç½²ï¼‰

### 2.2 æ¡†æ¶

- **LangChain**ï¼šLLMåº”ç”¨æ¡†æ¶
- **OpenAI Python SDK**ï¼šOpenAI API
- **Anthropic Python SDK**ï¼šClaude API
- **FastAPI**ï¼šREST APIæ¡†æ¶

---

## 3. LLMé›†æˆå®ç°

### 3.1 LLMæ¥å£æŠ½è±¡

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any
from pydantic import BaseModel

class ReasoningResult(BaseModel):
    answer: str
    reasoning_steps: List[Dict[str, Any]]
    confidence: float
    sources: List[str]

class LLMInterface(ABC):
    """LLMæ¥å£æŠ½è±¡ç±»"""

    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        pass

    @abstractmethod
    def embed(self, text: str) -> List[float]:
        """ç”ŸæˆåµŒå…¥å‘é‡"""
        pass

    @abstractmethod
    def reason(self, query: str, context: Dict[str, Any]) -> ReasoningResult:
        """æ‰§è¡Œæ¨ç†"""
        pass

class OpenAILLM(LLMInterface):
    """OpenAI GPT-4å®ç°"""

    def __init__(self, api_key: str, model: str = "gpt-4"):
        import openai
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model

    def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            **kwargs
        )
        return response.choices[0].message.content

    def embed(self, text: str) -> List[float]:
        """ç”ŸæˆåµŒå…¥å‘é‡"""
        response = self.client.embeddings.create(
            model="text-embedding-3-large",
            input=text
        )
        return response.data[0].embedding

    def reason(self, query: str, context: Dict[str, Any]) -> ReasoningResult:
        """æ‰§è¡Œæ¨ç†"""
        # æ„å»ºæ¨ç†Prompt
        prompt = self.build_reasoning_prompt(query, context)

        # è°ƒç”¨LLM
        response = self.generate(prompt, temperature=0.7)

        # è§£ææ¨ç†ç»“æœ
        reasoning_steps = self.parse_reasoning_steps(response)

        return ReasoningResult(
            answer=self.extract_answer(response),
            reasoning_steps=reasoning_steps,
            confidence=self.compute_confidence(response),
            sources=context.get('sources', [])
        )

    def build_reasoning_prompt(self, query: str, context: Dict[str, Any]) -> str:
        """æ„å»ºæ¨ç†Prompt"""
        prompt = f"""You are a knowledge reasoning assistant.
Given the following knowledge graph context, answer the query step by step.

Knowledge Graph Context:
{self.format_kg_context(context)}

Query: {query}

Please provide:
1. Reasoning steps
2. Final answer
3. Confidence level (0-1)
"""
        return prompt

    def format_kg_context(self, context: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡"""
        entities = context.get('entities', [])
        relations = context.get('relations', [])

        formatted = "Entities:\n"
        for entity in entities:
            formatted += f"- {entity['id']}: {entity['properties']}\n"

        formatted += "\nRelations:\n"
        for relation in relations:
            formatted += f"- {relation['source']} --[{relation['type']}]--> {relation['target']}\n"

        return formatted
```

---

## 4. çŸ¥è¯†å›¾è°±åµŒå…¥å®ç°

### 4.1 çŸ¥è¯†å›¾è°±åµŒå…¥

```python
import numpy as np
from typing import List, Dict, Any

class KGEmbedding:
    """çŸ¥è¯†å›¾è°±åµŒå…¥"""

    def __init__(self, llm: LLMInterface):
        self.llm = llm

    def embed_entity(self, entity: Dict[str, Any]) -> np.ndarray:
        """åµŒå…¥å®ä½“"""
        # æ„å»ºå®ä½“æè¿°
        description = self.build_entity_description(entity)

        # ç”ŸæˆåµŒå…¥å‘é‡
        embedding = self.llm.embed(description)

        return np.array(embedding)

    def embed_relation(self, relation: Dict[str, Any]) -> np.ndarray:
        """åµŒå…¥å…³ç³»"""
        # æ„å»ºå…³ç³»æè¿°
        description = f"{relation['source']} {relation['type']} {relation['target']}"

        # ç”ŸæˆåµŒå…¥å‘é‡
        embedding = self.llm.embed(description)

        return np.array(embedding)

    def embed_subgraph(self, entities: List[Dict],
                      relations: List[Dict]) -> np.ndarray:
        """åµŒå…¥å­å›¾"""
        # æ„å»ºå­å›¾æè¿°
        description = self.build_subgraph_description(entities, relations)

        # ç”ŸæˆåµŒå…¥å‘é‡
        embedding = self.llm.embed(description)

        return np.array(embedding)

    def build_entity_description(self, entity: Dict[str, Any]) -> str:
        """æ„å»ºå®ä½“æè¿°"""
        desc = f"Entity {entity['id']} of type {entity.get('type', 'unknown')}"
        if 'properties' in entity:
            desc += f" with properties: {entity['properties']}"
        return desc

    def build_subgraph_description(self, entities: List[Dict],
                                   relations: List[Dict]) -> str:
        """æ„å»ºå­å›¾æè¿°"""
        desc = "Knowledge Graph Subgraph:\n"
        desc += f"Entities: {len(entities)}\n"
        for entity in entities:
            desc += f"- {self.build_entity_description(entity)}\n"
        desc += f"\nRelations: {len(relations)}\n"
        for relation in relations:
            desc += f"- {relation['source']} --[{relation['type']}]--> {relation['target']}\n"
        return desc
```

---

## 5. æ¨ç†é“¾æ„å»ºå®ç°

### 5.1 æ¨ç†é“¾æ„å»ºå™¨

```python
from typing import List, Dict, Any, Optional

class ReasoningChainBuilder:
    """æ¨ç†é“¾æ„å»ºå™¨"""

    def __init__(self, kg_processor, llm: LLMInterface):
        self.kg_processor = kg_processor
        self.llm = llm
        self.max_chain_length = 5

    def build_reasoning_chain(self, query: str,
                             max_steps: int = 5) -> List[Dict[str, Any]]:
        """æ„å»ºæ¨ç†é“¾"""
        chain = []
        current_context = {'entities': [], 'relations': []}

        # æ­¥éª¤1ï¼šç†è§£æŸ¥è¯¢
        query_entities = self.extract_entities_from_query(query)
        chain.append({
            'step': 1,
            'action': 'query_understanding',
            'entities': query_entities,
            'description': f'Extracted entities from query: {query_entities}'
        })

        # æ­¥éª¤2ï¼šä»çŸ¥è¯†å›¾è°±è·å–ç›¸å…³å®ä½“
        relevant_entities = self.kg_processor.get_related_entities(
            query_entities, top_k=10
        )
        current_context['entities'].extend(relevant_entities)
        chain.append({
            'step': 2,
            'action': 'entity_retrieval',
            'entities': relevant_entities,
            'description': f'Retrieved {len(relevant_entities)} relevant entities'
        })

        # æ­¥éª¤3ï¼šè·å–å®ä½“é—´å…³ç³»
        relations = self.kg_processor.get_relations_between_entities(
            relevant_entities
        )
        current_context['relations'].extend(relations)
        chain.append({
            'step': 3,
            'action': 'relation_retrieval',
            'relations': relations,
            'description': f'Retrieved {len(relations)} relations'
        })

        # æ­¥éª¤4ï¼šLLMæ¨ç†
        reasoning_result = self.llm.reason(query, current_context)
        chain.append({
            'step': 4,
            'action': 'llm_reasoning',
            'result': reasoning_result.answer,
            'reasoning_steps': reasoning_result.reasoning_steps,
            'confidence': reasoning_result.confidence,
            'description': 'Performed LLM-based reasoning'
        })

        # æ­¥éª¤5ï¼šç»“æœéªŒè¯
        validation = self.validate_result(reasoning_result, current_context)
        chain.append({
            'step': 5,
            'action': 'result_validation',
            'valid': validation['valid'],
            'issues': validation['issues'],
            'description': f"Validation: {'Passed' if validation['valid'] else 'Failed'}"
        })

        return chain

    def extract_entities_from_query(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å®ä½“"""
        # ä½¿ç”¨LLMæå–å®ä½“
        prompt = f"""Extract entity names from the following query:
Query: {query}

Return a list of entity names, one per line."""

        response = self.llm.generate(prompt)
        entities = [line.strip() for line in response.split('\n') if line.strip()]
        return entities
```

---

## 6. ç»“æœéªŒè¯å®ç°

### 6.1 ç»“æœéªŒè¯å™¨

```python
class ResultValidator:
    """ç»“æœéªŒè¯å™¨"""

    def __init__(self, kg_processor):
        self.kg_processor = kg_processor

    def validate_result(self, reasoning_result: ReasoningResult,
                       context: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯æ¨ç†ç»“æœ"""
        validation = {
            'valid': True,
            'issues': [],
            'confidence': reasoning_result.confidence
        }

        # æ£€æŸ¥1ï¼šç­”æ¡ˆæ˜¯å¦åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡
        if not self.check_answer_based_on_context(
            reasoning_result.answer, context
        ):
            validation['valid'] = False
            validation['issues'].append(
                'Answer may not be based on provided context'
            )

        # æ£€æŸ¥2ï¼šæ¨ç†æ­¥éª¤æ˜¯å¦åˆç†
        if not self.check_reasoning_steps(reasoning_result.reasoning_steps):
            validation['valid'] = False
            validation['issues'].append('Reasoning steps may be invalid')

        # æ£€æŸ¥3ï¼šç½®ä¿¡åº¦æ˜¯å¦åˆç†
        if reasoning_result.confidence < 0.5:
            validation['valid'] = False
            validation['issues'].append('Low confidence score')

        # æ£€æŸ¥4ï¼šæ¥æºæ˜¯å¦å¯è¿½æº¯
        if not reasoning_result.sources:
            validation['issues'].append('No sources provided')

        return validation

    def check_answer_based_on_context(self, answer: str,
                                      context: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åŸºäºä¸Šä¸‹æ–‡"""
        # æå–ç­”æ¡ˆä¸­çš„å®ä½“
        answer_entities = self.extract_entities(answer)

        # æ£€æŸ¥è¿™äº›å®ä½“æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
        context_entity_ids = [e['id'] for e in context.get('entities', [])]

        # è‡³å°‘50%çš„å®ä½“åº”è¯¥åœ¨ä¸Šä¸‹æ–‡ä¸­
        overlap = len(set(answer_entities) & set(context_entity_ids))
        ratio = overlap / len(answer_entities) if answer_entities else 0

        return ratio >= 0.5

    def check_reasoning_steps(self, steps: List[Dict[str, Any]]) -> bool:
        """æ£€æŸ¥æ¨ç†æ­¥éª¤æ˜¯å¦åˆç†"""
        if not steps:
            return False

        # æ£€æŸ¥æ­¥éª¤æ˜¯å¦è¿è´¯
        for i in range(1, len(steps)):
            prev_step = steps[i-1]
            curr_step = steps[i]

            # æ£€æŸ¥æ­¥éª¤é—´çš„é€»è¾‘è¿æ¥
            if not self.check_step_connection(prev_step, curr_step):
                return False

        return True

    def extract_entities(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å®é™…å®ç°å¯ä»¥ä½¿ç”¨NERæ¨¡å‹
        import re
        # å‡è®¾å®ä½“æ˜¯å¤§å†™å­—æ¯å¼€å¤´çš„å•è¯
        entities = re.findall(r'\b[A-Z][a-z]+\b', text)
        return entities
```

---

## 7. APIæ¥å£å®ç°

### 7.1 REST API

```python
from fastapi import FastAPI
from pydantic import BaseModel
from typing import Optional, List

app = FastAPI()

class ReasoningRequest(BaseModel):
    query: str
    max_steps: int = 5
    llm_model: str = "gpt-4"
    context: Optional[Dict[str, Any]] = None

class ReasoningResponse(BaseModel):
    answer: str
    reasoning_chain: List[Dict[str, Any]]
    confidence: float
    sources: List[str]
    query_time: float

@app.post("/api/v1/llm-reasoning/query", response_model=ReasoningResponse)
async def llm_reasoning_query(request: ReasoningRequest):
    """LLMæ¨ç†æŸ¥è¯¢æ¥å£"""
    import time
    start_time = time.time()

    # åˆå§‹åŒ–LLM
    if request.llm_model.startswith("gpt"):
        llm = OpenAILLM(api_key=os.getenv("OPENAI_API_KEY"),
                      model=request.llm_model)
    elif request.llm_model.startswith("claude"):
        llm = ClaudeLLM(api_key=os.getenv("ANTHROPIC_API_KEY"),
                       model=request.llm_model)
    else:
        raise ValueError(f"Unsupported LLM model: {request.llm_model}")

    # æ„å»ºæ¨ç†é“¾
    chain_builder = ReasoningChainBuilder(kg_processor, llm)
    reasoning_chain = chain_builder.build_reasoning_chain(
        request.query, request.max_steps
    )

    # æå–æœ€ç»ˆç»“æœ
    final_step = reasoning_chain[-1]
    answer = final_step.get('result', '')
    confidence = final_step.get('confidence', 0.0)

    query_time = time.time() - start_time

    return ReasoningResponse(
        answer=answer,
        reasoning_chain=reasoning_chain,
        confidence=confidence,
        sources=final_step.get('sources', []),
        query_time=query_time
    )
```

---

## 8. æµ‹è¯•ä¸éªŒè¯

### 8.1 å•å…ƒæµ‹è¯•

```python
import pytest
from llm_reasoning import OpenAILLM, ReasoningChainBuilder, ResultValidator

def test_llm_integration():
    """æµ‹è¯•LLMé›†æˆ"""
    llm = OpenAILLM(api_key="test_key")

    response = llm.generate("What is a schema?")
    assert len(response) > 0

def test_reasoning_chain():
    """æµ‹è¯•æ¨ç†é“¾æ„å»º"""
    llm = OpenAILLM(api_key="test_key")
    chain_builder = ReasoningChainBuilder(kg_processor, llm)

    chain = chain_builder.build_reasoning_chain(
        "What schemas are related to OpenAPI?"
    )

    assert len(chain) > 0
    assert chain[-1]['action'] == 'result_validation'

def test_result_validation():
    """æµ‹è¯•ç»“æœéªŒè¯"""
    validator = ResultValidator(kg_processor)

    reasoning_result = ReasoningResult(
        answer="OpenAPI is related to REST API schemas",
        reasoning_steps=[{'step': 1, 'description': '...'}],
        confidence=0.8,
        sources=['entity_001']
    )

    validation = validator.validate_result(
        reasoning_result, {'entities': [{'id': 'entity_001'}]}
    )

    assert validation['valid'] == True
```

---

## 9. ç›¸å…³æ–‡æ¡£

### æ¶æ„å’Œè®¾è®¡æ¨¡å¼å‚è€ƒ

åœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œå»ºè®®å‚è€ƒä»¥ä¸‹æ¨¡å¼æ–‡æ¡£ï¼š

- **æ¶æ„æ¨¡å¼**ï¼š`../structure/ARCHITECTURE_PATTERNS_SUMMARY.md`
  - æ¨èä½¿ç”¨**å››å±‚æ¶æ„**ï¼ˆLLMå±‚ã€çŸ¥è¯†å›¾è°±å±‚ã€æ¨ç†å±‚ã€APIå±‚ï¼‰
- **è®¾è®¡æ¨¡å¼**ï¼š`../structure/DESIGN_PATTERNS_SUMMARY.md`
  - å·¥å‚æ¨¡å¼ï¼šåˆ›å»ºLLMæ¥å£
  - ç­–ç•¥æ¨¡å¼ï¼šé€‰æ‹©LLMæ¨¡å‹ç­–ç•¥
  - æ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼šå®šä¹‰æ¨ç†æµç¨‹
  - è§‚å¯Ÿè€…æ¨¡å¼ï¼šæ¨ç†ç»“æœé€šçŸ¥
- **ä¿¡æ¯å¤„ç†æ¨¡å¼**ï¼š`../structure/INFORMATION_PROCESSING_PATTERNS_SUMMARY.md`
  - æµå¤„ç†æ¨¡å¼ï¼šå®æ—¶æ¨ç†å¤„ç†
- **æ¨¡å¼å¿«é€Ÿå‚è€ƒ**ï¼š`../structure/PATTERNS_QUICK_REFERENCE.md` â­æ¨è

### å…¶ä»–å®ç°æŒ‡å—

- `MULTIMODAL_KG_IMPLEMENTATION_GUIDE.md` - å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±å®ç°æŒ‡å—
- `TEMPORAL_KG_IMPLEMENTATION_GUIDE.md` - æ—¶åºçŸ¥è¯†å›¾è°±å®ç°æŒ‡å—
- `USL_IMPLEMENTATION_GUIDE.md` - ç»Ÿä¸€Schemaè¯­è¨€å®ç°æŒ‡å—
- `README.md` - å®ç°æŒ‡å—ç›®å½•

---

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-27
**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv2.0
**ç»´æŠ¤è€…**ï¼šDSL Schemaç ”ç©¶å›¢é˜Ÿ
