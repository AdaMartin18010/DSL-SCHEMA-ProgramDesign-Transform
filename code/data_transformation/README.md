# æ•°æ®è½¬æ¢æ¨¡å—

## ğŸ“‹ æ¨¡å—æ¦‚è¿°

æ•°æ®è½¬æ¢æ¨¡å—ä¸“æ³¨äº**æ•°æ®æ¨¡å‹è½¬æ¢ã€æ•°æ®å¤„ç†**ç›¸å…³çš„è½¬æ¢åŠŸèƒ½ã€‚

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. å¢é‡è½¬æ¢å™¨ï¼ˆIncrementalConverterï¼‰

**åŠŸèƒ½**ï¼š

- Schemaå˜æ›´æ£€æµ‹ï¼ˆå“ˆå¸Œæ¯”è¾ƒã€è¯¦ç»†å˜æ›´æ£€æµ‹ï¼‰
- ä¾èµ–å›¾æ„å»ºï¼ˆè¡¨ä¾èµ–ã€å¤–é”®ä¾èµ–ï¼‰
- å¢é‡è½¬æ¢ï¼ˆæŒ‰ä¾èµ–é¡ºåºè½¬æ¢ï¼‰
- PostgreSQL DDLç”Ÿæˆ

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from code.data_transformation import IncrementalConverter

converter = IncrementalConverter()

old_schema = {
    'tables': {
        'users': {
            'fields': {
                'id': {'type': 'integer'},
                'name': {'type': 'string'}
            }
        }
    }
}

new_schema = {
    'tables': {
        'users': {
            'fields': {
                'id': {'type': 'integer'},
                'name': {'type': 'string'},
                'email': {'type': 'string'}  # æ–°å¢å­—æ®µ
            }
        }
    }
}

result = converter.incremental_convert(old_schema, new_schema, 'postgresql')
print(result['conversion_result']['statements'])
```

### 2. æ•°æ®æ¨¡å‹è½¬æ¢å™¨ï¼ˆDataModelConverterï¼‰

**åŠŸèƒ½**ï¼š

- æ˜Ÿå‹æ¨¡å¼åˆ°PostgreSQLè½¬æ¢
- é›ªèŠ±æ¨¡å¼åˆ°PostgreSQLè½¬æ¢
- Data Vaultåˆ°PostgreSQLè½¬æ¢
- æ˜Ÿå‹æ¨¡å¼ä¸é›ªèŠ±æ¨¡å¼äº’è½¬
- æ˜Ÿå‹æ¨¡å¼ä¸Data Vaultäº’è½¬

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from code.data_transformation import DataModelConverter, DataModelType

converter = DataModelConverter()

star_model = {
    'fact_tables': [{
        'name': 'sales_fact',
        'measures': [
            {'name': 'amount', 'data_type': 'decimal'},
            {'name': 'quantity', 'data_type': 'integer'}
        ],
        'dimension_keys': [
            {'name': 'customer', 'dimension_table': 'customer_dim'},
            {'name': 'product', 'dimension_table': 'product_dim'}
        ]
    }],
    'dimension_tables': [{
        'name': 'customer_dim',
        'attributes': [
            {'name': 'customer_name', 'data_type': 'string'},
            {'name': 'region', 'data_type': 'string'}
        ]
    }]
}

# è½¬æ¢ä¸ºPostgreSQL
result = converter.convert(star_model, DataModelType.STAR, 'postgresql')

# ç”ŸæˆDDL
ddl = converter.generate_sql_ddl(result)
print(ddl)
```

### 3. æ•°æ®å¤„ç†å™¨ï¼ˆDataProcessorï¼‰

**åŠŸèƒ½**ï¼š

- ETLç®¡é“å¤„ç†ï¼ˆæå–ã€è½¬æ¢ã€åŠ è½½ï¼‰
- æ•°æ®åˆ†æå¤„ç†

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from code.data_transformation import DataProcessor

processor = DataProcessor()

# å¤„ç†ETLç®¡é“
etl_config = {
    'extract': {
        'data_sources': ['sales_db'],
        'rules': []
    },
    'transform': {
        'rules': [],
        'quality_checks': []
    },
    'load': {
        'targets': ['warehouse_db'],
        'strategy': 'append'
    }
}

result = processor.process_etl_pipeline(etl_config)
print(result)
```

### 4. ETLå¤„ç†å™¨ï¼ˆETLProcessorï¼‰

**åŠŸèƒ½**ï¼š

- ETLç®¡é“åˆ›å»ºå’Œæ‰§è¡Œ
- æ•°æ®æå–ï¼ˆå…¨é‡ã€å¢é‡ã€CDCï¼‰
- æ•°æ®è½¬æ¢ï¼ˆæ¸…æ´—ã€éªŒè¯ã€ä¸°å¯Œã€èšåˆã€å…³è”ï¼‰
- æ•°æ®åŠ è½½ï¼ˆè¿½åŠ ã€æ›´æ–°æ’å…¥ã€æ›¿æ¢ã€åˆå¹¶ï¼‰
- æ‰§è¡Œå†å²è®°å½•

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from code.data_transformation import ETLProcessor

processor = ETLProcessor()

# åˆ›å»ºETLç®¡é“
pipeline_config = {
    'pipeline_id': 'sales_etl',
    'name': 'é”€å”®æ•°æ®ETLç®¡é“',
    'extract': [{
        'rule_id': 'extract_sales',
        'source_type': 'database',
        'source_config': {
            'table': 'sales_source',
            'connection': 'postgresql://localhost/sales_db'
        },
        'extract_type': 'incremental',
        'batch_size': 1000
    }],
    'transform': [{
        'rule_id': 'clean_sales',
        'transform_type': 'clean',
        'source_fields': ['customer_name', 'product_name'],
        'target_fields': ['customer_name', 'product_name']
    }],
    'load': [{
        'rule_id': 'load_sales',
        'target_type': 'database',
        'target_config': {
            'table': 'sales_warehouse',
            'connection': 'postgresql://localhost/warehouse_db'
        },
        'load_type': 'append'
    }]
}

pipeline = processor.create_pipeline(pipeline_config)
result = processor.execute_pipeline(pipeline.pipeline_id)
```

### 5. æ•°æ®åˆ†æå¤„ç†å™¨ï¼ˆDataAnalyticsProcessorï¼‰

**åŠŸèƒ½**ï¼š

- ç»Ÿè®¡åˆ†æ
- é¢„æµ‹åˆ†æ
- æè¿°æ€§åˆ†æ
- è¯Šæ–­åˆ†æ
- è§„èŒƒæ€§åˆ†æ

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from code.data_transformation import DataAnalyticsProcessor

processor = DataAnalyticsProcessor()

# åˆ›å»ºåˆ†æè§„åˆ™
rule_config = {
    'rule_id': 'sales_analysis',
    'analysis_type': 'statistical',
    'data_sources': ['sales_data'],
    'metrics': ['amount', 'quantity'],
    'dimensions': ['region', 'product_category']
}

rule = processor.create_analysis_rule(rule_config)

# æ‰§è¡Œåˆ†æ
sample_data = [
    {'region': 'North', 'product_category': 'Electronics', 'amount': 1000, 'quantity': 10},
    {'region': 'South', 'product_category': 'Electronics', 'amount': 1500, 'quantity': 15},
]

result = processor.execute_analysis(rule.rule_id, sample_data)
print(f"æŒ‡æ ‡: {result.metrics}")
print(f"æ´å¯Ÿ: {result.insights}")
```

### 6. æ•°æ®è´¨é‡æ£€æŸ¥å™¨ï¼ˆDataQualityCheckerï¼‰

**åŠŸèƒ½**ï¼š

- æ•°æ®è´¨é‡è§„åˆ™å®šä¹‰
- å®Œæ•´æ€§æ£€æŸ¥
- å‡†ç¡®æ€§æ£€æŸ¥
- ä¸€è‡´æ€§æ£€æŸ¥

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from code.data_transformation import DataQualityChecker

checker = DataQualityChecker()

# æ·»åŠ è´¨é‡è§„åˆ™
checker.add_quality_rule('sales_quality', {
    'field': 'amount',
    'type': 'completeness'
})

checker.add_quality_rule('sales_quality', {
    'field': 'amount',
    'type': 'range',
    'min': 0
})

# æ£€æŸ¥æ•°æ®è´¨é‡
data = [
    {'amount': 100, 'quantity': 10},
    {'amount': None, 'quantity': 5},  # è´¨é‡é—®é¢˜
    {'amount': -50, 'quantity': 3},  # è´¨é‡é—®é¢˜
]

result = checker.check_data_quality(data, 'sales_quality')
print(f"è´¨é‡åˆ†æ•°: {result['quality_score']}%")
print(f"é—®é¢˜: {result['issues']}")
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
code/data_transformation/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ incremental_converter.py      # å¢é‡è½¬æ¢å™¨
â”œâ”€â”€ data_model_converter.py        # æ•°æ®æ¨¡å‹è½¬æ¢å™¨
â”œâ”€â”€ etl_processor.py               # ETLå¤„ç†å™¨
â”œâ”€â”€ data_analytics_processor.py    # æ•°æ®åˆ†æå¤„ç†å™¨
â””â”€â”€ README.md                      # æœ¬æ–‡æ¡£
```

## ğŸ”§ ä¾èµ–

- Python 3.8+
- æ ‡å‡†åº“ï¼štyping, dataclasses, enum, datetime, hashlib, json

## ğŸ“ ä½¿ç”¨è¯´æ˜

1. **å¯¼å…¥æ¨¡å—**ï¼š

```python
from code.data_transformation import (
    IncrementalConverter,
    DataModelConverter,
    DataModelType,
    ETLProcessor,
    DataAnalyticsProcessor,
    DataQualityChecker
)
```

2. **ä½¿ç”¨è½¬æ¢å™¨**ï¼š

```python
# å¢é‡è½¬æ¢
converter = IncrementalConverter()
result = converter.incremental_convert(old_schema, new_schema, 'postgresql')

# æ•°æ®æ¨¡å‹è½¬æ¢
model_converter = DataModelConverter()
result = model_converter.convert(star_model, DataModelType.STAR, 'postgresql')
```

3. **ä½¿ç”¨å¤„ç†å™¨**ï¼š

```python
# ETLå¤„ç†
etl_processor = ETLProcessor()
pipeline = etl_processor.create_pipeline(pipeline_config)
result = etl_processor.execute_pipeline(pipeline.pipeline_id)

# æ•°æ®åˆ†æ
analytics_processor = DataAnalyticsProcessor()
rule = analytics_processor.create_analysis_rule(rule_config)
result = analytics_processor.execute_analysis(rule.rule_id, data)
```

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### æ•°æ®æ¨¡å‹è½¬æ¢

- âœ… æ˜Ÿå‹æ¨¡å¼ â†” PostgreSQL
- âœ… é›ªèŠ±æ¨¡å¼ â†” PostgreSQL
- âœ… Data Vault â†” PostgreSQL
- âœ… æ˜Ÿå‹æ¨¡å¼ â†” é›ªèŠ±æ¨¡å¼
- âœ… æ˜Ÿå‹æ¨¡å¼ â†” Data Vault

### æ•°æ®å¤„ç†

- âœ… ETLç®¡é“ï¼ˆæå–ã€è½¬æ¢ã€åŠ è½½ï¼‰
- âœ… æ•°æ®åˆ†æï¼ˆç»Ÿè®¡ã€é¢„æµ‹ã€æè¿°ã€è¯Šæ–­ã€è§„èŒƒï¼‰
- âœ… æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆå®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€ä¸€è‡´æ€§ï¼‰

### å¢é‡è½¬æ¢

- âœ… Schemaå˜æ›´æ£€æµ‹
- âœ… ä¾èµ–åˆ†æ
- âœ… å¢é‡è½¬æ¢æ‰§è¡Œ
- âœ… PostgreSQL DDLç”Ÿæˆ

## ğŸ“Š ä»£ç ç»Ÿè®¡

- **æ€»ä»£ç è¡Œæ•°**ï¼šçº¦2,500è¡Œ
- **æ ¸å¿ƒç±»æ•°é‡**ï¼š6ä¸ª
- **æ–¹æ³•æ•°é‡**ï¼šçº¦80ä¸ª

## ğŸ”„ åç»­è®¡åˆ’

1. **å®Œå–„ä¾èµ–ä¼ æ’­ç®—æ³•**ï¼šå®ç°å®Œæ•´çš„ä¾èµ–ä¼ æ’­å’Œå½±å“åˆ†æ
2. **å†²çªå¤„ç†**ï¼šå®ç°å˜æ›´å†²çªæ£€æµ‹å’Œè§£å†³ç­–ç•¥
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šç¼“å­˜æœºåˆ¶ã€å¹¶è¡Œå¤„ç†ã€å»¶è¿Ÿè®¡ç®—
4. **æµ‹è¯•éªŒè¯**ï¼šå•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€æ€§èƒ½æµ‹è¯•
5. **æ‰©å±•æ”¯æŒ**ï¼šæ”¯æŒæ›´å¤šæ•°æ®æ¨¡å‹ç±»å‹å’Œè½¬æ¢æ–¹å‘

---

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-21
**æœ€åæ›´æ–°**ï¼š2025-01-21
**ç»´æŠ¤è€…**ï¼šDSL Schemaç ”ç©¶å›¢é˜Ÿ
